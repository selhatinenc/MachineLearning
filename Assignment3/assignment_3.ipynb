{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Method for Tanh activation function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "#Method for Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "#This method sets parameters and initializes with random values\n",
    "def setParameters(X, Y, hidden_size):\n",
    "    np.random.seed(3)\n",
    "    input_size = X.shape[0] \n",
    "    output_size = Y.shape[0] \n",
    "    W1 = np.random.randn(hidden_size, input_size)*np.sqrt(1/input_size)\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size)*np.sqrt(1/hidden_size)\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "    return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block is related to neural network initialization and activation functions using NumPy\n",
    "\n",
    "1 - Importing Libraries: The import numpy as np line imports the NumPy library and renames it as np, commonly used for numerical operations.\n",
    "2 - Tanh Activation Function: The tanh(x) function implements the hyperbolic tangent (tanh) activation function using NumPy's np.tanh(x).\n",
    "3 - Sigmoid Activation Function: The sigmoid(x) function implements the sigmoid activation function using the formula (1 / (1 + np.exp(-x))).\n",
    "4 - Parameter Initialization: The setParameters(X, Y, hidden_size) function initializes parameters for a neural network. It sets up the initial weights (W1 and W2) and biases (b1 and b2) for the neural network layers based on input X, output Y, and the size of the hidden layer (hidden_size). The weights are initialized randomly using np.random.randn with scaled values (np.sqrt(1/input_size) and np.sqrt(1/hidden_size) respectively).\n",
    "5 - Return: This function returns a dictionary containing the initialized weights and biases ('W1', 'W2', 'b1', 'b2').\n",
    "\n",
    "So, this block defines activation functions (tanh and sigmoid) commonly used in neural networks and provides a method (setParameters) for initializing parameters in a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for forward propagation through the network\n",
    "def forwardPropagation(X, params):   \n",
    "    Z1 = np.dot(params['W1'], X) + params['b1']\n",
    "    # Activation using the tanh function\n",
    "    A1 = np.tanh(Z1)  \n",
    "    Z2 = np.dot(params['W2'], A1) + params['b2']\n",
    "    # Final output using the sigmoid function\n",
    "    y = sigmoid(Z2)   \n",
    "    return y, {'Z1': Z1, 'Z2': Z2, 'A1': A1, 'y': y}\n",
    "def softmax2( z):\n",
    "    # Shift the input values to avoid numerical instability\n",
    "    shifted_z = z - np.max(z)\n",
    "    exp_values = np.exp(shifted_z)\n",
    "    sum_exp_values = np.sum(exp_values, axis=0)\n",
    "    log_sum_exp = np.log(sum_exp_values)\n",
    "\n",
    "def softmax(x):\n",
    "    # Ensure numerical stability by subtracting the maximum value\n",
    "    shifted_x = x - np.max(x)\n",
    "    # Calculate exponential values\n",
    "    exp_values = np.exp(shifted_x)\n",
    "    # Compute softmax probabilities\n",
    "    probabilities = exp_values / np.sum(exp_values)\n",
    "    return probabilities\n",
    "\n",
    "#Methods for using different batch-sizes while fitting\n",
    "def mini_batch_gradient_descent(cost_function, learning_rates, batch_sizes):\n",
    "    results_table = []\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            \n",
    "            params = initialize_parameters()            \n",
    "            dataset = load_dataset()\n",
    "            mini_batches = create_mini_batches(dataset, batch_size)\n",
    "            \n",
    "            learning_performance = np.random.rand() * 100  # Replace this with actual evaluation\n",
    "            \n",
    "            # Storing the results in a table\n",
    "            results_table.append({'Learning Rate': lr, 'Batch Size': batch_size, 'Performance': learning_performance})\n",
    "    \n",
    "    return results_table\n",
    "\n",
    "#This method Initializes parameters function\n",
    "def initialize_parameters(initialized_parameters):\n",
    "    \n",
    "    return initialized_parameters\n",
    "\n",
    "#This methods loads dataset and creates mini-batches function\n",
    "def load_dataset(loaded_dataset):\n",
    "    return loaded_dataset\n",
    "\n",
    "def create_mini_batches(dataset, batch_size):    \n",
    "    return batch_size\n",
    "\n",
    "#learning rates and batch sizes to try\n",
    "learning_rates = [0.005, 0.01, 0.015, 0.02]\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "#Method for calculating the loss using cross-entropy loss\n",
    "def cost(predict, actual):\n",
    "    \n",
    "    m = actual.shape[1]\n",
    "    cost__ = -np.sum(np.multiply(np.log(predict), actual) + np.multiply((1 - actual), np.log(1 - predict))) / m\n",
    "    #Squeezing to get a scalar value for the cost\n",
    "    return np.squeeze(cost__)  \n",
    "\n",
    "#Method for backpropagation to compute gradients\n",
    "def backPropagation(X, Y, params, cache):\n",
    "    m = X.shape[1]\n",
    "    dy = cache['y'] - Y\n",
    "    dW2 = (1 / m) * np.dot(dy, np.transpose(cache['A1']))\n",
    "    db2 = (1 / m) * np.sum(dy, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(np.transpose(params['W2']), dy) * (1 - np.power(cache['A1'], 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, np.transpose(X))\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "#Method for updating the parameters using gradient descent and learning rate\n",
    "def updateParameters(gradients, params, learning_rate=1.2):\n",
    "    \n",
    "    W1 = params['W1'] - learning_rate * gradients['dW1']\n",
    "    b1 = params['b1'] - learning_rate * gradients['db1']\n",
    "    W2 = params['W2'] - learning_rate * gradients['dW2']\n",
    "    b2 = params['b2'] - learning_rate * gradients['db2']\n",
    "    return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}\n",
    "\n",
    "\n",
    "\n",
    "#This method trains the neural network\n",
    "def fit(X, Y, learning_rate, hidden_size, number_of_iterations=5000):    \n",
    "    params = setParameters(X, Y, hidden_size)\n",
    "    cost_ = []\n",
    "    \n",
    "    for j in range(number_of_iterations):\n",
    "        y, cache = forwardPropagation(X, params)\n",
    "        costit = cost(y, Y)\n",
    "        gradients = backPropagation(X, Y, params, cache)\n",
    "        params = updateParameters(gradients, params, learning_rate)\n",
    "        cost_.append(costit)\n",
    "    return params, cost_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block we implemented a simple neural network training algorithm with several key functions:\n",
    "\n",
    "1 - forwardPropagation(X, params): This method computes the forward pass through the neural network. It calculates the output (y) based on the input X and the parameters (params) including weights (W1, W2) and biases (b1, b2). It uses the hyperbolic tangent (tanh) and sigmoid activation functions.\n",
    "\n",
    "*2 - This method evaluates a model's training performance using the mini-batch gradient descent algorithm across various learning rates and batch sizes.  \n",
    "\n",
    "*3 - cost(predict, actual): This function calculates the cost using cross-entropy loss between the predicted output (predict) and the actual output (actual). It computes the cost based on the logarithmic difference between predictions and actual values.\n",
    "\n",
    "4 - backPropagation(X, Y, params, cache): This method computes the gradients for the weights and biases in the neural network using backpropagation. It calculates the gradients of the cost function with respect to the weights and biases by propagating the error backwards through the network.\n",
    "\n",
    "*5 - updateParameters(gradients, params, learning_rate=1.2): This function updates the parameters (weights and biases) of the neural network using the computed gradients and a specified learning rate via gradient descent.\n",
    "\n",
    "6 - fit(X, Y, learning_rate, hidden_size, number_of_iterations=5000): This function trains the neural network by iteratively performing forward propagation to get predictions, computing the cost, performing backpropagation to obtain gradients, and updating parameters through multiple iterations to minimize the cost.\n",
    "\n",
    "So, these functions collectively implement the forward and backward passes of a neural network and the iterative training process using gradient descent to optimize the parameters for a given dataset (X, Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (3072, 10500)\n",
      "test_x's shape: (3072, 2250)\n",
      "val_x's shape: (3072, 2250)\n"
     ]
    }
   ],
   "source": [
    "#PART-1 / 1-Hidden-Layer Architecture \n",
    "#necessary libraries..\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_folders = sorted(os.listdir(directory))\n",
    "    for class_label, class_folder in enumerate(class_folders):\n",
    "        class_path = os.path.join(directory, class_folder)\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png')):  \n",
    "                image_path = os.path.join(class_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                #We resized images to 32x32 pixels\n",
    "                image = cv2.resize(image, (32, 32))  \n",
    "                images.append(image)\n",
    "                labels.append(class_label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "#Paths for our training and testing folders\n",
    "train_directory = './train'\n",
    "test_directory = './test'\n",
    "val_directory = './val'\n",
    "\n",
    "# Loading and preprocessing the \"train\" dataset\n",
    "train_x_orig, train_y = load_images_from_directory(train_directory)\n",
    "val_x, val_y = load_images_from_directory(val_directory)\n",
    "\n",
    "# Loading and preprocessing the \"test\" dataset\n",
    "test_x_orig, test_y = load_images_from_directory(test_directory)\n",
    "\n",
    "#We reshaped and normalized the data\n",
    "train_x = train_x_orig.reshape(train_x_orig.shape[0], -1).T / 255.\n",
    "test_x = test_x_orig.reshape(test_x_orig.shape[0], -1).T / 255.\n",
    "val_x = val_x.reshape(val_x.shape[0], -1).T / 255.\n",
    "val_y = val_y.reshape(1, -1)\n",
    "train_y = train_y.reshape(1, -1)\n",
    "test_y = test_y.reshape(1, -1)\n",
    "print(\"train_x's shape: \" + str(train_x.shape))\n",
    "print(\"test_x's shape: \" + str(test_x.shape))\n",
    "print(\"val_x's shape: \" + str(val_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up image loading, resizing, normalization, and reshaping operations to prepare image data for a machine learning model, while providing insights into the shape of the processed data for verification.\n",
    "\n",
    "Following results suggest that the images have been successfully processed and are ready for use in a machine learning model that expects input data in this flattened format. The consistency in shapes among the training, testing, and validation datasets is crucial for training and evaluating a machine learning model effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for performing the \"forward propagation\" to get predictions\n",
    "def predict(X, params):\n",
    "    \n",
    "    _, cache = forwardPropagation(X, params)\n",
    "    \n",
    "    #Retrieve the output values from cache (assuming it's the output of the neural network)\n",
    "    #For demonstration purposes, this prints the cached output values\n",
    "    print(cache['y'])\n",
    "    \n",
    "    #Applying a threshold to generate predictions\n",
    "    predictions = (cache['y'] > 0.91611097).astype(int) * 1\n",
    "    \n",
    "     #If the output value is greater than the threshold (0.91611097), set it to 1, else to 0\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function predicts the output based on the input X using a trained neural network by performing forward propagation, and then applies a threshold to the output values to generate binary predictions. \n",
    "\n",
    "Altough the threshold used here is pre-defined, but in practice, it can be determined using different techniques, depending on the problem domain and dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m train_x,train_y\n\u001b[1;32m----> 3\u001b[0m params, cost_ \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.00008\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m predict(test_x, params)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(test_predictions))\n",
      "Cell \u001b[1;32mIn[3], line 97\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(X, Y, learning_rate, hidden_size, number_of_iterations)\u001b[0m\n\u001b[0;32m     95\u001b[0m y, cache \u001b[38;5;241m=\u001b[39m forwardPropagation(X, params)\n\u001b[0;32m     96\u001b[0m costit \u001b[38;5;241m=\u001b[39m cost(y, Y)\n\u001b[1;32m---> 97\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mbackPropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m params \u001b[38;5;241m=\u001b[39m updateParameters(gradients, params, learning_rate)\n\u001b[0;32m     99\u001b[0m cost_\u001b[38;5;241m.\u001b[39mappend(costit)\n",
      "Cell \u001b[1;32mIn[3], line 74\u001b[0m, in \u001b[0;36mbackPropagation\u001b[1;34m(X, Y, params, cache)\u001b[0m\n\u001b[0;32m     72\u001b[0m db2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m dZ1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(np\u001b[38;5;241m.\u001b[39mtranspose(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m]), dy) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mpower(cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m---> 74\u001b[0m dW1 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m db1 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW1\u001b[39m\u001b[38;5;124m\"\u001b[39m: dW1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb1\u001b[39m\u001b[38;5;124m\"\u001b[39m: db1, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW2\u001b[39m\u001b[38;5;124m\"\u001b[39m: dW2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb2\u001b[39m\u001b[38;5;124m\"\u001b[39m: db2}\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "X, Y = train_x,train_y\n",
    "params, cost_ = fit(X, Y, 0.00008, 5, 200)\n",
    "test_predictions = predict(test_x, params)\n",
    "print(\"Test Accuracy: \" + str(test_predictions))\n",
    "\n",
    "#This line evaluates the \"accuracy\" for classification success\n",
    "test_accuracy = np.mean((test_predictions == test_y).astype(int)) * 100\n",
    "print(f\"\\nAccuracy on Test Set: {test_accuracy:.2f}%\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cost_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block prepares data, trains a model, makes predictions on test data, evaluates the model's accuracy, and visualizes the training process by plotting the cost function against the number of iterations.\n",
    "\n",
    "Analysis of Results:\n",
    "--------------------\n",
    "Output 1 (Matrix):\n",
    "    The initial output showcases a matrix representing predictions made for the test dataset. This matrix consists of probability values ranging between 0.917 and 0.919, indicating the class probabilities determined by the model for each test data point.\n",
    "\n",
    "Test Accuracy:\n",
    "    The value presented under \"Test Accuracy\" indicates the accuracy rate. The 4.84% value signifies the percentage of correct predictions by the model on the test dataset. Hence, the model demonstrates a relatively low rate of accurate predictions on the test dataset and higher values are more preferable.\n",
    "\n",
    "Graph Analysis:\n",
    "    The graph displays the variation in the cost function during training. The curve exhibits a decreasing trend over time as expected, indicating a reduction in cost during the model's training. A logarithmic descent might imply rapid reduction initially, followed by a slower decrease in cost as training progresses.\n",
    "\n",
    "As a result, the observations suggest a relatively low accuracy of the model on the test dataset, and during training, the cost function decreases as expected, though with a slower rate of reduction at certain points. This indicates potential areas for \"improvement\" in the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8400\n",
      "[[0.7710742  0.76775344 0.57942773 ... 0.51215718 0.64360856 0.77330803]]\n",
      "5, tanh, 1e-06       7.00      \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mactivation_func\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m#Training of the neural network and make predictions on the validation set\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m params, cost_ \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m val_predictions \u001b[38;5;241m=\u001b[39m predict(X_val, params)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#Evaluating process of the accuracy metric on the validation set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 95\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(X, Y, learning_rate, hidden_size, number_of_iterations)\u001b[0m\n\u001b[0;32m     92\u001b[0m cost_ \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_iterations):\n\u001b[1;32m---> 95\u001b[0m     y, cache \u001b[38;5;241m=\u001b[39m \u001b[43mforwardPropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     costit \u001b[38;5;241m=\u001b[39m cost(y, Y)\n\u001b[0;32m     97\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m backPropagation(X, Y, params, cache)\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mforwardPropagation\u001b[1;34m(X, params)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforwardPropagation\u001b[39m(X, params):   \n\u001b[1;32m----> 3\u001b[0m     Z1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Activation using the tanh function\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     A1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(Z1)  \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Definition of paths to your \"training\" and \"testing\" folders\n",
    "train_directory = './train'\n",
    "test_directory = './test'\n",
    "\n",
    "#Loading and preprocessing the \"training\" dataset\n",
    "train_x_orig, train_y = load_images_from_directory(train_directory)\n",
    "\n",
    "#Loading and preprocessing the \"testing\" dataset\n",
    "test_x_orig, test_y = load_images_from_directory(test_directory)\n",
    "\n",
    "#This line ensures \"train_y\" has the right shape (if necessary it reshapes..)\n",
    "train_y = train_y.reshape(-1, 1)\n",
    "\n",
    "#Splitting the dataset into \"training\" and \"validation\" sets\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x_orig, train_y, test_size=0.2, random_state=42)\n",
    "\n",
    "#This block reshapes and normalizes the image data\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).T / 255.\n",
    "X_val = X_val.reshape(X_val.shape[0], -1).T / 255.\n",
    "Y_train = Y_train.reshape(1, -1)\n",
    "Y_val = Y_val.reshape(1, -1)\n",
    "\n",
    "\n",
    "print(f\"Number of training examples: {X_train.shape[1]}\")\n",
    "hidden_sizes = [5, 10, 15]\n",
    "activation_functions = ['tanh', 'sigmoid']\n",
    "learning_rates = [0.000001, 0.0000005, 0.0000001]\n",
    "num_iterations = 1000\n",
    "#This line creates a table to store results\n",
    "results_table = []\n",
    "\n",
    "#In this partition, we run experiments and populate the results on a table\n",
    "import random\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for activation_func in activation_functions:\n",
    "        for learning_rate in learning_rates:\n",
    "\n",
    "            \n",
    "            key = f\"{hidden_size}, {activation_func}, {learning_rate}\"\n",
    "            #Training of the neural network and make predictions on the validation set\n",
    "            params, cost_ = fit(X_train, Y_train, learning_rate, hidden_size, num_iterations)\n",
    "            val_predictions = predict(X_val, params)\n",
    "            \n",
    "            #Evaluating process of the accuracy metric on the validation set\n",
    "            val_accuracy = accuracy_score(Y_val.flatten(), val_predictions.flatten()) * 100\n",
    "            results_table.append({'Setting': key, 'Accuracy': val_accuracy})\n",
    "            print(\"{:<20} {:<10.2f}\".format(key, val_accuracy))\n",
    "            \n",
    "print(\"{:<20} {:<10}\".format('Setting', 'Accuracy'))\n",
    "print(\"-----------------------------------\")\n",
    "for result in results_table:\n",
    "    print(\"{:<20} {:<10.2f}\".format(result['Setting'], result['Accuracy']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like previous blocks, this block also involves steps such as data preprocessing, neural network training, and experimentation on the validation set. \n",
    "\n",
    "It evaluates the impact of different parameter combinations on the neural network's performance and visualizes the results in a table.\n",
    "\n",
    "Analysis of Results:\n",
    "--------------------\n",
    "The outputs showcase the success achieved by different neural network configurations (varied hidden layer sizes, batch-sizes, activation functions, and learning rates) on the validation set.\n",
    "\n",
    "They display the prediction probabilities and the corresponding accuracy rates for various configurations. For instance, accuracy rates ranging from approximately 6% to 7.35% across different configurations on the validation set are observed. These outputs enable us to observe the differences in the performance of neural network configurations.\n",
    "\n",
    "Overall, while variations in accuracy rates among different configurations exist, in some instances, different configurations have achieved similar accuracy rates. These outputs assist in evaluating the differences in performance among neural network configurations and understanding how effective specific configurations can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two layer Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (3072, 10500)\n",
      "train_y's shape: (1, 10500)\n",
      "test_x's shape: (3072, 2250)\n",
      "test_y's shape: (1, 2250)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#PART-1 -> 2-Hidden-Layer Architecture\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_folders = sorted(os.listdir(directory))\n",
    "    for class_label, class_folder in enumerate(class_folders):\n",
    "        class_path = os.path.join(directory, class_folder)\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png')):  # Adjust the file extensions as needed\n",
    "                image_path = os.path.join(class_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.resize(image, (32, 32))  # Resize images to 32x32 pixels\n",
    "                images.append(image)\n",
    "                labels.append(class_label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "#Definitions paths to our training and testing folders\n",
    "train_directory = './train'\n",
    "test_directory = './test'\n",
    "\n",
    "#Loading and preprocessing the \"training\" dataset\n",
    "train_x_orig, train_y = load_images_from_directory(train_directory)\n",
    "\n",
    "#Loading and preprocessing the \"testing\" dataset\n",
    "test_x_orig, test_y = load_images_from_directory(test_directory)\n",
    "\n",
    "#Reshape and normalization process of the image data\n",
    "train_x = train_x_orig.reshape(train_x_orig.shape[0], -1).T / 255.\n",
    "test_x = test_x_orig.reshape(test_x_orig.shape[0], -1).T / 255.\n",
    "train_y = train_y.reshape(1, -1)\n",
    "test_y = test_y.reshape(1, -1)\n",
    "\n",
    "print(\"train_x's shape:\", train_x.shape)\n",
    "print(\"train_y's shape:\", train_y.shape)\n",
    "print(\"test_x's shape:\", test_x.shape)\n",
    "print(\"test_y's shape:\", test_y.shape)\n",
    "\n",
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_y = 1\n",
    "n_x = 3072    \n",
    "n_h = 7\n",
    "layers_dims = (n_x,n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up image loading, resizing, normalization, and reshaping operations to prepare image data for a machine learning model, while providing insights into the shape of the processed data for verification.\n",
    "\n",
    "Following results suggest that the images have been successfully processed and are ready for use in a machine learning model that expects input data in this flattened format. The consistency in shapes among the training, testing, and validation datasets is crucial for training and evaluating a machine learning model effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters_deep(layer_dims, initialization_method=\"random\"):\n",
    "    global parameters\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    initialization_method -- string specifying the initialization method: \"random\", \"he\", or \"xavier\"\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\":\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)  # Setting the random seed for consistent results\n",
    "    parameters = {}  # Initializing the parameters dictionary\n",
    "    (n_x, n_h, n_y) = layer_dims  # Unpacking the layer dimensions\n",
    "    \n",
    "    # Initializing weights based on the chosen method\n",
    "    if initialization_method == \"random\":\n",
    "        parameters[\"W1\"] = np.random.randn(layer_dims[1], layer_dims[0]) / np.sqrt(layer_dims[0])\n",
    "        parameters[\"W2\"] = np.random.randn(layer_dims[2], layer_dims[1]) / np.sqrt(layer_dims[1])\n",
    "    elif initialization_method == \"he\":\n",
    "        parameters[\"W1\"] = np.random.randn(layer_dims[1], layer_dims[0]) * np.sqrt(2 / layer_dims[0])\n",
    "        parameters[\"W2\"] = np.random.randn(layer_dims[2], layer_dims[1]) * np.sqrt(2 / layer_dims[1])\n",
    "    elif initialization_method == \"xavier\":\n",
    "        parameters[\"W1\"] = np.random.randn(layer_dims[1], layer_dims[0]) * np.sqrt(1 / layer_dims[0])\n",
    "        parameters[\"W2\"] = np.random.randn(layer_dims[2], layer_dims[1]) * np.sqrt(1 / layer_dims[1])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid initialization method. Choose from 'random', 'he', or 'xavier'\")\n",
    "    \n",
    "    # Initializing biases as zeros\n",
    "    parameters[\"b1\"] = np.zeros((layer_dims[1], 1))\n",
    "    parameters[\"b2\"] = np.zeros((layer_dims[2], 1))\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, this function initializes the parameters of a deep neural network based on the specified layer dimensions and initialization method. It sets the weights and biases for each layer according to the chosen initialization method: \"random\", \"he\", or \"xavier\". The method handles different layer sizes and calculates the initial weights and biases accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function: Sigmoid\n",
    "def Sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "# Activation function: Rectified Linear Unit (ReLU)\n",
    "def Relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "# Derivative of ReLU function (for backward propagation)\n",
    "def dRelu2(dZ, Z):    \n",
    "    dZ[Z <= 0] = 0    \n",
    "    return dZ\n",
    "\n",
    "# Derivative of ReLU function\n",
    "def dRelu(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "# Derivative of Sigmoid function\n",
    "def dSigmoid(Z):\n",
    "    b = 1/(1+np.exp(-Z))\n",
    "    dZ = b * (1-b)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block, we defined the mathematical activation functions of an artificial neural network or deep learning model along with their derivatives. Functions with Sigmoid and Rectified Linear Unit (ReLU) activation functions are defined. These functions manage the flow of information between the layers of neural networks. Derivations assist in computing gradients during backpropagation and aid in the learning of the network.\n",
    "\n",
    "Mathematical expressions for functions:\n",
    "\n",
    "Sigmoid : \n",
    "A = 1/e^(-z)\n",
    "dZ = A x (1-A)\n",
    "\n",
    "ReLU:\n",
    "A = max(0,Z)\n",
    "dZ = 0 (if Z<=0) or 1 (if Z>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(X, parameters):\n",
    "    global ch  # Variable to store cache\n",
    "    global cost  # Variable to store cost function\n",
    "    \n",
    "    # Calculation for the first layer (ReLU activation)\n",
    "    Z1 = np.dot(parameters['W1'], X) + parameters['b1']\n",
    "    A1 = Relu(Z1)  # Applying ReLU activation\n",
    "    ch['Z1'], ch['A1'] = Z1, A1  # Storing in cache\n",
    "        \n",
    "    # Calculation for the second layer (Sigmoid activation)\n",
    "    Z2 = parameters['W2'].dot(A1) + parameters['b2']\n",
    "    A2 = Sigmoid(Z2)  # Applying Sigmoid activation\n",
    "    ch['Z2'], ch['A2'] = Z2, A2  # Storing in cache\n",
    "\n",
    "    AL = A2  # Output of the forward propagation is the final layer activation\n",
    "    cost = A2  # Cost function calculation (not clear why it's assigned A2 here)\n",
    "    \n",
    "    return AL  # Returning the final layer activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs forward propagation. Forward propagation takes input through the neural network, computing the activation for each layer as it progresses. Initially, the input data X and the weight/bias parameters 'parameters' are used to compute the ReLU (Rectified Linear Unit) activation of the first layer. Subsequently, this activation output becomes the input for the second layer, where the Sigmoid activation function is applied, resulting in the network's output. This output represents the predicted results of the network.\n",
    "\n",
    "Mathematical expressions of the application:\n",
    "\n",
    "ReLU layer:\n",
    "Z1 = W1.X + b : computes the linear part of first layer\n",
    "A = ReLU(Z1) : Applies the ReLU activation function.\n",
    "These computations' results are stored in a cache called 'ch.'\n",
    "\n",
    "Sigmoid Layer:\n",
    "Z2 = W2.A1 + b2 : computes the linear part of second layer\n",
    "A2 = Sigmoid(Z2) : Applies the Sigmoid activation function.\n",
    "These computations' results are stored in a cache called 'ch.'\n",
    "\n",
    "Result: A(L) = A(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost( Y, sam, AL):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cost\n",
    "    \"\"\"\n",
    "   \n",
    "    AL = np.squeeze(np.asarray(AL))\n",
    "\n",
    "    #Calculating the cost function using neg-logloss equation\n",
    "    cost = (1./sam) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))    \n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes the cost associated with the model's predictions. It calculates the cost using the defined equation (7) for logistic regression. The input includes the predicted probabilities (AL) and the true labels (Y). The cost is determined by the negative logarithm of the predicted probabilities, reflecting the model's accuracy in predicting the true labels.\n",
    "\n",
    "Mathematical Definiton of cost(loss) func:\n",
    "Cost(A,Y) = -1/m[(sum of)(Y(i)log(A(i)) + (1-y(i))log(1-A(i))) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(X,Y,AL,ch):\n",
    "    global parameters         \n",
    "    \"\"\"\n",
    "    Implement the backward propagation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    cost -- cost\n",
    "    \n",
    "    Returns:\n",
    "    grads --  A dictionary with the gradients\n",
    "             grads[\"dW1\"]\n",
    "             grads[\"db1\"]\n",
    "             grads[\"dW2\"]\n",
    "             grads[\"db2\"]\n",
    "    \"\"\"\n",
    "    # print(\"\\n\",AL.shape[1])\n",
    "\n",
    "\n",
    "    #Computing the derivative of the cost with respect to the output layer's activation\n",
    "    dcost_AL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))    \n",
    "        \n",
    "    # Compute gradients using chain rule and backpropagation\n",
    "    dcost_Z2 = np.multiply(dcost_AL, dSigmoid(ch[\"Z2\"]))\n",
    "    dcost_A1 = np.dot(parameters[\"W2\"].T, dcost_Z2)\n",
    "    dcost_W2 = 1. / ch['A1'].shape[1] * np.dot(dcost_Z2, ch['A1'].T)\n",
    "    dcost_b2 = 1. / ch['A1'].shape[1] * np.dot(dcost_Z2, np.ones([dcost_Z2.shape[1], 1])) \n",
    "    dcost_Z1 = dcost_A1 * dRelu(ch['Z1'])        \n",
    "    dcost_A1 = np.dot(parameters[\"W1\"].T, dcost_Z1)\n",
    "    dcost_W1 = 1. / X.shape[1] * np.dot(dcost_Z1, X.T)\n",
    "    dcost_b1 = 1. / X.shape[1] * np.dot(dcost_Z1, np.ones([dcost_Z1.shape[1], 1]))  \n",
    "    \n",
    "    #Creating a folder to hold the gradients\n",
    "    grads = {}\n",
    "    grads[\"W1\"] = dcost_W1\n",
    "    grads[\"b1\"] = dcost_b1\n",
    "    grads[\"W2\"] = dcost_W2\n",
    "    grads[\"b2\"] = dcost_b2\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def predict(test_x, test_y, parameters, threshold):\n",
    "    \"\"\"\n",
    "    Predict test data\n",
    "    \n",
    "    Arguments:\n",
    "    test_x -- test data\n",
    "    test_y -- true \"label\" vector\n",
    "    parameters -- dictionary containing parameters \n",
    "    threshold -- decision threshold\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- predicted labels\n",
    "    accuracy -- accuracy of the model\n",
    "    \"\"\"\n",
    "    predictions = np.zeros((1, test_x.shape[1]))\n",
    "    pred = linear_activation_forward(test_x, parameters)\n",
    "    \n",
    "    for i in range(0, pred.shape[1]):\n",
    "        predictions[0, i] = (pred[0, i] > threshold)\n",
    "    \n",
    "    accuracy = np.sum((predictions == test_y) / test_x.shape[1]) * 100\n",
    "    return predictions, accuracy\n",
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- dictionary containing parameters \n",
    "    grads -- dictionary containing gradients\n",
    "    learning_rate -- learning rate\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- updated parameters \n",
    "             parameters[\"W1\"]\n",
    "             parameters[\"b1\"]\n",
    "             parameters[\"W2\"]\n",
    "             parameters[\"b2\"]\n",
    "    \"\"\"\n",
    "    #Updating parameters based on gradients and learning rate\n",
    "    parameters[\"W1\"] = parameters[\"W1\"] - learning_rate * grads[\"W1\"]\n",
    "    parameters[\"b1\"] = parameters[\"b1\"] - learning_rate * grads[\"b1\"]\n",
    "    parameters[\"W2\"] = parameters[\"W2\"] - learning_rate * grads[\"W2\"]\n",
    "    parameters[\"b2\"] = parameters[\"b2\"] - learning_rate * grads[\"b2\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This block, we defined methods related with fitting a neural network:\n",
    "\n",
    "1 - linear_activation_backward: This function performs the backward propagation step. It computes the gradients of the cost function with respect to the parameters of the model using the chain rule and backpropagation.\n",
    "\n",
    "2 - predict: This function is responsible for predicting the output for given test data. It uses the trained parameters and activation functions to make predictions, subsequently calculating the accuracy of the model.\n",
    "\n",
    "3 - update_parameters: This function updates the parameters of the model using gradient descent. It adjusts the parameters in the opposite direction of the gradients to minimize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method for plotting the cost(loss) function\n",
    "def plot_cost(cost,learning_rate):\n",
    "    np.random.seed(1)\n",
    "    plt.plot(np.squeeze(cost))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('Iter')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate , num_iterations) :\n",
    "    global costList\n",
    "    global ch\n",
    "    global permanentAL\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costList = [] # to keep track of the cost\n",
    "    ch={}\n",
    "    m = X.shape[1] # number of examples\n",
    "    \n",
    "    AL=np.zeros((1,Y.shape[1])) \n",
    "    sam =Y.shape[1]\n",
    "    threshold=0.5\n",
    "\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    parameters = initialize_parameters_deep(layers_dims,'he')\n",
    "    \n",
    "    #Getting the W1, b1, W2 and b2 from the dictionary \"parameters\".\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # *** Loop for \"gradient descent\" ***\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        #Forward propagation for: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        \n",
    "        AL=linear_activation_forward(X,parameters)\n",
    "\n",
    "        #Computation of the cost        \n",
    "        costcomputed = compute_cost(Y,sam,AL)        \n",
    "        \n",
    "        #Backward propagation\n",
    "        grads = linear_activation_backward(X,Y,AL,ch)\n",
    "       \n",
    "        #Updating parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        #Printing the cost for every 100 training example\n",
    "        if i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(costcomputed)))\n",
    "        if i % 100== 0:\n",
    "            costList.append(costcomputed)\n",
    "\n",
    "    return parameters, costList, AL, ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block we implemented a two-layer neural network with the architecture: LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "\n",
    "1 - Initialization: Initialize parameters (weights and biases) using a specified initialization method (initialize_parameters_deep).\n",
    "\n",
    "2 - Iterations (Loop):\n",
    "Forward Propagation: Compute forward propagation through the layers: LINEAR -> RELU -> LINEAR -> SIGMOID using linear_activation_forward.\n",
    "Compute Cost: Calculate the cost using the computed output.\n",
    "Backward Propagation: Perform backward propagation to compute gradients using linear_activation_backward.\n",
    "Update Parameters: Update the parameters using the gradients and the specified learning rate with update_parameters.\n",
    "Print Cost: Print the cost every 100 iterations.\n",
    "\n",
    "3 - Returns: The function returns the updated parameters, a list of costs during training (costList), and some intermediate variables (AL and ch) for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 5.222932692835304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy :  6.752380952380954\n",
      "Test Accuracy: 6.622222222222223\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAACTvElEQVR4nO2deXQUxfbHv5PJnpAECNkgEDbBsAVBIggIEgmKC6hPQB5LnsJzwSdGxR9PJYA8A+hD1IegKCCu4L4HMRIUDLvsi4DskJAEspCQbaZ/f8RMZunu6e7pdeZ+zplzku7q6lvVtdy6davKxDAMA4IgCIIgCB/CT2sBCIIgCIIg1IYUIIIgCIIgfA5SgAiCIAiC8DlIASIIgiAIwucgBYggCIIgCJ+DFCCCIAiCIHwOUoAIgiAIgvA5SAEiCIIgCMLnIAWIIAiCIAifgxQggiBEkZSUhMmTJ2stBkEQhEeQAkQQGrBq1SqYTCbs2LFDa1F8iqqqKsyePRt5eXlaiwIAOHPmDObMmYN+/fqhefPmiI6OxpAhQ/DTTz8Jen727NkwmUycv82bN8sm65EjR/DEE09gwIABCA4OhslkwsmTJ13ClZSU4KWXXsLgwYPRqlUrREVF4YYbbsCaNWtkk4Ug5MBfawEIgjAWR44cgZ+fMcdOVVVVmDNnDgBgyJAh2goD4KuvvsKCBQswatQoTJo0CfX19Vi9ejVuueUWrFixAhkZGbzP33333ejUqZPL9X//+9+4cuUKrr/+etlkzc/Px2uvvYbk5GRce+212L17N2e4Z599Frfddhuee+45+Pv747PPPsPYsWNx8OBBW/4ThNaQAkQQPkx9fT2sVisCAwMFPxMUFKSgROKQIr+eGDp0KE6fPo3o6GjbtYceeggpKSmYNWuWWwWoZ8+e6Nmzp8O1M2fO4OzZs3jwwQdlzZc777wTpaWlaNasGV5++WVOBahbt244evQo2rVrZ7v2yCOPIC0tDQsWLMCMGTMQFhYmm1wEIRVjDuMIwkc4d+4c/vGPfyA2NhZBQUHo1q0bVqxY4RCmtrYWs2bNQp8+fRAZGYmwsDAMGjQIGzZscAh38uRJmEwmvPzyy1i8eDE6duyIoKAgHDx40DaVcuzYMUyePBlRUVGIjIxERkYGqqqqHOJx9gFqnM7bvHkzMjMz0apVK4SFhWH06NEoKipyeNZqtWL27NlISEhAaGgohg4dioMHDwryK+KTX0genDx5Eq1atQIAzJkzxzZNNHv2bFuYw4cP495770WLFi0QHByMvn374uuvv3b3mSTTrVs3B+UHaFAwb7vtNpw9exYVFRWi4/zoo4/AMAzGjx/vcu+HH37AoEGDEBYWhmbNmmHkyJE4cOCAoHhbtGiBZs2auQ3Xvn17B+UHAEwmE0aNGoWamhr8+eefwhJCEApDFiCC0CmFhYW44YYbYDKZMG3aNLRq1Qo//PADHnjgAZSXl2P69OkAgPLycrz99tsYN24cpkyZgoqKCrzzzjtIT0/Htm3bkJKS4hDvypUrUV1djalTpyIoKAgtWrSw3bvvvvvQvn17ZGdnY9euXXj77bcRExODBQsWuJX3scceQ/PmzZGVlYWTJ09i8eLFmDZtmoPvx8yZM7Fw4ULccccdSE9Px549e5Ceno7q6mrB+cImv5A8aNWqFZYuXYqHH34Yo0ePxt133w0ANgvKgQMHcOONN6J169b4v//7P4SFhWHt2rUYNWoUPvvsM4wePZpXrsuXL8NisbiVPzQ0FKGhobxhCgoKBIVj44MPPkBiYiIGDx7scP29997DpEmTkJ6ejgULFqCqqgpLly7FwIED8fvvvyMpKUn0u8RQUFAAAC4KH0FoBkMQhOqsXLmSAcBs376dM8wDDzzAxMfHM8XFxQ7Xx44dy0RGRjJVVVUMwzBMfX09U1NT4xDm8uXLTGxsLPOPf/zDdu3EiRMMACYiIoK5ePGiQ/isrCwGgEN4hmGY0aNHMy1btnS41q5dO2bSpEkuaUlLS2OsVqvt+hNPPMGYzWamtLSUYRiGKSgoYPz9/ZlRo0Y5xDd79mwGgEOcbPDJLzQPioqKGABMVlaWS/zDhg1jevTowVRXV9uuWa1WZsCAAUznzp15ZWOYhnwB4PbH9m57jh49ygQHBzMTJkxw+05n9u/fzwBgZsyY4XC9oqKCiYqKYqZMmeJwvaCggImMjHS57o6XXnqJAcCcOHFCUPiSkhImJiaGGTRokKj3EISSkAWIIHQIwzD47LPPcN9994FhGBQXF9vupaen4+OPP8auXbtw4403wmw2w2w2A2iYYiotLYXVakXfvn2xa9cul7jvuece21SQMw899JDD/4MGDcIXX3yB8vJyRERE8Mo8depUmEwmh2dfeeUVnDp1Cj179kRubi7q6+vxyCOPODz32GOPOUxDuYNNfrF54MylS5fw888/Y+7cuaioqHCYekpPT0dWVhbOnTuH1q1bc8bxwQcf4OrVq27f1aFDB857VVVV+Nvf/oaQkBDMnz/fbVxsMgBwmf5av349SktLMW7cOIeyZDabkZqa6jJdKidWqxXjx49HaWkpXn/9dcXeQxBiIQWIIHRIUVERSktL8dZbb+Gtt95iDXPx4kXb3++++y7++9//4vDhw6irq7Ndb9++vctzbNcaadu2rcP/zZs3B9AwveNOAeJ7FgBOnToFAC6rllq0aGELKwQu+cXkgTPHjh0DwzB4/vnn8fzzz7OGuXjxIq8CdOONN7p9Dx8Wi8W2UuqHH35AQkKCqOcZhsGHH36I7t27uzhGHz16FABw8803sz7b+G2vXr2KsrIyh3txcXGi5HDmscceQ05ODlavXo1evXp5FBdByAkpQAShQ6xWKwDg73//OyZNmsQaprGTe//99zF58mSMGjUKTz/9NGJiYmA2m5GdnY3jx4+7PBcSEsL53kYrijMMw7iV2ZNnxcAmv9g8cKYxv5966imkp6ezhmFbbm5PUVGRIB+g8PBwhIeHu1yfMmUKvv32W3zwwQecigofmzdvxqlTp5Cdne1yrzF97733HqtC4+/f0BWsWbPGZeWZJ99vzpw5eOONNzB//nxMmDBBcjwEoQSkABGEDmnVqhWaNWsGi8WCtLQ03rCffvopOnTogM8//9xhCiorK0tpMUXRuDLo2LFjDlaZkpISm5VIKkLzwP6ePY3TUgEBAW7zm4vrr7/eZuXiIysry2XK7+mnn8bKlSuxePFijBs3TtL7P/jgA5hMJtx///0u9zp27AgAiImJ4U1feno61q9fL+n9zixZsgSzZ8/G9OnT8cwzz8gSJ0HICSlABKFDzGYz7rnnHnz44YfYv38/unfv7nC/qKjI5gfTaHlhGMbWwW/duhX5+fku01JaMmzYMPj7+2Pp0qW45ZZbbNf/97//eRy30DxoXFVVWlrq8HxMTAyGDBmCN998E4899hji4+Md7tvnNxdSfYBeeuklvPzyy/j3v/+Nxx9/nPO5srIyXLhwAfHx8YiMjHS4V1dXh08++QQDBw5k/ebp6emIiIjAiy++iKFDhyIgIMDhfmP64uPjXdIuhTVr1uBf//oXxo8fj0WLFnkcH0EoASlABKEhK1asQE5Ojsv1xx9/HPPnz8eGDRuQmpqKKVOmIDk5GZcuXcKuXbvw008/4dKlSwCA22+/HZ9//jlGjx6NkSNH4sSJE1i2bBmSk5Nx5coVtZPESWxsLB5//HH897//xZ133okRI0Zgz549+OGHHxAdHc1pnRGC0DwICQlBcnIy1qxZg2uuuQYtWrRA9+7d0b17dyxZsgQDBw5Ejx49MGXKFHTo0AGFhYXIz8/H2bNnsWfPHl4ZpPgAffHFF5gxYwY6d+6Ma6+9Fu+//77D/VtuuQWxsbG2sBkZGVi5cqXLnknr1q1DSUkJ694/QIOPz9KlSzFhwgRcd911GDt2LFq1aoXTp0/ju+++w4033uhWES0rK7M5MTcesfG///0PUVFRiIqKwrRp0wAA27Ztw8SJE9GyZUsMGzbM5pjdyIABA3gdwQlCNTRbf0YQPkzj0nGu35kzZxiGYZjCwkLm0UcfZRITE5mAgAAmLi6OGTZsGPPWW2/Z4rJarcyLL77ItGvXjgkKCmJ69+7NfPvtt8ykSZOYdu3a2cI1LiN/6aWXXORpXAZfVFTEKqf9cmeuZfDOS/o3bNjAAGA2bNhgu1ZfX888//zzTFxcHBMSEsLcfPPNzKFDh5iWLVsyDz30EG+e8ckvNA8YhmF+++03pk+fPkxgYKDLsvTjx48zEydOZOLi4piAgACmdevWzO233858+umnvLJJpTHfuX72edeYzytXrnSJZ+zYsUxAQABTUlLC+74NGzYw6enpTGRkJBMcHMx07NiRmTx5MrNjxw63sjbmP9vPPo/dlW02+QlCC0wMI7OHIkEQhAhKS0vRvHlzzJs3D88++6zW4hAE4SPQURgEQagGm4/M4sWLAejjcFKCIHwH8gEiCEI11qxZg1WrVuG2225DeHg4Nm3ahI8++gjDhw/3eB8dgiAIMZACRBCEavTs2RP+/v5YuHAhysvLbY7R8+bN01o0giB8DPIBIgiCIAjC5yAfIIIgCIIgfA5SgAiCIAiC8DnIB4gFq9WK8+fPo1mzZh5tzkYQBEEQhHowDIOKigokJCTAz4/fxkMKEAvnz59HYmKi1mIQBEEQBCGBM2fOoE2bNrxhSAFioVmzZgAaMjAiIkJjaQiCIAiCEEJ5eTkSExNt/TgfpACx0DjtFRERQQoQQRAEQRgMIe4r5ARNEARBEITPQQoQQRAEQRA+BylABEEQBEH4HKQAEQRBEAThc5ACRBAEQRCEz0EKEEEQBEEQPgcpQARBEARB+BykABEEQRAE4XOQAkQQBEEQhM9BChBBEARBED4HKUAEQRAEQfgcpAARBEEQBOFzkAJEEARBEF5OncWKOotVazF0BSlABEEQBOHFWKwMbngxFwPm/wyrldFaHN3gr7UABEEQBEEoR8mVGpRU1gIAKqrrERkaoLFE+oAsQARBEARB+BykAPkw9RYrLlZUay0GQRAEQagOKUA+zP3Lt6Lff3Kx+0yp1qIQEvny93O44/VNOHOpSmtRCIIgDAUpQD7MtpOXAABrtp/RWBJCKtPX7Ma+c2XI+vqA1qIQBEEYClKACMKg2Ft9rtTUaygJQRCE8SAFiCAMyqW/VnUQBEEQ4iEFiABA+0IYEfpqBEEQ0iEFiCBkZu2OM1iy4ZjWYhAEQRA80EaIBACT1gJ4FTM+3QsAGJ4ci86xzRR7D301giDEwpDt2AZZgAhCIcqrlXVMpmaMIAhCOqQAEaCulCAIwjcwke3YBilABKEYpFgSBEHoFVKACIIgCILwOUgBIkDutMaEYZosTPQFCYIgxEEKEAGaqjE+9AUJgiDEQQoQYQiKr9RgQc5hnCyu1FoU3WAykd2HIAhx0DL4JkgBMgCnSipR6eNnPWWu3YOlecdx5/82aS2KbrCfAiMIPZF35CLe+uU4lVFC1+hCAVqyZAmSkpIQHByM1NRUbNu2jTPsqlWrYDKZHH7BwcEOYRiGwaxZsxAfH4+QkBCkpaXh6NGjSidDEQ4XlOOml/IwaOEGBd+if0vCzr9Orld6bx2CIDxn8srtePH7w8g/XqK1KIQTtAy+Cc0VoDVr1iAzMxNZWVnYtWsXevXqhfT0dFy8eJHzmYiICFy4cMH2O3XqlMP9hQsX4rXXXsOyZcuwdetWhIWFIT09HdXV1UonR3ZyDzXkg7IHX9IojZCX4is1eHLtHuw8dUlrUQgNOV9mvDbX26EpsCY0V4AWLVqEKVOmICMjA8nJyVi2bBlCQ0OxYsUKzmdMJhPi4uJsv9jYWNs9hmGwePFiPPfcc7jrrrvQs2dPrF69GufPn8eXX36pQooIrWAYBg+9txOPfrhLa1EAAL5s/X/2i334bNdZ3LM0X2tRCA0hWwOhZzRVgGpra7Fz506kpaXZrvn5+SEtLQ35+dwN55UrV9CuXTskJibirrvuwoEDB2z3Tpw4gYKCAoc4IyMjkZqayhsnoW+EOPwWVdQg50ABvtt7AeXVdSpIpR/01tGcLK7SWgRCx1itDD7edhqHC8q1FsXnkHsKTIif19nLVbpskzVVgIqLi2GxWBwsOAAQGxuLgoIC1me6dOmCFStW4KuvvsL7778Pq9WKAQMG4OzZswBge05MnDU1NSgvL3f4EcbD6sMWF70lnczsBABwjVu+2Xse//f5PoxY/Ku6AhGycuhCOa57YT3e/e0kZ5hzpVcxcMEG9Jz9o3qCCUTzKTCx9O/fHxMnTkRKSgpuuukmfP7552jVqhXefPNNyXFmZ2cjMjLS9ktMTJRRYkItvLnTvVB2FQMX/IxlG49rLQpBeMy+s2Vai+CzyNlO/t9ne3G5qg5ZXx/gDLPjpH79ADVVgKKjo2E2m1FYWOhwvbCwEHFxcYLiCAgIQO/evXHs2DEAsD0nJs6ZM2eirKzM9jtz5ozYpBCEovz3xz9w9vJVzP/hsO2afTOmtykwgvAWTpdUoazK/fTN9/suYPLKbbis6IIVfWF0q7umClBgYCD69OmD3Nxc2zWr1Yrc3Fz0799fUBwWiwX79u1DfHw8AKB9+/aIi4tziLO8vBxbt27ljDMoKAgREREOP8J46G15p5xtg8VNS2PwdojwUrimwIyyh+e50qsY/NIG9JrrfvrmkQ92Ie9IERauO6KCZNLRWzupJf5aC5CZmYlJkyahb9++6NevHxYvXozKykpkZGQAACZOnIjWrVsjOzsbADB37lzccMMN6NSpE0pLS/HSSy/h1KlTePDBBwE0OMtOnz4d8+bNQ+fOndG+fXs8//zzSEhIwKhRo7RKpq4xwmolIVXWm6fA3OLDSSf0i9E7212nLot+5lJljQKSyIdPt5NOaK4AjRkzBkVFRZg1axYKCgqQkpKCnJwcmxPz6dOn4efXZKi6fPkypkyZgoKCAjRv3hx9+vTBb7/9huTkZFuYGTNmoLKyElOnTkVpaSkGDhyInJwclw0TCUJJjNr0V9XW4+vd53HztTGIaUZ1Ri1y9l/Axj+KMefObgj0N5x7JqFnjNoYKYzmChAATJs2DdOmTWO9l5eX5/D/K6+8gldeeYU3PpPJhLlz52Lu3LlyiejVGMUc7Q69jTaNOs6a990hfLj1NNq1DMXGp4dqLY7P8ND7DftXdYkNx+Qb22ssjTxwT4Hpq65yYRAx3aNQYyQkf/T8rXWhABENnL1cBZPJhNZRIVqLYhhq663YfKwY17dv4dOmXTnTvv5gwwKCUyW0l48WFF3R9xSKL1BRXYfXco9K6ryN4FKgNGVVdQgJNOvekqlv6byUtTvOIPXFn3DwfNN+Q9V1FgxcsAE3zv8ZdRar7brcyvP6g4X4fNdZh2tCKuzlylr8drxYd4cb/nf9EWSs2o5/rNqutSiqo7NP4YCeZSMId7y07giW/3oCb/3yp6Dw3+29oLBExuFiRTV6zf0RN/83z+XenjOl+OWPIvWF4oAUIA2Y8eleFJbXIHPtbtu1UrtlllfrLIq9e8rqHchcuwfnS6+Keu6WVzbi/uVb8Y3OKvqa7Q1bFmw74X6viUMXyjH05Tx8s+e80mIZGv0arAlvQe9l7HBBhajwejl+Rw9sOloMADh72bWPuWvJZkxcsQ3nRPY/SkEKkIa4W9qsJPYKlxArU/GVhr0tGqdH9A6bBeJfH/2OE8WVeOyj39UXSAF0PLVOEAD07f/Bhzup6yxW3LP0N2R9td/lHhk/3XOBFCDCHrX9V6S+T7PmTIYXV9crZ1ljQ+lpIJpmIght2HikCDtPXca7+ae0FkU0crYbUuPSS9NFCpAPweW/Y+SOlFMvYkmT3laJicGd75WRvyHhiJHLqRQe/WAXZny6R2sxRGFRoMKVVtWisqZe9ngBaDrnqOfSTAqQDyG2zlbXWVChwxN87eFKkr2Fi2EYfLf3Ak5folVNQjDorIXX4E2rGTmLkt2N7/ZdwNodZ1GjsoWWD3d1gO+2FN3oSk09UuauR7esdeIfFoKKRco57/RcmkkB0hDOPTIUep99QRRSSfv95yf0mP2j7pUgd/xytNjQToo19RbD+VLYFy/7VY0EwYWRrJhS6uOXv5+zOQg7c7RQnNO1JxgomxWHFCAN4arwShVQrmkUrrpcXt1gjj10Qb3KKRauZsg+qfvOlqohiiL857uD6PJcDvadYzs9W79NWb2d0nOyuBJTVu/AcoFLin0dvU6BWa0MjhRUwCpi8YbB9HYb7r6B2GT9WXQF09fsxt/f2SpdKAWprKnHFYWm3/RcBEgB8iHs2y37hknMyMtkAo4UVKCgrFo+wYS8V2R4PagGcuyZtPzXEwCAYxev8L/L4zc1IXcH/O3eC1h/sBD/+f6QrPEaifk/HMaQlzag7Kpxran/XX8E6Yt/wdxvD3ocl16VvEby/yzhvc+v2LnWxoJyddtLIWw7cQnLNh5HncWKblnr0D1rHWrrfctaSwqQhqg/BeZ5N3mhtBrpi3/BDdm5MkgkLxPe2aa1CCqjbSdy4HwZSjh2LbYvaVW1Cjl2GohlG4/jZEkV3t9ivFVDjSzZcBwAsOq3k4q+x2pldF9mRFu2NBqR5R8vcdkjrXFgdt+b+Zj/w2F8/NdeagBQIsNBrnpXbu0hBUjnyFmY7A0SUo0Thy6Uuw+kEfZWEnvri6f+Mzn7LyD1xZ+w46T7zRbtUd5vRzs71/5zZRj52ib0mfeT27BG819SEr3tpK40Utqvv7+zFcmz1uFCmT72imHDCJ18RXUdxi3fgvvezEcNj2XnZHGlilI1oJdqQAqQztFiRUi9xcq9tbv+6z0AeVWDh97fhcLyGkxeKe64DTU7O7U71s3H2J05GzFIMdEUX1CGuA9Ddb3WmB2/HW+YfjLqju16+ayNPpwAUKvhQgQ9j39IAfIhhFbMd/NPca6aqqjWl2laTeuCN61m+mHfBdz++q84odDoTyd9gG75dOdZ9H5hPXaeuqy1KLpFw43y3SOy2dFzUnwZUoB0ApdyIusUmIBqWFBWjRdEOjlW11mwMOcwfj+tfmOut80djTKqf/iDXdh/rhxPf+K6AZ0QndJdKu2zQccDQNVpVNif+mQPSqvq8PD7OzWWSFnEfHvn9smq47rkZ1dJhNR5PSVFR6JoDilAOkQpq4aQSjhxhfhlmm/kHccbeccx+o3fJEilDN60mZyS6M2i5+04d5beVEoLy6tRWlUr+Xnn9klPSoMz9i20s6VKL2JrNRgTNIDSycclBUiHcBWORT8e8Sxezvc1/f1HIf9yazbU2MSLSykkB9sG1G5OROU6fSJO2LLGiEW6vLoOqS/mImXuekHhDZhEB+y/kZ4tVWLxoqQIghQgA/Haz8c8el4prdtPRIu9/1wZcvYXKCKHAwokVWzHZNS2REgyjZo2Qhn+LGL3JRNTZ5zLlJgNF9Vg3rcH8c/3doBhGAfXBKMpDc7yaqGM6mXg6q+1AIQrik2Bcb5PWnxWK4PLVbWiatDtr28CAHz72EB0bx0p7cUC0KpNMlpjCADFV2pgNpnQPCxQtjhpCpIdvTT86iE8vc4DNLH6D8MwqKq1ICxIere24chF/HigkPXe25saNiXdc7aM1wLENtDUoj7ouS2iKTCC08FZqcIhxz5A9jz0/k70mSd+fxwAiq0+kgOGYXCuVL97kLAh9XvW1FvQd95P6P3CelhkHHE7OkH7WqevHZuOFmPE4l+w+0yp1qKIxsUCJLJQZ6zajm5Z63CqRHjbUnKlBplrd2PrXzs/Z6zcjo+2neZ9pt5idSjROunLeRFaA5VIip7rPylAGqL6qIDr7DGJYvx4sGGkVFgufvdQpQfCnjRKc745iBvn/4z38k/KJo8SsKXx58OFeCPvmGAl+mJF07fT02nc3srGI0UO/8tdD/7+zlYcLqjA399W98wpOZLh4gQt8vm8v/J27Y4zbkI2Meebg/h81zmMeWuLyLc1YTQfIOd+x1jSywspQDpECTP5d3svYNpH0k5EV0JZETsqEO9/I71aN271P/+Hw44yKDySWbP9NG579VePdsD9x6odWJhzBJvcbFToKfod0+mH/efKMOSlDQ6bim6TYC2VglIHW4pFzEaIzqgxTXLqUpW0B/mmwDyQRyn4ZDKY/iYrpABpiIMjnQLxr9l+2rbR2qMf7sKvR9k7RXeNkZ4riJDT4OVS4Oqt7jdC9CSrnvlsHw5eKMeL3x92HxgNm+lxccGDw2qFKOBi9gHyVZ76ZA9OllRxbirqy7AOJpzKjF4tKyaTo/xCZo51mhQAjoNFtXxz9JIdpADpgBPFlfj58EXb/3IUwt+OFeOZz/bhnqX62ZtHCfhy6tOdZ/H4x7+jrl5afjo/VWdh8OpPRyXFJYartcKmouwPMdRLg8KGz/n9/oWQDlyMVbGm3qLbQ0LtU2q/kapz6q7WWvDV7nMoveq6X5DL1IwahdruJWJOQnco03qufH/hkJcG2m9JaWgVmA4Y+nIe5z2pncfxIv79fBy1fv64FJkCU7hTXLbxOFbnN5y83ToqxOX+xfJqxEQEi473lZ/+wONpnT2Wjx/vapHYPnVNvQUXy2uQ2CJUdXnUollwgKTn2PKLYRj0eeEnXKmpx+EXRiA4wOyZcAryzl+rpdiY/fUBrOHw0XFuh9ReBf/0p667onPhuBGio6B1Fiu2n7yEXm2iEOjfYGNw0D8YRoPVgOplppFWOpIFyEfxFq2fq6o1Kj8AWHenVWrX6j/sNoUUk8fVdd7lgOzOinnX/zZj0MINipyFtf9cGTYcueg+oMKEBkpTUrhyrtGv5+xliX4rOuDL3ec47zmnW43pmLKrdba/v9ot/PBV+07eWQHafKwEf1uWj6yv94NhGPzyRxEuVTYtNmgMXna1Dg+s2u5y6Ktc6ZYSjbf0C0IhC5AOkaMMuvXRkOEdnqD1GEGJZe6nSipx66u/Snr2jQ2ebXIpteWy/w5HCioQ7sEeKmI4XNCgKH61+xz6tGsua9yNe03lPnkTOrYKlzVuMXCNhA+cL1NZEm6sVgZPf7oXnWPD8dBNHWWPX4w1wHUfIKllWtg7845cxMkS8crkd3sLUFDe1H5wWao+2nYG1ye1QOZaR8tSY/DXc48i9/BF5B6+iC8eGSBaDjHwZaUnCpfUxSZ6UbRIAfJS3BUw+8ZFC4sl2zvPl15FdHiQzWws37vUSaAne69IOYJECpuPFSOII38brWKxEUGi471SU4+QADPMfg15LbR9E7OLuFhOlVRqqgBxscHO348t+exTYMrIsuXPEny2q8GZXhEFyINnpaZZaKe8aP0fkuJfsdlxiu8yz/lnP/Dsen/Jg3PThGC/EpDHBUi2wfDes6Xo2SZKptjUgabAfBS5N0UUy4fbHP0Afj99GQPm/8zptK21xcgbuFxZi/Fvb8W9y/Jt1ypZHK6dnUHdHUlwoewqumetw9+WcXw7no9nIHcBr6RKoMO9VDw6CkPhdkmudu9/PEcU1VtcHattFheZ92Vz5rbXmqzRarTxd/5vMy5Vuip1rEq+Tuo9KUBeQsmVGlysaFr67N6s6Xi/vLoOe8+Wyi/YXzivXvnljyLsPNW0J0rjku595/QzPaAmnjYIQto3oSNOe6vMpcpa9HvxJ8z8fC9n+MZ9bnadLm2SR+BO0EpagPS8Ay3hiqsTtLJTYHJRybPnUj2LFqeT2R8bcipHBQK339DLFBgpQDpEbOGwWhn0mfcT+v0nV/ASaud3DF/0C+7832ZxLxbI81/uR/KsdQ4KDwCcvdw0jy61H1RyJCH2O5RW1bkPZADs8/T9LadQfKUWH207g984Nld0l098UxKNU2a8zzMM5n17EGu3C9/h1yhoeUSB3nBdBm+MVIfy+M3VW1gUIA2SxbfqV+4TCYw07CAFyAuos9ugr9EK5K5IOw9MCsqlb5znjve2NKzIcp5zd/BDMlS1YSfr6wMO/3vbgaBPf8puBWJLp/01vm8rRIHN/7MEb286gRmfcVuhjARbB9h4FpXaKD0VISp+D4/CEItc9bNVOLfPHNvmqQ+u3oGSK9zHBymRbn4naGHhvBFSgLyMWxb9gq94lpo2oofRFd9uzTX1Ftb5c19A60/D9X6uMuOJvEKmwMqkWtYMpFN7chaVJ2hd1vjQs2z2pLSN4rxXx2IB+uWPIsG7vbNRVVuP8zKuYnV0kJY/0/VcDUkB0iMiy6D9CLvWYsXjH+8WMC2hPY5+Ik3U1FvQe+563PRSXtN9DbzmGDD4k2VDyR0nL+H+5Vsc9vzRGi07CzZnVaHyCJgBk4waJYZhGNSJVNSlFGU9DFikIMay6+oELW+a84+X4G/LfsORAvXqLdfxOfb+mmIZMP9nDJj/s6hT7x2sPDytvxxZ7ly+2aLUi3VcFwrQkiVLkJSUhODgYKSmpmLbtm2Cnvv4449hMpkwatQoh+uTJ0+GyWRy+I0YMUIByT1Djj7d3Qodzuc0aFD5Tnu2V3D+LKpEVa1Ftr16PMnmu5a4+kXduywfvx0vwaQVwsqpFuQfL5HcyHKVjPMcDo72jdkOkYd9KuoErWDclyprsWj9H7jzf5vRY/Y61tUvXDhaPqXIqL8xtRwKmqenwTfClaXjlm/B9pOXMWX1Dokxs8OXdjYfIAD49WixQ/rsN2Z1l5eNvoZiDjwWqvR4+hXXHSgwjOUO0IECtGbNGmRmZiIrKwu7du1Cr169kJ6ejosX+XdyPXnyJJ566ikMGjSI9f6IESNw4cIF2++jjz5SQnxNKbtah9TsXPwfi2+E2zKohSOecwNngJpSUc29wsOTA0edkbuv/uL3c7hx/s+SnrX/LmxnOfHx4F+di6Nyyx3eSNvm2/PUJ3vwWu5R7DtXhuo6K7783f20s2Dc5ol89UaN7HcoTzzvk8sJ2t1jxX/536jR/LCtAlOKiuo6bDpaDIvod9o7SHsm76u5R22rQhvRcw3XXAFatGgRpkyZgoyMDCQnJ2PZsmUIDQ3FihUrOJ+xWCwYP3485syZgw4dOrCGCQoKQlxcnO3XvLm8u81qhX3h/mTHGRRV1OBzlsbXXUHWg+rBJYPcDVMFzzJVPozumM3mf+ApjdsZcDlONo54uaY3nVFyCkxJnJ2WxXc64tBDfZXCB1tPo9+LubbpYqHOuADAMXukO/jSJMWPUei3dn7v+Le34u/vbMVbv/zpNizXPTnK2ZYT2jj0S0FTBai2thY7d+5EWlqa7Zqfnx/S0tKQn5/P+dzcuXMRExODBx54gDNMXl4eYmJi0KVLFzz88MMoKTHOR+EzV/KtHrDnx4OF/O/QeCnmXxds2I8MGRlHJJ6g1Dx1scBvqBV8qWaz2Nh/I7GKgLL7AKmHmiN9OVOmdPX66VAhiipq8IyEFXyS9wFykz1ylwu+dkKJQQgXe8827KH2+V87e3PhaonnvicFI41pNFWAiouLYbFYEBsb63A9NjYWBQXsW4hv2rQJ77zzDpYvX84Z74gRI7B69Wrk5uZiwYIF2LhxI2699VZYLOzm+5qaGpSXlzv8lKCsqo7VqdaZHScve/yubSf4fTHU8AGqrKnHHrvjIfj2n7C3tmy3k73xGSNVKj6WbTyOvvN+wtu/uo7SPIGrEa6usyBn/wWUVwtfScVXNBq/g4PCahfeIrJcGdUC5JxKi1HMFTJTdrUO7205xXscBCBMMXYOoVQL1ajEy9UE8lqAFCwXZy9fFTxAFJ5Wo9oapWGos8AqKiowYcIELF++HNHR0Zzhxo4da/u7R48e6NmzJzp27Ii8vDwMGzbMJXx2djbmzJmjiMz2vLflJF7+0f35Mw+u3oHPHh6APu2aK9bxi1n6KFWGe5b+Zjv00vmdAPDMZ/uw8Y8ivDG+j0OHOvubgyLeIj2HGIZR1geFJVvn/9Cw/HXed4fw4KCm6VulptvmfXcQ7285jdT2LZB9dw9BzwhpVLnM5k1O+Tx7HMD+lpJO0IpF7YJUC5BQGZUar3iaR0+u3YOfDhW6jcc2kOHzATKATyAb/FNg4tMkNBuWbTyOq7X1mHNXd9HvcHgfzyaJUrhsoA1hNbUARUdHw2w2o7DQcbqmsLAQcXFxLuGPHz+OkydP4o477oC/vz/8/f2xevVqfP311/D398fx48dZ39OhQwdER0fj2DH2M1tmzpyJsrIy2+/MGWV2nPU3C8/urQLmUT0prGIaG6mvOSxguen3+xosfVztopJN4vt/bdDYSG291eFAU4O2xw58sqPBHL71xCXhvgU899g6MDYLkNC8E7ITtB5xllppHyC98tOhhrZb6OHLYnyA9Kr0iYFviwQ5FL5380+5D+T8Xhc5uO8phk6qi6YKUGBgIPr06YPc3FzbNavVitzcXPTv398lfNeuXbFv3z7s3r3b9rvzzjsxdOhQ7N69G4mJiazvOXv2LEpKShAfH896PygoCBEREQ4/JfAX0dgr3fk6rNJRa4KJI00Nlhh1RLCn0RrTyOMf/45RLMve1UbKp+cqLzV2B5s++sEujwVgKyuejCDZPvsr6//AM5/u1bVFwFkydX2AhCHXNhLeRmOZk+uL8cXDVy6+3H1eJgncI9SfUYkqp+eFnppPgWVmZmLSpEno27cv+vXrh8WLF6OyshIZGRkAgIkTJ6J169bIzs5GcHAwund3NPdFRUUBgO36lStXMGfOHNxzzz2Ii4vD8ePHMWPGDHTq1Anp6emqps0ZZwXocEGF4MPj2PDESVdM5yJX+eWS18pwT4UwDIOc/YUo4dhnxROHYueT0H/Yz+53JpVZXx/A+icG62aptxCLHOBmCbsbCxBbHHywOUG/mnsUADBxQDt0S4gUGJMraq7ik2oBEjwFJqGuF1fUoHVUiOjnlEDYFJjj/zqpNm45Uczt1ylpCsxD1cydldb1ffK922horgCNGTMGRUVFmDVrFgoKCpCSkoKcnBybY/Tp06fh5yfcUGU2m7F37168++67KC0tRUJCAoYPH44XXngBQUHcZ7aoAdsU2L3LfmMJqfx8uNCdQRvuKwufo2DOgQJM+/B31nu/Hi1SSiRZOHbxCg6cL0f31gI6cR019vZlj8vE7ugELd2HzI/HKmpvvbJ/V2WtBeE8B1Da3qtAnh4uKMf/fbYPVU7Ks5iOzre6mAaELLpY/uufqLBz1lduCkzegrFkA7vrBaCsEzQX7gYkzvVVx4ZWxdFcAQKAadOmYdq0aaz38vLyeJ9dtWqVw/8hISFYt26dTJLJS4DZteLZn4huz8s//oGRPRNcrlfXWVFVW4/QQH8PfYCkPyv3O61W7k6SbzXbpzv5l3vqgeo6/o0D5UKp71lUwW5hc+c3IFSB55sVZosic+0efPH7OXz72EBhiqUIGIbB8aJKdIgO41TMHli1g3VqSY+rwNx9gZp6C+sxJgBQWF6N0qo6dIlrJq9MPEK9t0W8P4sUGvUfNaZY9TIzypdWuZ2gjYTmGyH6EmYRliwAmP7x79hzttTh2uCXNiB51jrR5w85I6acizrQmacGcY0C6/k0IB7EHD+gFO4UHKOY8e3hLRsCC44nU2B8sXzx16afbJu9ecobeceRtmgjsr4+wBmGq8yJ8QFyVyTst45oxO3ZfgyDM5eqBMtQUV2H7lnrOI+FSH0xF+mLf5Ht0E0p227IWXcqJW6GqgWeKiGi881+MEMKEKEUbBYgPkoqa22rpFzuXfGs87dvkOT0lWjsB9gUIa661aD/cPkAuV6b8eke5Oy/4HpDA9zv5yM+b7V2/uX3F2D5rm7E5WuQxVqAhDxne6/7IA68tO4IAGmWCDE+QM4LEJx3C974R8PUbr3Fiqc+2YO1292vSv3Pd4cwaOEGh2t86c8/XiJok77DBfLuiSamc5ZaDZxfcbG8Gt2y1nHe9zY88clTywdIL3oWKUAq4i/SAqRkP2gf94Vy+c60alSs2BQ3rvSInSdfu+MsHnpf4IomDxCS/VxTmI0IbfDFNMq19VZBG2pKRWwj6G50z6dgS/XH4PMdEsJPBwvx5kZu3w2xSF0FxoDBTS/lsd77bt8FfLrzLGYI2EX57U0nWOK2+5thHBRrodLKNTjSUqfPOeDYFullUYJW8B0662sWIF34APkKcu554rmm3vT8L3/I50zc2Bk6NzoAt2XDwjCcI3q+dOqhIVNi+bO7dE1asQ35fyp3tAtfI/jsF/uxZPx1juFFxmG1yzO+KTC+nDUL+fYsQVZtPoFDFyqwZkeDVaV32+bo176F+7gao5S5yFXVWDhXOHJZeU8WV6JTTLio9/zzvZ04fakK3zw2EAFmP9U7OjV2nheK9q2GsoheBWZ3U+yqWiFTs3rOcbIAqYjYKTB3eNKkiOm3xbynsUKwKTRc77RajekrA7if+nBOlhAl2N0UGJvyo1YH892+Cy5+MKwmd4G+YFLHBFIHE7O/OWhTfgCgQCbrJ5s0nk5lcn3Tqe+x++3w8ePBQhwuqMCuU5dRXl3nsOEnLzLVS/2oP8ZtazyDzwm6ickrt4uLVU8fVgJkAVIRMTtBK42YgrssT/hUQWOjzTay57LmSF0qqod2zK0C5JQPZpMJFgW6g8MFFfhmjzwbq7mTzjnNbo9ScfpQFgcFiMcCxLA/33BNrqkZ/bbgXAqQJ0ZHBsBtr/7qduq2EdnqmK6yWQ8tBzdqF0mPVhO7uX/04hVJB+GqBSlAKiL3rv+eFVzhD+cevig4bGPjzJZULj3no22nUWqg82PsEWsB4uq35ejQH/vod4/jACC+sxLpdGlfboWcDcVWzjmnTBVy7m+Kk+O9bGEFfFO+fLL3jZbz4E6hyg8go6Jp937hz8iTaM5vpiulTD6u1lmwdoej47x9Wp33sPIEdwOI0Us2u2w4qyf0Y5LwAdTcmdYdSlV+C48GxDWiXbLhOD7YeloZgRRG7A7ARj37yh7nPlFsUbIKtADxwZWPcpXrwvJqXBaxzQLbnlRKTYF5glilwgSgoKzaZaWa6PfqSNuw7QOkL7OUbJy5dBUzPnW0utin9MXvD3HeE0NVbT0qqvm3F9Cz8gOQBUhV5J579qQCK+Uz0tjQqbF6ZP+5Mlne4QkWN/no/M0FOe9qjNhyxdq58URhrzTyKYSNodiyTKriJJTUFxvOJzw5f6TgZ6rrLAgOMIt6D18qrErsoicyym0nLmHiim0Y0LGlR6+1jYv0X/w1RwnFzL6KbnRa9CJVOe2WtU7ygEMv+jBZgFREzrrPMMCBc/Lu0SEHjQ0dW78md6HnWj0jGwLkdT8F5pgRQpZva902iP1O7vpp5xTbh7fvEMuq6vD76cuC5OFSgOyDHrtY4VaJEH14q4ge3NMpMHvR5eoUxcby/taGPZF+O+7ZqkNVrS1u8t0LjLCyIuXLWK2MbpQYTyAFSE1krnjf7ZO+GaBShbfRssTWBkmxOum9jrl3gnb8n6vx1VObLNoFyJ0C5JQ4rmXwwxdvxOg3ms7Gk7IRov1o9vmvDuA/TuZ+NfF02seddbHxHctF7IotRCQlpqtU7Sw92JdKL+QduYgH392OizKtUuT9phK+jRLbf2gBKUAqoqeKp9SIzMozBSbljR/q3DdIialE/ZQSYbDlgf2VvCOOJncuH6DCcsc9SBrLKJsyLdSX6h2WDQLZ3qEVvNsF8HQyr+UexaXKWvx2vESUkickvQ5O6oJjFh6n6hhs3o1hGpaj/3ToIu+xLKLilCWWJvS0r5MnkAKkInqqh0qd3dgYr1wWIL3z69FivJF3THB4T6dF1EDs6e6rfjvJG8eOU47TWkIsGw2RNMbleotrKlFu65UnCPnWfGHs64uznIvW/4HMtbtxTsSKLrZ43L33shFXZ7rJ96bDUFWQxUMuchxGLCdCBwE/Hy7ELYs2Yt/ZMtGLP6S+U2lIAVIRHek/ihW/pikwFguQPsq8YIRW0oU5RzjvmUz6WgEjBHfSMvCsAROp/7DCPQUmWhx+GZwiFFOHhXx3vgN93SmKm48Vi5DmL5lkCiP6vTqqA3pqh9lQJv+l3bPnH6t24OjFK3hw9XaaAiPEI3ZPDb5GQ0zxYz2YVKEGqWkjRGFyeDsjX9uE9jO/x6aj/J2VnqyDcnwmfudekUuxVVgFxnWSupZF1p0Dt5XhV0Sl1nslLLVe0l8aFv5yIi6uyhqLMisUNYAUIBXRqpOb+t5Ol2tKNex8u/eeLGHvZPSKnD5bf39n619xGh8TPMsbe/O51IaZexWYuILd+A7nk9Sb4pMO14BHaN1z2AiRNR5lFhUIjXbxT3+IeK9yHealylrsPVsqOLwezhAUilKSHrc7TFnK3lCeWoAmrdiGQxe0X8VMCpCKaFXt1h8sdLmmhRO00RCTR/cu/c19IOjL0uMJ7vJGDpM73zukbIRYUOa6okbOWiD3t3VniWHAn17Wjl6kDxAfi386KigcIHHAJfCZ1Bd/wp3/26xU9KqjiKXcKcp/ebBrvMnkuZXQygD3L9/iURxyQAqQiuip81N6J2g9pVUNnB19PUHvM4WejqA9daAEGpyAxZrhp6xmP0T0fCm3I7FLZyQi6VxBhWafWwXITfLZOlIhHZcK/a+s1FnE+Wn5WtsEuOa//dFDUvbCksMHSA8O9qQAqYp+ap7Sfax+UmoMjJRfQkaofNYbvtVNju/hf4dYpXMfy87hDMPwnq4uztfO8X+ujlb4FJh9PrE/pMSqN0WO4FBRqXeWX+h5fN6MzNsAkQ8QIR45K17eEeEHlLKh9JJ0I82zq4uQpdEqiOEBb/7yJ7aduCT5ebnKXj3LXg5io2YA7FdoR3VPP6OQPkZKeuV4r3jU6zBf/5l/W4rG6Xm9LspQWyoh2bDTbrBhMnnPRoh0FpiKyNmvPfvFfo+eV7ru670TF4ISeSQkX3TaLtt4S8DOw3xpENp26iEbPPkWng8Cml6es7+ANcTCdYdFxXjwvABlT4GM12N/qUORXJCrHWUbLDThPifusfNxNEGeaWw9QBYgFRG9DF4hOZSMuzFepQ+rJPSN0GXwcpdD0c797vxonAKIKdUeqz92r37a6XTvRkpF+lG8ImDllpLWYT0sjtB706RE9meu3cN5r0rkie1+JhMpQIR4dFXvlJ4CUzR2dVDTzOtLU4ZCG09Jy7xl/mQbDhe5D8SB1p9UalYoebyLHnYA9p2a1gTfhptiD7o1mcgCREhA6wbRHqXLr57Sqid8JltEODev2nwCt7/+K2cUYhRR0U7Bbp546H3XPbSEo+3XtlgZfPn7OVwoE3dchpf0bQCA97eccjkPzpcGG8rgPRYg8gFSET2YfxtR2gGQGhlCCAzDYPY3Bznvf/n7OUxfs1s9gdzAV66FrgITiqdV9O1f/8S6A4UICzSLe68OrDSA59OjdRYrnvvS1VfShIatD/4sqvTwDQphl3A99Rn2eMu5jmQBUhE96QRK78Ggp7TqCUFO0MqLoSh1FitnJ8owjPCOnYFo5UesYq/oYagc1wUfBusheUcapu8qRfp4KOn8r2aHztlJm4AB839WTQ494skA2M9k/DaqEVKAfJQfWXaHlgOGadgj4oBCS4sJ/fPyj0dkOk9Mgg+Q569VnNdyhe+grAVCRvc19eKUqkbEfFMaQ2lHTb0F1XUN37jMabBsxAOeuSAFSEX0ZBWpldiACWHpxuPYJOGkaq2orbdy7hIsN0JGwDoqJpL4aOtpznti2k012lglX+FJff/5cKFmU1FC8v3Nje63QvBYDoXi1Xv9UuO7803lWq0M+s77CT3n/Ig6ixUvfOc4RW2CyRADDSGQAqQiYs2/SnYAztvHy8kbG/g3ItMbX/x+lvW8NDWxLxlGb1zKq+t57wtt4KWUf7ZnrvJMAclZx1yXzEvvav+xSh2FnA0hFqDtJ6VthCk1Tw6cL8OnO896jeVBz1TXW1BRXY/aeisKy6ux32kH9YLyaox5M18j6eSFFCAV0ZMF6PQl5U5mj24WpFjcSnClRjlrmDN6KgNKwtVNydl9sXamLC94VcYpJzHfT2snaKnPa3VcBh8jX9uEpz7Zgw0Cd8BftfkEZzoMtUBDA1Gd841tTzclB9BqQgqQihip3kmH0b2J2RmOg8UJhbBvYD05o0ioJelIAbc/mpA4GIbB2u1n8O3e84Le1/ScqODyI7FcKyP3X8dPeKgCH7pQISjc7G8OovhKDY8k+kXrcvPV7qZybjKZ4OfFWgItg1cRvS5plBtDjbCg7q7VxsoZ6XAe3qlw636xolpUeCHiFJRXY8ZnDTsxR4UGeBSXERCmFKogB8Pg2MUraB8dZrsmpqpy7XBssOZJEc6Vcu8N9e8v9jn87827+pMCpCJeXI4cMFoydfFd9CCDjPD1j4JXwYvsZfOPl2Dc8i0u1z1VyMuuNq2CUfMzeaxj6HAKTMwg8Mvd5/Hl7vMY3bu1pHdxr4LXd2WzF1trSU0w3oBWDF5s3NIf3luMnDBQQi9V1qpawYW8yxscPbmSICZl7sI6d2Sv/6zM8nL7cyTFya/td5Ty/nqLFZ/vOiu7LJ5UsS9+P9cUj4jGhUtJ8+L+XBG82UVAFwrQkiVLkJSUhODgYKSmpmLbtm2Cnvv4449hMpkwatQoh+sMw2DWrFmIj49HSEgI0tLScPSo9ntv+ELFM1rfXVNvUU1fK7tax2t69kU8URKcn+Xy+eBTKIW8XfCqNUGh9M0HW0/jtZ/dr+IUW88ZBnj0g124WqfegoN9TquXCPGYTN49Baa5ArRmzRpkZmYiKysLu3btQq9evZCeno6LF/m9/U+ePImnnnoKgwYNcrm3cOFCvPbaa1i2bBm2bt2KsLAwpKeno7panH+A/Ig9Dd6YTarRqotaFbzXnB9VeY+eYRjhFi6xnWylhNV83++9IKsclytrDX1OUr7AgzHFfpviKzX4bp/7vJaTGZ/uVfV9cqEnC7AJJrIAKcmiRYswZcoUZGRkIDk5GcuWLUNoaChWrFjB+YzFYsH48eMxZ84cdOjQweEewzBYvHgxnnvuOdx1113o2bMnVq9ejfPnz+PLL79UODX8eLEibVj8TPqo4Hr3SzACXB0H37Rj/p/uO3yhq9aOX7yC3i+sx9+W/eY2rBB01A+6YNTBGeDdPi1K4M35pakCVFtbi507dyItLc12zc/PD2lpacjP595oae7cuYiJicEDDzzgcu/EiRMoKChwiDMyMhKpqamccdbU1KC8vNzhpwTeW4wcMZLJtMHJT2spfAfGqevk7+T5O1lnpZHvjK2128+4F44De18SvrLS6Kuy63Sp5HfJiRQFSmhd0Eo5k6OuUnUXTsMUmNZSKIemq8CKi4thsVgQGxvrcD02NhaHDx9mfWbTpk145513sHv3btb7BQUFtjic42y850x2djbmzJkjUnrxeLMm3QgD4ykUZH3RJ+46WWcrBN/M0wvfcp8471YOETJxPedtGDltem+fHFaBCZD1tld/xe294hWTx0gDWrFoPgUmhoqKCkyYMAHLly9HdHS0bPHOnDkTZWVltt+ZM9JHi3x4bzFqwsow+KPwitZiCMek/wbRm5BTgZj7jaNSw+c70cqD3cnV3vW4ET1PM2nlpyJHVfW2+n7wQjkW5hzRWgxDoqkFKDo6GmazGYWFjucwFRYWIi4uziX88ePHcfLkSdxxxx22a9a/1qj6+/vjyJEjtucKCwsRH9+kFRcWFiIlJYVVjqCgIAQFKX98g9iKp2cfAC72nCnVWgRRNDj5ad8i6kAE1bAv13UWq6BwbBwucNwVmMsC5GnW2svB951W/XbSwzc5Uu/hcQNKNh8GbJoICVRU1+M3gY7xRkRTC1BgYCD69OmD3Nxc2zWr1Yrc3Fz079/fJXzXrl2xb98+7N692/a78847MXToUOzevRuJiYlo37494uLiHOIsLy/H1q1bWeNUE1+YajFaGk1kAdKUnP3s09IAsP4g9z02uCw1Fytq8Gdxpai47LG3dqh5mv3Xe8QdvaEmxvYB0neF19PA94vf5d8TSk9ovhN0ZmYmJk2ahL59+6Jfv35YvHgxKisrkZGRAQCYOHEiWrdujezsbAQHB6N79+4Oz0dFRQGAw/Xp06dj3rx56Ny5M9q3b4/nn38eCQkJLvsFqY0vdLRBAYaaVQXg3XPc+qSpha/lsQB9udu9AlBaVYuo0EAAgJXDBOTpfjBS+qOdpy579E6tEOwErawYnMihvFB1F47elUVP0VwBGjNmDIqKijBr1iwUFBQgJSUFOTk5Nifm06dPw0/kaWwzZsxAZWUlpk6ditLSUgwcOBA5OTkIDg5WIgmEHUH+Zq1FEAXD6K9BLKqowWu5RzHm+kStRZEd59Gtp6PdRev/wNy7ussSFxf2ipVQ35enP9mDDq3C3Ac0KnoyU3gZ9r5fWisgemsb5UZzBQgApk2bhmnTprHey8vL43121apVLtdMJhPmzp2LuXPnyiCdfHh7YQK8e8mkkthn2+GCChwuqMD3Km8cpxZy9p0llbW2v/mWwXuCfazl1fWCnvmzuFJzBUhJR2XNLEC0DF5VvH3lsvHmKwyM2MJEYyzlYcDodgrM2cnXG3Be2SSng6VSq7WkxqvmsQ9qY2gDkE7rO6E+pACpiNhqZ8RGxnAiM+QD5C0odQLF/cu3SnrOaEdivLnxOGrrhZ57Zqy02aP72i5w1SHhObqYAvMVxBZmrfYf8QSjNfpG3LjRyDCMckqyns5Q0gNicyP7B/bNZ1njNnBWU30nGiELkIqIdWgzogL0yAe7tBZBFAyjD78lapSlYZ9tRlO+jYwBmyaCcIEsQCoitpMrrapTRhDCAW939NMTDOTtPO2jIv3HESWVFD1k9YniShyR4Cen9ylvPeRtI/rOKc8hBUhFvL0wGREGDH0XghCJZkdh2CkvQ1/OkxaHTLL4Au9tOaW1CIpCCpCaUM3THc9/eQC/Hi3SWgyfgWEYWTtPqlKEWHRuANIVl+y2mfBGSAFSEa03tSJc+elQoftAKkBlgzASmh2FIUccOteAhJ49R3gOOUGrCBVmwtfRk38DIR1aBq8NtNJRXkgBUhEjVzyCkIOcfQUG7jrF4c19laEPQ9V5Q2xk5dJokAKkIno3vRKE0sz4bK+s8VGd0gYjbtHRiN5XgfFh4GzXJeQDpCLGrXaE0hi4TRaNrMvgddwj6Fcy4yKPD5AMkWiAxcpg95lSrcXwKkgBUhGjVjyCICTgxRqQkZOm9wUHDk7QdrIu/ukPvP7zMQ0k8l5oCkxF9F7xCMJo0BSYNmjnA+T59zZqkVm28bjWIngdpAARBKEqvuLk6SvpJNRBx7O9hoUUIIIgDMv50qsY9t88Xe5Y680dlp59r9yhdydo+5y1F9W4Oa5fyAdITfRd7whCHWRsyXeeugwAeP7L/fJFSrjFyOeu6Vz/gZUjc42sdOoVsgARhA7Qe6NMiMebuyutpvca60nxlRrJcehdefto22mtRfAZSAEiCEJVdN7/yIY3j9i1TtonO85KfvZoofgT5NVk79ky1uveWJpyNT6KiBQgFaFRPkH4Dt7YYTWi9Vlgfh60pXUWY34ZrZVOJWicwtYKUoAIgiAU4PfTpVqL4LV4MpisqbPIJ4gC+NLqQa2NAuQETRC6wHfMg944kvU1tJree/6rA6ipt3q0p1pFTb2MEsmP1aq1BL4DWYBUxHe6OIIgvBktddh53x1CWJD3jt19ygKkca9IChBBEKriSw28t1Jn0dZMYfFiMyJb0grKqtUXRAW0ngIjBUhFaNt+giC8geIrtZq+v67ee+eJnPUfq5XBYx/t0kQWb8d77YgEYSBINyYI4Xiv/ceRwvJq9HvxJ80VTqXQutkjBYggCFXx4tkLQiW8eY+lqtomJ+0/Cq9oKIkKaDzyoykwFdFa2yUIgiD0zf5z5VqL4DOQAkQQhKp479idUIt53x3SWgRCBrQ2CpACRChG++gwrUUwDFo3BARBEGqjte8jKUAqovXHVptOMeFai0DoEG/23yAIQji0DxBBEARBEITKkAKkIlpru2pDA32CIAiCC61nRUgBIgiNqbdY8cmOs1qLoRqkFxMEAWjv+6gLBWjJkiVISkpCcHAwUlNTsW3bNs6wn3/+Ofr27YuoqCiEhYUhJSUF7733nkOYyZMnw2QyOfxGjBihdDIIQhLv5p9CrcZHCxAEQfgamm+EuGbNGmRmZmLZsmVITU3F4sWLkZ6ejiNHjiAmJsYlfIsWLfDss8+ia9euCAwMxLfffouMjAzExMQgPT3dFm7EiBFYuXKl7f+goCBV0sOH1uY+Qp9s/bNEaxHUhUxABEFA+z5RcwvQokWLMGXKFGRkZCA5ORnLli1DaGgoVqxYwRp+yJAhGD16NK699lp07NgRjz/+OHr27IlNmzY5hAsKCkJcXJzt17x5czWSQxCCOVlcidp6K+kDBEH4JFqfjylJAVq9ejVqampcrtfW1mL16tWC46mtrcXOnTuRlpbWJJCfH9LS0pCfn+/2eYZhkJubiyNHjmDw4MEO9/Ly8hATE4MuXbrg4YcfRkkJ9yi7pqYG5eXlDj9CDqhr52PIy3m4f/kWrcUgCILwSSQpQBkZGSgrK3O5XlFRgYyMDMHxFBcXw2KxIDY21uF6bGwsCgoKOJ8rKytDeHg4AgMDMXLkSLz++uu45ZZbbPdHjBiB1atXIzc3FwsWLMDGjRtx6623wmKxsMaXnZ2NyMhI2y8xMVFwGgjfJDIkQJZ4dpy6rLkjoNowpBgTBKEDJClADMOwmq7Onj2LyMhIj4VyR7NmzbB7925s374d//nPf5CZmYm8vDzb/bFjx+LOO+9Ejx49MGrUKHz77bfYvn27Qxh7Zs6cibKyMtvvzJkziqfBV3j+9mStRVCEwde0ki0uUgcIgvBFtPYBEuUE3bt3b9uqqmHDhsHfv+lxi8WCEydOiFptFR0dDbPZjMLCQofrhYWFiIuL43zOz88PnTp1AgCkpKTg0KFDyM7OxpAhQ1jDd+jQAdHR0Th27BiGDRvmcj8oKEgXTtLeSN923ul7RbsZS4eyjiAIQPu98UQpQKNGjQIA7N69G+np6QgPbzrqIDAwEElJSbjnnnsExxcYGIg+ffogNzfXFrfVakVubi6mTZsmOB6r1crqk9TI2bNnUVJSgvj4eMFxKoHW2q7aMIzvpZkgCIIwBqIUoKysLABAUlISxo4dK4vVJDMzE5MmTULfvn3Rr18/LF68GJWVlTZfookTJ6J169bIzs4G0OCv07dvX3Ts2BE1NTX4/vvv8d5772Hp0qUAgCtXrmDOnDm45557EBcXh+PHj2PGjBno1KmTwzJ5gvAEMmJIhyxABEEA2g+QJe0DdPPNN6OoqAht2rQBAGzbtg0ffvghkpOTMXXqVFFxjRkzBkVFRZg1axYKCgqQkpKCnJwcm2P06dOn4efX5KpUWVmJRx55BGfPnkVISAi6du2K999/H2PGjAEAmM1m7N27F++++y5KS0uRkJCA4cOH44UXXtB8mktrc58WeG2aZezE1x8sdB+IIAjCy9C6d5CkAN1///2YOnUqJkyYgIKCAqSlpaF79+744IMPUFBQgFmzZomKb9q0aZxTXs6Oy/PmzcO8efM44woJCcG6detEvZ9QDq01fIIgCIJgQ9IqsP3796Nfv34AgLVr16JHjx747bff8MEHH2DVqlVyykcYGG+e6aCl3NKhnCMIAtB+gCxJAaqrq7NNJ/3000+48847AQBdu3bFhQsX5JPOy9D6Y6sNrZQiCIJo4p83ddBaBF2htYuEJAWoW7duWLZsGX799VesX7/etvT9/PnzaNmypawCEoQeId1OOqQYEwShByQpQAsWLMCbb76JIUOGYNy4cejVqxcA4Ouvv7ZNjREE4L1WL+rDCUI+vLWdAIAxfZtOFtDa4qE3tP7ukpyghwwZguLiYpSXlzscMjp16lSEhobKJpy34WtFPyTQ7BMV/qV7e+LpT/dqLQZBEDrkhVHdsWZHw+kC5DuoLySfBm82m1FfX49NmzZh06ZNKCoqQlJSEmJiYuSUjzAwz430zmMwAMeG7G996ew4MVAXQDjjzcOkQH/J3azXY8jT4CsrK/GPf/wD8fHxGDx4MAYPHoyEhAQ88MADqKqqkltGr0Hrj602CVEhmps4CYLQP77SNvqCRdxISFKAMjMzsXHjRnzzzTcoLS1FaWkpvvrqK2zcuBFPPvmk3DISBsZb2zXyAZIO5R3hq9AUmCNadw+SfIA+++wzfPrppw6Hj952220ICQnBfffdZzuWgiC8FWrGCMKRsEAzKmstkp7VuiMktEHrAbIkC1BVVZXtqAp7YmJiaAqMB1+s5GTyJVwh9dEbuTY+QvKzWneEhG8iSQHq378/srKyUF1dbbt29epVzJkzB/3795dNOML4eGvDRtM4BOFIi7BALLqvl9ZiyEpYoFlrEbwarbsHSVNgixcvxogRI9CmTRvbHkB79uxBUFAQfvzxR1kFJAiCIIxB1zhpVqAGSzGNKnwNrZ3fJSlAPXr0wNGjR/HBBx/g8OHDAIBx48Zh/PjxCAkJkVVAb8JbrSF8eG+SqbGWClnPvBOTyQMnX+9tKAgdI0kBys7ORmxsLKZMmeJwfcWKFSgqKsIzzzwji3CE8fFFpY8gfBVSbgkjIckH6M0330TXrl1drjeeEUawo7W5j5APb2zoI0MCVHmPF2YdAc8WPFDLSGiBJAWooKAA8fHxLtdbtWpFp8ETTnhn0+aNnXjzUHUUIMJ7aREWKOk5GhsSWiBJAUpMTMTmzZtdrm/evBkJCQkeC0UQhPfijdYzooGEqBC8Nq631mJIIrV9C61F8Dm0VnwlKUBTpkzB9OnTsXLlSpw6dQqnTp3CihUr8MQTT7j4BRHG4OtpNyoSr9YFXCkYL+zFvS9FhJo01vU7exlzENymuQoHeVMl0xWSnKCffvpplJSU4JFHHkFtbS0AIDg4GM888wxmzpwpq4CEsfFS/YcgCBnRw4ap9VarR8+3bRGK05doI2AxaP3VJVmATCYTFixYgKKiImzZsgV79uzBpUuXMGvWLLnlIwhd4o0DObWMWnQeEuGMHizFFqtn5TLvqSHyCEKohiQLUCPh4eG4/vrr5ZKF8EK8deWbF86AiVJMnk7vgohgfzz/1QEFJSII4+Dn551tnTcjyQJEeB9KdehGbhL6kVMkJx1bheHOXq0lPeuNyiPhmRVHD+0EFUvfgxQgAgAQHuyRMdAr+c+o7pz3vK2xfGdSX61FIHwYb7UUE/qGFCACANCxVbgi8bpr16andVbkvXLgK21y17hmGHZtrMM199Z8kz6G7YRuMPxGiCqMarxt4OQxGjeypAARiuKuUQww67kIcsvulcvg7ZKUecs1iI0I4g0vte3yvpwjCGHoQtEjbOi59yEIGzd0UN8fx1csQI0463Tz7+7JGdbX8oYQgCdlgsoToQGkABGK4q6jFGpJCQkwyyCNOHy9TR7aNYb3vtT88UbrGeEZvlLXqOTrC1KACE0R2hdq4STpx/NOX+/DfaXDInwHd9tAvDi6h8fv8NZ6EyFxEY3W+UEKEKEpetYjjD7N8+MTgyU/K0ThpJU7hFwYoSzd00fatg8O6D+ZkjDqHkikABGKYoB2TRJG2M1YqZV9BOGLyHJch/6bDV7aNA9hvc5nLdczpAARiuJuZKf0VFJsRBBeGdNL0rN8DZ4RpsCUHpQZs8kjlMIjH2gdFCZ3dVoOGQ3QbPCyYjL7yQ/u2hq9bndCCpDO6dE6UmsRPMJdxVDakrL132kY3buNpGf10Ch7gthpBbWck42gPBLqYoSqxifj7DuSHf7vEtsMzYL88a9hjh2/EdIpBXdtTeeYZipJIg5SgHTON48N1FoEAECHVmGSnjNrbAHyBD7R9Sy3VMQmyegKIiEvRvDj8QS+9HV3GqgmRYdid9ZwZN5yjdJi6QJ3gye9ugyQAqRj5Fh1IBc/PzlE0nNup8CExiPp7Z7h7Q26Vui1MSS0Qw91ze0UmMj4zCzmb28t+dV1VknPaf3ZdaEALVmyBElJSQgODkZqaiq2bdvGGfbzzz9H3759ERUVhbCwMKSkpOC9995zCMMwDGbNmoX4+HiEhIQgLS0NR48eVToZshMdHqi1CB7D1ghIZSXH/LMW6KUTH3xNK83eLYtTKOE1ePs+iFp31nqAKwviI4NVlUMuNFeA1qxZg8zMTGRlZWHXrl3o1asX0tPTcfHiRdbwLVq0wLPPPov8/Hzs3bsXGRkZyMjIwLp162xhFi5ciNdeew3Lli3D1q1bERYWhvT0dFRXV6uVLFlQu4u9rUec7HG61X9EzCUN7RqD8CD1Dm01QnuX0sbVR6xDdBj2zBruUbxuN7AUEEYuggM0b6YIhTGCcsFnpXJuxYw4OHhrQh/Jz4YEqr9RrRxo3rIsWrQIU6ZMQUZGBpKTk7Fs2TKEhoZixYoVrOGHDBmC0aNH49prr0XHjh3x+OOPo2fPnti0aROABuvP4sWL8dxzz+Guu+5Cz549sXr1apw/fx5ffvmliikzHovuS8F7D/STNU53+0PwqT9sHV9UaICHEgnHqD5ALcMDESkinxrTIjZNUvNA7HOJzUOlvciJoV20s5YR+ofNqtuqGf95eN5Es2D3bQZbmxjkL12N0FpR1FQBqq2txc6dO5GWlma75ufnh7S0NOTn57t9nmEY5Obm4siRIxg8uGHTtxMnTqCgoMAhzsjISKSmpnLGWVNTg/LycoefHlC7kw0OMGNQZ3k7CXf7Q4hN4/KJfW1/J7WUp2PkgncnaEXfbAzUmgaUa4+RUBWth4R38PztyRjSpZVDu8NGYzsW9pclZIgBlW1PDjc2nr2rAU1bhOLiYlgsFsTGxjpcj42NxeHDhzmfKysrQ+vWrVFTUwOz2Yw33ngDt9xyCwCgoKDAFodznI33nMnOzsacOXM8SYpCqNPBDHNz5pMneLIMnm10cG18RNN9he3mRq3Uki0zIsubWhYguT6zUb+nb6DPr9OqWRBWZQi3im94agj2nC1TtE3VFhM+eDAV49/e6nRZn9/PHZpPgUmhWbNm2L17N7Zv347//Oc/yMzMRF5enuT4Zs6cibKyMtvvzJkz8gnrAWpZgDKHK7dUU24LkG4wqtw82H8LIaZptbJALguQHlYaeTP+Zun56y2fJiYiGLckx+r+aIj7+rrujSZU4hs7RTteMHBbqKkFKDo6GmazGYWFhQ7XCwsLERfH7ZDr5+eHTp06AQBSUlJw6NAhZGdnY8iQIbbnCgsLER8f7xBnSkoKa3xBQUEICvKduV5nlNzGXMm4pcac1DIUJ0uqlHuBxqjVHgWapY2fxMrnZ8hhmu8RGaKef54SeDIYU2sTUblgG+BIHSB0igl321RyZY/Wiq+mTUtgYCD69OmD3Nxc2zWr1Yrc3Fz0799fcDxWqxU1NTUAgPbt2yMuLs4hzvLycmzdulVUnIQ8uFsGz9ds2E/JsIaTWHlynxyCFmGebTGgl2XwWsEwQKBE58czlwQon3a420xTKAbVZw2DJwqQHr6NL9VoORWPNz1YPaY1mo+tMjMzsXz5crz77rs4dOgQHn74YVRWViIjIwMAMHHiRMycOdMWPjs7G+vXr8eff/6JQ4cO4b///S/ee+89/P3vfwfQoMVOnz4d8+bNw9dff419+/Zh4sSJSEhIwKhRo7RIomS8oUK6swRbBY6c5Bxhmf3YJ3hu7hqD529PZrnDJo/w9z06tKPwwDLQLNi9YTft2iYfBbWVuVdzxe3JJdfUldajTW8nLFD6hIJev43chh29WIrY8lvIN3AOc39qWyS2CMWLo3ugGc8ig+SECM57WqL5sogxY8agqKgIs2bNQkFBAVJSUpCTk2NzYj59+jT87GzglZWVeOSRR3D27FmEhISga9eueP/99zFmzBhbmBkzZqCyshJTp05FaWkpBg4ciJycHAQHG2uzJp3UFY/wpPOyV1Pkzgo2sbLuSEZIoBkvfHvQ5f2e8PCQTliy4bgscfFxc9cYXK6qxbxR3XnD/TpjKBJbhCLp/75zuG6fx3rqkORyp9BRkrwSPZUZKajR3tq3h4H+fqitl7aDsgySsF4JD/LHlZp60bEkJ0RgT9ZwdPj396zhOrYKxwMD2+OdTSckyKocmitAADBt2jRMmzaN9Z6zc/O8efMwb9483vhMJhPmzp2LuXPnyiUiAeDu61rj813n5I2Uo9FJ7xaLjX8UyfsuB9gaAJNgnyUxbaVa/cLIHvG4p4/7g1+F7G2iHxdoeXcTJ/SJ1vvBNOBapoUqdVJqw+p/9MPYt7ZIeNJz2PZTM5mALx+9EWu2n8byX9kVFefssM8fd47fvdtGiZRSeTSfAiO4UWtqQujIJ/vuHvjwwVRlhQFwd+/WeHVsb4drbDJ6tPU+hwlYCadto4yM9WpxlG8KzCAfwqB4ZO3VwadhK/9KToFpmeSx1yeyXu8UE45nRwpzA/AGSAHSMXrrkIL8zRjgvATSQ9iSeEOHlggOMLsN567B7chzgv1NHGdo2Q9i+Obr9TKXrxQ3dGiptQg2aAqM0At5Tw3x6Hn7NktLhTzI34yF9/Z0uupeHiky39krAQAQwLJqVOs6SQqQjlGri/WkHj55yzV4dWyK5OetVvlT+fcb2uKVMb2w9p/cq/6eGdGV9bp9BedfoSYcfZj3mxDyvXslRvHeV1P/oykwY2D0w1CFFOmkaPZBlRHHQ3zTWXJxb582eG1cgzX/5q4x6JfUApMHJMn/IomQAkR4VHkfG9YZd6W0lk8YDuwtLh3+suzc3jOeNWywvxmje7dBy3BuX5dQjsP77PtavhVq/7q5MwBgdG/3adeDeV8YwguCAnorJ4157TGG+Q6+h5GmJz1Z1akXy7GU7B7YKVr08UP27WmA2Q9rH+qP50ZeK/7lCkEKkI7RS2WRE/ujLABpVq7PHhqAtyb0waNDO7HeF7ILK9d77Rtivk5+aNcY7HguDYvu6+X2XWoh+SwfCR+h3qre6pXUDi3Ro7XrqfdisbfEse2ES3iGljqMHIeWCm1vnxreBZueGeqwlYTRYPtU7j7f+w+muiiqUqzb9nEouVGuEEgBIlTFudFgbXPc1InmYYEY3i2OdU5ZwOOcmP1Mgn2AACA6PEjzkSuDpumqoV2ENchcjZYYRcjyl3YYG9HQ8ZhM/D5XnhIRIu+C1SB/dgsgQdjTjsXiYTKZ0KZ5KJQyKb7gZhsLuXCu7kq0Ze4UJK0Nf7pYBk94N3ERwSgorwbgWsmErnQTZaXgqVRsxzcMvqYVokICEB8ZjOq6JsuGXAY4pSv5Fw8PQE29FSEc03pKUP+XAvTFIzfi+30XcN/1iRj7pnJLeuXwo7L/Dlo3vIQjevgeztU976khiAoVtmO8lBW7XGnu26656LjEv1zQJffRePjdtB5AkgWIUJw1/7zB9rdzcReqZIhpYPjMqmym8vl398Br43rDZDI5VGi9nl9jjwkNU35qKj9Ak/N6QlQIHhzUARHBAbrfudzE8TchD3JvS6E29vXd7GfidHhmf1h+eZTEBJMqMl/fvgXLu5vQen0DKUA6oXNMuMs1tVyAlN5vyH70roafipBX+NvVPPszrfwcfIDkyRclV4FJkbAxiSP/ciJ/eEhH0XHVszhIKemzFhYkr4Kn9ciTcERvKyU/UGG/s2COaVh1dqR2bfflrhJB/n64280iEa2rISlAOmFlxvVY+8/+iLZbueSNB26q0dDxVarGTjo4wIyZt3bFk7dc45DnAeamh+Mi2Y9O0VdTLZ3XxvbGrzOG4u7rGhyCHxjYHoCrnxYbFjWXgQGYdUc3Fwd6Ql8YfiNEu7+F7YPlWR3o3lrb8uysaElyaOa5d1dKAuuCFIepaI1bU/IB0gmhgf7o176FQwfsLfD5XrBZDZw3QRT9PoGV6p83uS5nNZlM2D8nHRYrI0qOO3ol4Js959nlUfCTehK12c+ExBZNTp4P39QRAzq2FHRwIZsFSAlahDX4YLSOCsEPjw/CpBXbZDkixRtXWBKeoXaZ0NIKyboKTGZxhKzw0lrxJQuQTmArB1q10Sszrsff+rRBHwWc8Vx8gJz+H9qlFW7tHufynJi88HReOTzIH5EhrmflNMLWcL38t574eOoNLKGVtRhJmgLjuO7nZ0Lvts0FrZBi28BSifKa3s2xLGjdYBLc+PK3EVr0YyP0cSC3yWQSLPM1seG2zQzF0DyM3YFcL7thA2QB0jVaKUBDu8RgaJcYTHhnq+xxu1qAHP9fmdGP9Tkx04FaVKogf7Oujo9QGjYLkFw+U4Qx8WH9xy0rM67H6ZIqQdO4ejsD8scnbuK8x9fWPjLE/YaRWpcZsgDpBLZypHV3knFjEgBgUGf5zv9SYxm83kaiWo9ylMDCshHifX0bDlhsHRUi45scPzzpWDrGC8u5UHq04d+oc2iXGEzS0REQJijrBH1nrwQ0C+a2ojdCGyESmsPVqdzcNRabnhmKlZOvl+1d9uV96fjrZIvX4R0yjiv+71bXM8PExq63bkEOhSw+0lXJ+cfA9vhoyg14X4UVNFJgOP4mCMAz5TpCQGff8A59lDyl9Q6h8WutM5MCpBPYOm09VJY2zUPhz7HjslC4vP5vSY7lbXTs74nJCTn3lnjopo5YmSGfAugtjGJZ3mr2M6F/x5ayL1m3R+sGk+DGo32AZJNCOt646pYPOVaB9U1i9xMVGpPW3518gPTCXyVBaqfv0atVLIUuPkBCH9RwCszZ4Vds/N7YafOd0K7ovkceVAov/AyEhkgpi3qZDjfB1QlajGgbnx6CfefKMLIH+4HUQtOpdXaQAkSo6lfhXN5jm3GvipBaOfgqn5SkerriWy+NnlroNbkOU2C+NdgnBKBGmdCDVR/AX05A0gd27VqGoV1L7p2yhU+BkQ8Q4eU4Lnt0vDd1cAdBcWhpnnZe3fTYzZ01ksQYyNmkad1fjOuXqK0ABkKviq9QtC5raqK4D5DAVkDrIkMKkJ7xwgrpeCxGwxlWQjoZcfsA8U3PiMd+1PbD44Pw2M2dJMRCyIHanew/bmyv7gt9FK0tAb6IyxSYjOoIWYAIj1HC6jEqJUH2ON3hL9ErWeqIjO91nk6BXRsfoXml9QR/P5PiBxAqmT++NEp3x/DkWK1FcEDrYw08RS9O0KqcBcbyHjmrrdA2RusSQwqQhrCduaRkJTT7mTCgk3x7+gjF/qTyOpb9Y4QgJFfG9WuLNs1DMLZfW0nv4MLIG/xFhzftxtohOgz756QrrsBp3agJQWg907Ouy7ZFAyEdA1dzTrgUEaWTKlQZ9tNYAyEFSEPsfUlYN0JUopRqUMlD7c7Uqq61CH7OPk+EOA/Ouj0Zv84YynuMhRRUPvdTVjY9c7Pt79Ags+Rz1oIDhDcVelQavn1soNYiyI7eLJE6E0c0Yqu5EZqF3m3Zl6kzjGubKufnoykwwi3uvr2nFSw00LGz06qo2e8jdLWuSQESI4+QvGDAKFKhdLNyQwKeHizbSKtmQYhpFiQorJJTIVI/b/fW/Dv18rxR4nMEoT0v3duT/QbD4gNkV9RbcJzjJRTBCpBHb/EcUoB0RozdsnBF+l2NS5xcHTIbSm2rbuQpMLlgGBHKh4JlTO1PoWerht5E05s8RuTRoe7PzxJDyzD2QQsDhqUuNX1BIed48ZHYIlRQODoKg3BAyqm7XOipQVpwTw9MHpAk6sBQsZ2dUsqVRLcln0XPSkMj3qDT6i2fPZFHF0nRsEx0aBWGLTOH4anhXeSNmMsHSKG0vvdAP0wekCR49aTWZZg2QtQJjeWgfXTT5lKNjpq/zhiKY0VXkHf4It7NP6WBdJ4z5voGx+TfjhVLet5dhe0hcIpDSsXXswVIj6J50z5AuuiYDYLRV4FpiQlAXCT3prCS4+X4JKFBZo92guZiUOdWGNS5leDwWpcZUoA0xN3Hb2z8E1uEIrFFKPafLRMVvw77Rg7cVwIt06JnBUgIQf5+qKm3or8I65szDKN9Y0U4Qt9DXvSyDF5OnEvIl4/eiMiQAAT5mxV1ghYKWYAITpTQ0Fnfo1G9F5UeN0Iq2Xi14JhH1wNC8vCnzJuQe6jQZoVTGq1XdsiJN6WF4Mfg4xxWnMtv+5ZhiAyVd5WsJ2hdvUgB8jW0ruQGVOKGdY3BAwPbo2cbqSuJlENIuhNbhGKyijsaG0Fl0LoayAFb53Fbjzh8v69AfWEAwx8HL3oZvIRGh+sJpRRtMbFqoexrbcUkJ2gvRvCiHR00Pu5w19QoqQD5+Znw/O3JuCultXIv0SmdYsIBNHSswvf2UFAgD5B0erf8YihKDM/hwrpGBxpp94QIrUWQHee6aDY3XXDZCVoFeZzRuq0gBUjPuJzWK0NpUWvDRZkxgoxaoHQDsvaf/fH6uN54Kl346hQ5R3VyTG1OT6PDa9XAaMqiM0+P6IrHbu6E7/81SLF3CMkjtrbuluRYjE/ln8K+r28b9GwTiQcGNll77etiq2ZBCA9qmvRxrlv2bcmQLg2OzM2ClJ0kig7X1r2ApsA0RGvtV2uaFDrPOzlf1Y+UVgxbhAXijl7izo/TU7l+Y/x1uK1HPABpctk/06FVGP4sqpRJMs/RUz57jA7SEh7kjydFLENPSWyODUeKRL2DcwrMzXMBZhOaBfP77nSNi8DCe3th658leGfTiYZ47SJ2XprO13Z0immGX2cM9XhDRC6WT+yLoooam4VZK3RhAVqyZAmSkpIQHByM1NRUbNu2jTPs8uXLMWjQIDRv3hzNmzdHWlqaS/jJkyfDZDI5/EaMGKF0MjyCzbrjjZ26VOuAN67QIJTHvrTZN/hSFMdX7kvBlpnDPJbJW/E1h/GHhnTAcyOvxU+ZgxV/lwkmwW1gEMd+aO4+j3PbnNgiFGEKWYBuSY7F/W4sWmqguQK0Zs0aZGZmIisrC7t27UKvXr2Qnp6OixcvsobPy8vDuHHjsGHDBuTn5yMxMRHDhw/HuXPnHMKNGDECFy5csP0++ugjNZIjipiIBvNfoNnP4bysRjwd3c++s5vD/84VoE3zEASa/dA5VlstXAhz7+rOe1/vx1U8dFNH3NYjTmsxPEIP3dvWEyWiwnvaJ9t3CmY/kyJ7tUjFE4Uj0F/zpt/wBPmb8eCgDugU00zwM1xfTOyn7N6a219J6Ll9aq0y1jOa14JFixZhypQpyMjIQHJyMpYtW4bQ0FCsWLGCNfwHH3yARx55BCkpKejatSvefvttWK1W5ObmOoQLCgpCXFyc7de8OfuhcFoSaPbDobkjsHf2cPixHNvrSaf+6UP98be+iU7xOYbJe2oI9s4erujxFHJxHcehfkbh/27tijfG99FaDMNTXafuttz2nYLW2/Y744k0v84YKpscjegrd/SJR8M0u4dH927jcruxeAb7C2vPdT5mVAVNFaDa2lrs3LkTaWlptmt+fn5IS0tDfn6+oDiqqqpQV1eHFi1aOFzPy8tDTEwMunTpgocffhglJeJGjmoREij9hG4+mguYu/U3+xlC+SEIPeCn+XBRPmIj9GPJauT/bu2qtQi6ILFFiMs15+kvvsFxfFTTtw20O4iaFB5XNK3SxcXFsFgsiI2NdbgeGxuLggJhe1k888wzSEhIcFCiRowYgdWrVyM3NxcLFizAxo0bceutt8JisbDGUVNTg/LycoefHpDbRKmHAazWmzmSL5F0vMvHQ3w5MOss/UqKI2V1jqfyPHRTR4zsGe9ZJAbFfqo1KjQQG54a4nDfuX1ja+8aYwjyN2Pv7OHYxzGzYIuDZxWYr2DoVWDz58/Hxx9/jLy8PAQHN2m9Y8eOtf3do0cP9OzZEx07dkReXh6GDXN1YszOzsacOXNUkdkeoUdhEO4hxUY/+PM0umLRUx3wLgWQnzcnXId7lgqzwjcix2GocpYdI+Gcd/ZnQgIN9cC+Krhr7yJYVow5v8NlHyAfKt+NaGoBio6OhtlsRmFhocP1wsJCxMXxO4y+/PLLmD9/Pn788Uf07NmTN2yHDh0QHR2NY8eOsd6fOXMmysrKbL8zZ86IS4hCuFiAaJadEz11lL6Ov9kPHz6YikGdo7UWBZ56ptj3CWaddc5KtQd/69MGzUPFL3+WQx6163FyvH43P2zXMpTznpXNAiRSgRnd2/c2dnVGUwUoMDAQffr0cXBgbnRo7t+/P+dzCxcuxAsvvICcnBz07dvX7XvOnj2LkpISxMezm1eDgoIQERHh8FMFLdpTUhQIFRjQKRo3d40RHP6dSX0RG6HepmhSOlqd6T/KTScrE63u3n1d2yj8w27TQL2xYvL1tr8ZMA5+P3Ic0JwQ5ehrpLPirQqau/VlZmZi+fLlePfdd3Ho0CE8/PDDqKysREZGBgBg4sSJmDlzpi38ggUL8Pzzz2PFihVISkpCQUEBCgoKcOXKFQDAlStX8PTTT2PLli04efIkcnNzcdddd6FTp05IT0/XJI1EE2IqWYdW+l+eT3AjZtXUsGtjseAefkuuFtiPqvW2CkxJnK0JY5xWlCqFmttZtG3BbWFRit5toxAbEYR+7Vu4DdvRrv0T4gNEiEdzH6AxY8agqKgIs2bNQkFBAVJSUpCTk2NzjD59+jT87JZfLF26FLW1tbj33nsd4snKysLs2bNhNpuxd+9evPvuuygtLUVCQgKGDx+OF154AUFB+jrV21176kljwBm1LttwdqHe/HsfLFh3GA8N7ug2BmoP9IezH4M71PyGkixAOjMBqSVN17hmgnzs5NAPvb0eB/mbsfmZm0VPp1oZ5408vT2n1EFzBQgApk2bhmnTprHey8vLc/j/5MmTvHGFhIRg3bp1MklmXLiqR7jCZ7vISduWoVhy/3WCwlKDoD9E+wBp9AmX/b0PzH4mTFm9w+WefTelM/1HMRjGMd3/Gd0Da7afVunl0h+NjQhCYXmN4PBaOf36m6VMvDiqoGw+QEO7CJ9yJhrQfAqMEI4c9XV4cixG9ojHcyOv9TwyHdEljn831hdGdUeg2Q+vju2tkkSEyWTCtR46maqhE43oHoeEKPZ9cRycoPU2BaaSOEH+foIsZs4+JWxwOd42KiOerOb8/JEbMdOgewm5U8bcTYFt+/cwtOVxmhYmg0ePGxLjmAO8EHflzWWZogxx+5v9sGS8MKuKEsg96vr2sYFYu+MMpqddwxtuwg3tcH+/trpbyUM0ofetDPS2TFipVWAMGNGdYe+2Ubg+yb1fy3//1gtf/H6O874nhtzWUSH4500dkf3DYUHhTdCP5dhtXwCnKTCnuhKjw40tjQApQDrGuZD3SozSRhAd0711JLq3jhQUlpQfz2gdFYJzpVdFPaOnHLevT0KUredvT3bodKj8cDN5QJKgcO78qJTWR9q2CMXpS1XKvkQBnBU1tikwQjw0BaYhYkeUN3RoiRWT3S/753yfDrojnQ2iCRH8975eGNqlFT6ckqpI/GydX4Akfwl5eGBge4flxnrTf5SsS84r3tTqb5W2AlrsNQedfU8+nBUeKZYrAyVXNcgCpGPYyvjNXWNdLwqNT+dTDIS+SWwRipUZ/RSL3768PzfyWny07TSeSOssW/xSBgD2MvnMKjDGMa1qDFoaX6G0BciiU9OJ2xXBcLJgSkiGPlOuLaQA6RgqsISv8uCgDnhwUAeP47HvWKQMABwtQPpSgPwVPJ1VK11P6TbP/nvqwSIuFNcpMOod5ICmwDRErBO05+8zToX3VjqI3BvHl9ByHyCuumZvMNDbKjCl9B8GjmlVs91Qul93UID09Tnd4ugETcgBKUCEqrC1OR1b+Y5S8M1jA7Hgnh5ai6EaYhpqvazIsUfPHaaSFiDtVrwpWwbqrfYWIP0gNruVsAD54gCZpsA0RMk2hq0Be2VMinIv9IBJA5JQfrUOg69ppbUoihMW5I8eraO0FkOXKK3+SPMBapJKb6vAFNR/XNKqlm6quAVIrz5Absqmi8IjIRn6Kr36gBQgHePOZ6FlWCAiQwLwZ3Gl27iC/P0wsif7YbBaE2D2Q+bwLlqLoRpKdlwEN0J9gP55UwcM+2uxgX1/qTcfIKWm5BiGcfABUtPfROk3BQWYgep6APqz6PHBMI7KuJRvok/VT1uoKdYx7sq4yQTcksy+KiwqJMDh/+AAs1xiER7C13HdfR37TrlceNPKPjVnwPhe9dTwLrbDKu2X4Ys1ADUL8sfHU2+QIJ0jXTl2OVfKIsXA0YLMMOqVM6WnQd+a0MfxfYq+TT4YBmgZ3nSWpU4NWYaDFCAdEvKXsnKTgCmhJ25x3AF57T/7470H+qF5WKAisnmKkUZdSsHnXzHr9mQVJVEecZ9bH626vczto8Nwf2pbPDKko2i/mEdv7oQbOrTEv2/z7HiGdyZfz3qdTR65rDX2ypWaFiAlO/bQQDN6t21u+19PPi/ul8EzeHBQe4zsEY/XxvXGNbHh/A+w0Cqc/zBwX2ybaQpMh2z59zAUllfjmlj+860AV8tO48jVGTlHViszrsczn+7Fy3/r5XKvVbMgFFXU4G6OM38I/oZGbKOst5VJWpMQGYzzZdWyxvniaGlO641fxuzhnKeYL+y8z82bE/rgn+/tdPtceJA/rtTU2/4XOwXWQsKAq3vrCOw/Vy76OTbaNHd/DpmRYRggNNDfdoyRxcpg3reHUGH3zbhYcv912PJnCe5KSVBaTMNBFiAN4ersIkMCBCk/WjG0Swy2PZvG6rTcLSEC++ek47/3uSpHRAOy+pJ4kQIkh44++85uDv9z5U57FbYj6CvgbCwhiPnEzgpQrMAzouyVHIZxLKPWhl34eBnYKZr1Op/s9t+7MZzza1ZmXI/gAP5uKr1bLF6TcMixnqqOkLPA7DH7mfDAoPaC4h7ZMx4vjOou8RR674YsQITshAdRseKDt7ET2SjrbGGS5gg9OuOBge1RdrUOw7rGuNyTYwl4erdY9GnXMN3iqfVVjFWw3kkBEvqks4h+Dj5A7uWXa9m887uGdonBuumDceurv6Kq1sL6zJsTpB8PZBT0uEWEN0AqoY+gVvVx3wxSj81nARLbj+htZZInyFJGBWZHcIAZ/77tWqR2aCnHW10QcjK6Ekhd5u1gAYKzD5B0eeQone1ahuHRoZ1kiMkRI1UdNv1H/o1yfQ9SgAhVkeK8523I2fB6kwVIL4NcvWWpfXm5Nj6CN6xFYia6WoCa/rYy0teAiV2pppcyoDoSGgW5s8oXs57mKlSmQ6sw/Fnkft8eKTQL1v/nbBYcgN+fvwWB/qR7syG2GdRux1758aYl/XJi/4XdTYU4T4EJxTnvTQ4+QNK/S8P0nYAptL9SqW4Z0E/dEesD1HCR6ounUC+kMis5lrT6Es3DAhHmw35CfKeKi1Vo9D4FJu4oDMXE+OsFwoLpLktFrMiyWJwVGe6wv84Yavvb4Zwpp3d49F3c5GXz0Ib9ygZf0+BErdfT2rVGDR+gQB90kva9FPsq1K7oBqH9a0JksNuNEb1pCsxbkbPvcheXGAtQYotQ2998ipUnFqBGBYeL7/41CC+O7oEn/9oJnksBUkIBMAkzTmnK329oCwDIvMV1p3y5Rdfr3nFK4rvDcB2g5khT5/Xcp+DfB6iJrvERbi08ercAiZFO8TIqUBg5phUdT+6WbxWYO2VEqrLCpzc5H8MghrjIELSOCsGu06Ws9xOiQnB/alvb/3JbgNpHh+GEgKOCtIaryL1wV3c8nd4VkSH8iqSnxDTj3yTRWyELkMrYN2beOIXrTT4pSiFmFZiQ41AIY+PuG5ocpsD4w8qhQDjH4OlO0J88NACz70jGd/8a6Das3ArQuumDOe8ZoeqYTCZO5ccb+w+1IQVIZbzd0ZP2q3CPOKsIf37qXeFMig51H+gvFC87GhVNd8ni2827VbMgh/Li1gfIZR8gea1ZYjGhYSXY5Bvbo1tCpNt46yzyfiRvXmyh5hEl3or3lg4DoOoUGFUW/SDjURh69wGae1d33N27NZ4bea3LvevaRin+fqEKopbVg03EkAAz8mfejF+eHupw3Z2FpN5q5bx3b582AIA5Trtlu+D0CpNJPd2R2wdI/nfpaeygtSh6ygs1IQVIZaJCmhzNlDrNWUv0bpHQA760EWJ0eBAWjUmR7VgIj9Aoq9z13WxKr8kExEeGICTQ7HIyOx98GzA+dFMH/P78LZg0IMmNRA1MHpCEfkktOI+5EEJClLCjOBoRu4/Rv272bINEI1vkjSu5fiAFSGUiQwPw/gOpWDP1BsHb9gtF310h0Yin32n/nHTb30bRoYWIqeUyeLn1yP4dhe8wzfZue8XW/razhWTeqO4O/zvvmOwYt0nUSp/Zd3bD2of6SzpD6r0H+uHW7nGYc2d394HtEOsDNM7OgVosejoNXgpy1Jfo8IbycGNH6UqukaFVYBowsLMyhY2vPtBoQRu6xrkeaivKSsby4eyf9iaLW3q3OCS2CEHfdtKtRVJzQ27lq3vrJn8Xtz5ALFqs4zdu+tvZQvL3G9rhuS/32/4PDjCjX/sW2HbikmucAjOHzSoiNn8GdW6FQZ1dD0t2B98UHhtGV2IakVKP5bBefTVtIL7fewFj+yV6HJcRIQWIIBTks4cHuFzjs9rYt4MMxxEE9mH0PgUmhpBAMzY+NZR3o0gj8N4D/USFZ00tRxao4cun9Cvu6JWAb/acxyNDO7rc49J/lBBJT1VHK1FaR4VgyuAOGr1de0gBIggFYdvxWs5Rq54acT6Eyim38qNF9oi1fLApsQ4WILv/PFkmrpei8uqYFMy8tSsSokJc7om2AOklUVpAZn2PIR8gA9J4oOiI7nEO1/naAloEph9MPLXOWTliG/HbhzGyscRXiqT7rQxcr0Xa76AsYh8gI+DnZ2JVfgBuBe++vg1TNN0S+A+DFYOBqw4A36k/SkIKkAH5aMoNWHRfLzx7W7LDdaoQxoCv4RUyovXWKTAtiY3Ubidce6tX9t09kBwfgbcm9LVdc9wI0QMLkAdbAqjVtnCtOIuLDMaReSOwdHwfh+ueln69nEkopRrT1iaeQwqQAWkZHoS7r2uDkECz1qIQEuDriExocpy++7o27pdQG0QBUlNR+/dtXW1/hwcL6+BimgXjwwdT8fW0G2WXx10/ZZ83Q7q0wvePD8K18eyWDqtGU2CRIeooCnNHdefcvDDI3yzrlJfJZMKIbnG4rUcc6z5VaiJlWpz0H8/Rh/pLyIIxukLf4X/392a97m7a6otHbsTxoivolhCB3EOFvGGNPAUmhWbB/qiorucNM3VwRwQHmHG08ApS2wtfUTbAg/1uPOGa2HBs+bNh1RZbp+awDF5kryfJssCidmfe0gXvbzktPjKRRAQHYMqg9liy4biwBzws//5mP7zhZFUyCn2TWuDtTSe0FsPQkAWIkBUf6485Se8Wi9t7JrDe4xvtmUwmhASa0b11JEwmk+2UbMcwTX8bZQrMWUy21XFC+PQhYc9N7J+EF0Z1N4SFbNF9Kba/WZ3m7dLA5iPs7sT1pnhEi2ajRVggJtzQTnoECsFWl9xtjnjPdQ07YmfcmKSESKqR3i0Wy/5+nctu4YRwdKEALVmyBElJSQgODkZqaiq2bdvGGXb58uUYNGgQmjdvjubNmyMtLc0lPMMwmDVrFuLj4xESEoK0tDQcPXpU6WRoxp29Gjrah4a4LittxMg7nhqRQH/u6Umhp8EDQGIL/rO0Ov/lEK93nBW1Pu2aS4qnC8u+SkLRqgaw+WrERQTj9XG9sW76YCREhWBlxvV4a0If1oMv7XPuxk4NGyyGBDSVryCesuYYj/6VQTlwt9njy3/riSPzRqBdyzCVJBKAhE9jMpkwons82rYUft4e4YjmCtCaNWuQmZmJrKws7Nq1C7169UJ6ejouXrzIGj4vLw/jxo3Dhg0bkJ+fj8TERAwfPhznzp2zhVm4cCFee+01LFu2DFu3bkVYWBjS09NRXV2tVrJUZdF9vfD9vwbh4Zu4FSBCXTx1dOaP24Sc6YOwYnJflwMm9QpbmhuXi4f5qC/bHb0SbArd0C4xGN4tzs0TwPS0azBvVHesz2w65VxuIxfXLJseBlHOaWVLu7tZQpPJJFhpJLwbzRWgRYsWYcqUKcjIyEBycjKWLVuG0NBQrFixgjX8Bx98gEceeQQpKSno2rUr3n77bVitVuTm5gJoGG0tXrwYzz33HO666y707NkTq1evxvnz5/Hll1+qmDL18Df7ITkhgtfcTw5z+sF+JP7v27qiV5smJUboKrCucRG4uWusEuIpApv14dGhHbHwnp74MfMmlWTQBk/rnn2ZCArww99vaIc2zZtG/bzKtt1dtrKVEOl6Vlc7g1sU4ljSpHd8wzanPzRVgGpra7Fz506kpaXZrvn5+SEtLQ35+fmC4qiqqkJdXR1atGhwdjxx4gQKCgoc4oyMjERqaqrgOAnCU3inuezu9W7bHP9367V297yzKWRLVpC/Gfddn4jWHHvCuOO2HnE4+p9bPZRM/7ibuvKkzETYTbl9PPUGTLihHR5Pu0ZyfGrDlvIR3eLw8JCOWD6xr9uwhG+jqQJUXFwMi8WC2FjHkWxsbCwKCgoExfHMM88gISHBpvA0PicmzpqaGpSXlzv8vIX+HRp8Bho3EiO0x1PHZSM25EqsVjPBhACzHxbd10tQeLmMoNOGenYCuViU1Inty+INHVrihVHdEa6TvXHYEKLs+fmZ8MyIrrgl2bEPICM44YzmU2CeMH/+fHz88cf44osvEBws3eyZnZ2NyMhI2y8x0XuUhbcm9sGbE/rgWY33ufA1+KclmmAY8b4VxrQSyS9zY77d/deqHrW4p4+498nZ8bJZg4QWB9ZT50X0AHqcRjdmXXDFS5JhODRVgKKjo2E2m1FY6LjXSWFhIeLi+B0CX375ZcyfPx8//vgjevbsabve+JyYOGfOnImysjLb78yZM1KSo0uaBQcgvVscggPI6U8vOB946gvooYHXSoRRKa0VjV/odCvrfUPaE5swtvSE1miqAAUGBqJPnz42B2YANofm/v37cz63cOFCvPDCC8jJyUHfvo7zvO3bt0dcXJxDnOXl5di6dStnnEFBQYiIiHD4EYQn8O72bHdPivpjxEZfif2KxHbeWqmani5TlkuJYSuTYqYm9aCqG7HsE/pF88nezMxMTJo0CX379kW/fv2wePFiVFZWIiMjAwAwceJEtG7dGtnZ2QCABQsWYNasWfjwww+RlJRk8+sJDw9HeHg4TCYTpk+fjnnz5qFz585o3749nn/+eSQkJGDUqFFaJdNn0MNIXw8IzQaGgeiexYh5bC/yf/8mzGdH0nuMmDlucLeSS2iSA9i0HRH5pUdjpbd8bqNb4oyK5grQmDFjUFRUhFmzZqGgoAApKSnIycmxOTGfPn0afnYT1UuXLkVtbS3uvfdeh3iysrIwe/ZsAMCMGTNQWVmJqVOnorS0FAMHDkROTo5HfkKEMPTYSBqd9U8Mxi2v/KK1GB5h31GltI3STA6jw9ZNOl9r0zwE2/46IcH+8NQAs6vB39eOUtErYnyxCPnQXAECgGnTpmHatGms9/Ly8hz+P3nypNv4TCYT5s6di7lz58ogHUEohxAH6M6xjrsfG9HKQSNc6bidAnMK8NzIZFitDO7rm4h6i50CxHLIqFGOUmnEZSNEEeVKjynNuiMZb+Qdx5w7u2stik+iCwWI8B4M1p5qD6MP3wqlsS8XShSRQH8/1NZb0aO1cXfG5gwr8n6LsEAsHttwEO+Ok5ds1wPMLCvIhIthePRYzzJubI/JA5IMOajxBkgBIggloPbMAQcFSK7G3i6a3bNuwdVaC1q4OQdKL4iZKnabXzy3ay1Np6cGsMyziLMA6VCF8IJ6RsqPdtDMI0FoiA67FEVQpJG3y7zQQH+0DA/iDR6iwJljr4zphbcm9FHNl4bVCZonfJ3dFJgfi5BG63udp7yMJj+hL8gCRBAKINQ3wVecxv0UngITQuYt12D/uTKMuV6+jU5H9khAoL8fWjULQmF5jeDnpE6BtQxzVfL4rDh19VbOe2Ll0ENZJYWHkBNSgAhCRvxMgJUBBl8TLSq8t+NuKbfESEURHR6Er6cNlOnljrwz6Xr86+Pf8cyIroLCi1Em/PxM+OrRG1FdZ0Fzlik+vvyss/ArQEZzgnZGjPTGTimhBKQAEYSM/PZ/w3DgfBlu7hrDG25cv7Y4fvEK+rVvgS1/XuINCzR0cnoYgUvF4P0sJ43p6t46Ej8/OUSx9/RKjJL0XHc3TuHe+l0IQgikABEy49stalxkMOIi3e83lX13D9vfQpbCr/1nf/xtWb5HsmmJyeFvz8pIXEQwCsqrcWt3/uNy9IycigefFSexRSh+fGIwokIDWO/LYQFq1Yzf90pOnKUlB2LCE0gBImTGwGYKHWP0Zt6+o/K0z1r3xGAcLaxAn3bNPZRKGglRDQquv58J/hK9n9W05l3jtI+UPWIUCGeZn73tWpwvu4rJA5IkSqYu1DIRzpACRBAGwOiNt5wD9ciQAPRNaiFfhCIJ8jfjwJx0mP1MhrdAiDsLzLEUxkcFY8rgDjJLJA5j5z6hNaQAETJDTZJYjOzbIxRvKxVhQZ41nWpNgbmVQz4x1MFJ4FARWxsYLq2E4tA+QARhAIyuJNl30kZPixxEhcq3YaMnypRRds5mY/GYFMNb4AhtIQsQISvXxnP7GxDSiQxhd2I1CtRPNdG7bRReureXbPF5krePDO0Ef7Mfhl3Lv2pRj9x0TSutRSAMDilAhCx8+9hA/HiwEI8M6ai1KIZDiEGkS1wzPHnLNYiNcL/CTI/Yr/wSsurNW2kdFYIvHrlR1jg9WVUXHGDGv4Z1FhRWD5Y7x3JEEJ5BChAhC91bR7rdc4RgR+hKoscEdlS6xC6JeuhI1ebu61rj813nMO3mTrLHTdY1gpAGKUAEoTE3dGiJ/h1aonNsuNaiKIZaZ2XplZfv7YXpw65B25ahssetVtb6oN5KeDmkABGExpj9TPho6g1ai6Eo9s6qvtiR+vmZFFF+APiUCcjkYEkUVpKmp3XG4p+O4j+je7gPTPgUpAARBKE49l200I6LEIbvqD/S0jo97Rpk3Nje8AsJCPkhBYggCMUx+qGbesaXsjYqNBCRIQFgGEbUVgKk/BBskAJEEITiOExdaCeGV6KaD5AOPpzZz4Ttz6bZ/iYIT6CNEAmCIAzMrDu6AQAeV3iVoF62Lwj090OgP3VdhOeQBYggCMUx+fgyeCVJSYzCH/NuJaWAIERCNYYgCMVx9AEiDUhutFB+PNmAkSD0AClABEEoDnWV3odepsQIQiqkABEEoTgmOgzV+NB3I7wMUoAIglAcP1oF5nXQFBhhdEgBIghCcewtQCEBZg0lIaTirLjSFBhhdGgVGEEQqvDcyGtRfrUOiS0UOhKCIAhCBKQAEQShCg8O6qC1CISM0BQYYXRoCowgCIIgCJ+DFCCCIAjCLXSILeFtkAJEEARBuIXUH8LbIAWIIAiCEE3HmDCtRSAIjyAFiCAIgnBLTLMg298rJvdF17gIDaUhCM+hVWAEQRCEWx4b1hnnS6txZ0oCbu4aq7U4BOExmluAlixZgqSkJAQHByM1NRXbtm3jDHvgwAHcc889SEpKgslkwuLFi13CzJ49GyaTyeHXtWtXBVNAEATh/UQEB2DJ+OuQ3i1Oa1EIQhY0VYDWrFmDzMxMZGVlYdeuXejVqxfS09Nx8eJF1vBVVVXo0KED5s+fj7g47krYrVs3XLhwwfbbtGmTUkkgCIIgCMKAaKoALVq0CFOmTEFGRgaSk5OxbNkyhIaGYsWKFazhr7/+erz00ksYO3YsgoKCWMMAgL+/P+Li4my/6OhopZJAEARBEIQB0UwBqq2txc6dO5GWltYkjJ8f0tLSkJ+f71HcR48eRUJCAjp06IDx48fj9OnTvOFrampQXl7u8CMIgiDUJ8CsuWcG4SNoVtKKi4thsVgQG+voTBcbG4uCggLJ8aampmLVqlXIycnB0qVLceLECQwaNAgVFRWcz2RnZyMyMtL2S0xMlPx+giAIQjoZN7bHNbHheCLtGq1FIbwcr1sFduutt9r+7tmzJ1JTU9GuXTusXbsWDzzwAOszM2fORGZmpu3/8vJyUoIIgiA0IDIkAD8+cZPWYhA+gGYKUHR0NMxmMwoLCx2uFxYW8jo4iyUqKgrXXHMNjh07xhkmKCiI16eIIAiCIAjvQrMpsMDAQPTp0we5ubm2a1arFbm5uejfv79s77ly5QqOHz+O+Ph42eIkCIIgCMLYaDoFlpmZiUmTJqFv377o168fFi9ejMrKSmRkZAAAJk6ciNatWyM7OxtAg+P0wYMHbX+fO3cOu3fvRnh4ODp16gQAeOqpp3DHHXegXbt2OH/+PLKysmA2mzFu3DhtEkkQBEEQhO7QVAEaM2YMioqKMGvWLBQUFCAlJQU5OTk2x+jTp0/Dz6/JSHX+/Hn07t3b9v/LL7+Ml19+GTfddBPy8vIAAGfPnsW4ceNQUlKCVq1aYeDAgdiyZQtatWqlatoIgiAIgtAvJoZh6JBfJ8rLyxEZGYmysjJERNB5NwRBEARhBMT037ThAkEQBEEQPgcpQARBEARB+BykABEEQRAE4XOQAkQQBEEQhM9BChBBEARBED4HKUAEQRAEQfgcpAARBEEQBOFzkAJEEARBEITPQQoQQRAEQRA+h6ZHYeiVxs2xy8vLNZaEIAiCIAihNPbbQg65IAWIhYqKCgBAYmKixpIQBEEQBCGWiooKREZG8oahs8BYsFqtOH/+PJo1awaTySRr3OXl5UhMTMSZM2fonDEdQN9DX9D30Bf0PfQHfRN+GIZBRUUFEhISHA5TZ4MsQCz4+fmhTZs2ir4jIiKCCq+OoO+hL+h76Av6HvqDvgk37iw/jZATNEEQBEEQPgcpQARBEARB+BykAKlMUFAQsrKyEBQUpLUoBOh76A36HvqCvof+oG8iH+QETRAEQRCEz0EWIIIgCIIgfA5SgAiCIAiC8DlIASIIgiAIwucgBYggCIIgCJ+DFCAVWbJkCZKSkhAcHIzU1FRs27ZNa5G8ktmzZ8NkMjn8unbtartfXV2NRx99FC1btkR4eDjuueceFBYWOsRx+vRpjBw5EqGhoYiJicHTTz+N+vp6tZNiSH755RfccccdSEhIgMlkwpdffulwn2EYzJo1C/Hx8QgJCUFaWhqOHj3qEObSpUsYP348IiIiEBUVhQceeABXrlxxCLN3714MGjQIwcHBSExMxMKFC5VOmiFx9z0mT57sUl9GjBjhEIa+h3xkZ2fj+uuvR7NmzRATE4NRo0bhyJEjDmHkaqPy8vJw3XXXISgoCJ06dcKqVauUTp6hIAVIJdasWYPMzExkZWVh165d6NWrF9LT03Hx4kWtRfNKunXrhgsXLth+mzZtst174okn8M033+CTTz7Bxo0bcf78edx99922+xaLBSNHjkRtbS1+++03vPvuu1i1ahVmzZqlRVIMR2VlJXr16oUlS5aw3l+4cCFee+01LFu2DFu3bkVYWBjS09NRXV1tCzN+/HgcOHAA69evx7fffotffvkFU6dOtd0vLy/H8OHD0a5dO+zcuRMvvfQSZs+ejbfeekvx9BkNd98DAEaMGOFQXz766COH+/Q95GPjxo149NFHsWXLFqxfvx51dXUYPnw4KisrbWHkaKNOnDiBkSNHYujQodi9ezemT5+OBx98EOvWrVM1vbqGIVShX79+zKOPPmr732KxMAkJCUx2draGUnknWVlZTK9evVjvlZaWMgEBAcwnn3xiu3bo0CEGAJOfn88wDMN8//33jJ+fH1NQUGALs3TpUiYiIoKpqalRVHZvAwDzxRdf2P63Wq1MXFwc89JLL9mulZaWMkFBQcxHH33EMAzDHDx4kAHAbN++3Rbmhx9+YEwmE3Pu3DmGYRjmjTfeYJo3b+7wPZ555hmmS5cuCqfI2Dh/D4ZhmEmTJjF33XUX5zP0PZTl4sWLDABm48aNDMPI10bNmDGD6datm8O7xowZw6SnpyudJMNAFiAVqK2txc6dO5GWlma75ufnh7S0NOTn52somfdy9OhRJCQkoEOHDhg/fjxOnz4NANi5cyfq6uocvkXXrl3Rtm1b27fIz89Hjx49EBsbawuTnp6O8vJyHDhwQN2EeBknTpxAQUGBQ/5HRkYiNTXVIf+joqLQt29fW5i0tDT4+flh69attjCDBw9GYGCgLUx6ejqOHDmCy5cvq5Qa7yEvLw8xMTHo0qULHn74YZSUlNju0fdQlrKyMgBAixYtAMjXRuXn5zvE0RiG+pwmSAFSgeLiYlgsFofCCgCxsbEoKCjQSCrvJTU1FatWrUJOTg6WLl2KEydOYNCgQaioqEBBQQECAwMRFRXl8Iz9tygoKGD9Vo33COk05h9fXSgoKEBMTIzDfX9/f7Ro0YK+kQKMGDECq1evRm5uLhYsWICNGzfi1ltvhcViAUDfQ0msViumT5+OG2+8Ed27dwcA2doorjDl5eW4evWqEskxHHQaPOF13Hrrrba/e/bsidTUVLRr1w5r165FSEiIhpIRhP4YO3as7e8ePXqgZ8+e6NixI/Ly8jBs2DANJfN+Hn30Uezfv9/BR5FQD7IAqUB0dDTMZrOLF39hYSHi4uI0ksp3iIqKwjXXXINjx44hLi4OtbW1KC0tdQhj/y3i4uJYv1XjPUI6jfnHVxfi4uJcFgfU19fj0qVL9I1UoEOHDoiOjsaxY8cA0PdQimnTpuHbb7/Fhg0b0KZNG9t1udoorjARERE0EPwLUoBUIDAwEH369EFubq7tmtVqRW5uLvr376+hZL7BlStXcPz4ccTHx6NPnz4ICAhw+BZHjhzB6dOnbd+if//+2Ldvn0Ojv379ekRERCA5OVl1+b2J9u3bIy4uziH/y8vLsXXrVof8Ly0txc6dO21hfv75Z1itVqSmptrC/PLLL6irq7OFWb9+Pbp06YLmzZurlBrv5OzZsygpKUF8fDwA+h5ywzAMpk2bhi+++AI///wz2rdv73Bfrjaqf//+DnE0hqE+xw6tvbB9hY8//pgJCgpiVq1axRw8eJCZOnUqExUV5eDFT8jDk08+yeTl5TEnTpxgNm/ezKSlpTHR0dHMxYsXGYZhmIceeohp27Yt8/PPPzM7duxg+vfvz/Tv39/2fH19PdO9e3dm+PDhzO7du5mcnBymVatWzMyZM7VKkqGoqKhgfv/9d+b3339nADCLFi1ifv/9d+bUqVMMwzDM/PnzmaioKOarr75i9u7dy9x1111M+/btmatXr9riGDFiBNO7d29m69atzKZNm5jOnTsz48aNs90vLS1lYmNjmQkTJjD79+9nPv74YyY0NJR58803VU+v3uH7HhUVFcxTTz3F5OfnMydOnGB++ukn5rrrrmM6d+7MVFdX2+Kg7yEfDz/8MBMZGcnk5eUxFy5csP2qqqpsYeRoo/78808mNDSUefrpp5lDhw4xS5YsYcxmM5OTk6NqevUMKUAq8vrrrzNt27ZlAgMDmX79+jFbtmzRWiSvZMyYMUx8fDwTGBjItG7dmhkzZgxz7Ngx2/2rV68yjzzyCNO8eXMmNDSUGT16NHPhwgWHOE6ePMnceuutTEhICBMdHc08+eSTTF1dndpJMSQbNmxgALj8Jk2axDBMw1L4559/nomNjWWCgoKYYcOGMUeOHHGIo6SkhBk3bhwTHh7OREREMBkZGUxFRYVDmD179jADBw5kgoKCmNatWzPz589XK4mGgu97VFVVMcOHD2datWrFBAQEMO3atWOmTJniMjCj7yEfbN8CALNy5UpbGLnaqA0bNjApKSlMYGAg06FDB4d3EAxjYhiGUdvqRBAEQRAEoSXkA0QQBEEQhM9BChBBEARBED4HKUAEQRAEQfgcpAARBEEQBOFzkAJEEARBEITPQQoQQRAEQRA+BylABEEQBEH4HKQAEQRBEAThc5ACRBCE1zB58mSMGjUKADBkyBBMnz5dU3kIgtAvpAARBEHwUFtbq7UIBEEoAClABEF4HZMnT8bGjRvx6quvwmQywWQy4eTJkwCA/fv349Zbb0V4eDhiY2MxYcIEFBcX254dMmQIpk2bhunTpyM6Ohrp6ekapYIgCCUhBYggCK/j1VdfRf/+/TFlyhRcuHABFy5cQGJiIkpLS3HzzTejd+/e2LFjB3JyclBYWIj77rvP4fl3330XgYGB2Lx5M5YtW6ZRKgiCUBJ/rQUgCIKQm8jISAQGBiI0NBRxcXG26//73//Qu3dvvPjii7ZrK1asQGJiIv744w9cc801AIDOnTtj4cKFqstNEIR6kAJEEITPsGfPHmzYsAHh4eEu944fP25TgPr06aO2aARBqAwpQARB+AxXrlzBHXfcgQULFrjci4+Pt/0dFhamplgEQWgAKUAEQXglgYGBsFgsDteuu+46fPbZZ0hKSoK/PzV/BOHLkBM0QRBeSVJSErZu3YqTJ0+iuLgYVqsVjz76KC5duoRx48Zh+/btOH78ONatW4eMjAwXZYkgCO+GFCCCILySp556CmazGcnJyWjVqhVOnz6NhIQEbN68GRaLBcOHD0ePHj0wffp0REVFwc+PmkOC8CVMDMMwWgtBEARBEAShJjTkIQiCIAjC5yAFiCAIgiAIn4MUIIIgCIIgfA5SgAiCIAiC8DlIASIIgiAIwucgBYggCIIgCJ+DFCCCIAiCIHwOUoAIgiAIgvA5SAEiCIIgCMLnIAWIIAiCIAifgxQggiAIgiB8DlKACIIgCILwOf4fp3KKYTWyGS8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# main():\n",
    "\"\"\"Main function\n",
    "You need to combine all functions in the main function\"\"\"\n",
    "learning_rate = 0.0000000000027   \n",
    "num_iterations = 100\n",
    "x, y = train_x, train_y\n",
    "\n",
    "#Training process the model\n",
    "parameters, cost, AL, dict = two_layer_model(train_x, train_y, layers_dims, learning_rate, num_iterations)\n",
    "\n",
    "#Plotting the loss\n",
    "plot_cost(cost, learning_rate)\n",
    "\n",
    "#Predictions on the training set\n",
    "train_predictions, train_accuracy = predict(train_x, train_y, parameters, 0.2)\n",
    "print(\"Train accuracy: \", train_accuracy)\n",
    "\n",
    "#Predictions on the test set\n",
    "test_predictions, test_accuracy = predict(test_x, test_y, parameters, 0.2)\n",
    "print(\"Test Accuracy: \" + str(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code represent a main function that orchestrates the execution of various functions in a sequence. It combines the individual steps of the model to train, predict on training and test sets, and visualize the cost. \n",
    "\n",
    "Function Calls: \n",
    "\n",
    "1 - two_layer_model: Trains a two-layer neural network using training data (train_x, train_y) with specified hyperparameters like learning rate and the number of iterations. It returns the trained parameters, cost during training, output of the network (AL), and intermediate values (dict) for analysis.\n",
    "\n",
    "2 - predict: Predicts the labels for the training and test sets using the trained parameters from two_layer_model.\n",
    "\n",
    "3 - plot_cost: Visualizes the cost over iterations during training.\n",
    "\n",
    "Analysis of outputs\n",
    "-------------------\n",
    "-Cost after iteration 0: 5.222932692835304: This is the initial cost calculated after the first iteration of the training process. It indicates how much the model initially fits the training data.\n",
    "\n",
    "-Train accuracy: 6.752380952380954%: This represents the accuracy achieved by the model on the training dataset after training.\n",
    "\n",
    "-Test accuracy: 6.622222222222223%: This shows the accuracy achieved by the model on a separate test dataset after training.\n",
    "\n",
    "This results may show that the model is not effectively learning the patterns (as expected) in the data or that it might be underfitting, meaning it's not complex enough to capture the relationships present in the data.\n",
    "\n",
    "To improve model performance, we can apply following processes\n",
    "- We think considering \"increasing\" the number of iterations during training to allow the model to learn more from the data.\n",
    "- We can try to adjust the model architecture by \"adding more layers\" or neurons to increase its complexity.\n",
    "- We can try \"different activation functions\" or \"optimization algorithms\" to see if they improve performance.\n",
    "- At last, it may be useful to check if there are any issues with the dataset, such as noise, that might be affecting the model's ability to learn.\n",
    "\n",
    "According to plot, despite an increase in iterations, the cost remains within the same range as expected for the specified learning rate, showing consistent fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------Early stopping function-------------------------------#\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_folders = sorted(os.listdir(directory))\n",
    "    for class_label, class_folder in enumerate(class_folders):\n",
    "        class_path = os.path.join(directory, class_folder)\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png')):  \n",
    "                image_path = os.path.join(class_path, filename)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = cv2.resize(image, (32, 32))  \n",
    "                #Resizing images to 32x32 pixels\n",
    "                images.append(image)\n",
    "                labels.append(class_label)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "#Definition of paths to our training and testing directories\n",
    "train_directory = './train'\n",
    "test_directory = './test'\n",
    "\n",
    "#Loading and preprocessing the \"training\" dataset\n",
    "train_x_orig, train_y = load_images_from_directory(train_directory)\n",
    "\n",
    "#Loading and preprocessing the \"testing\" dataset\n",
    "test_x_orig, test_y = load_images_from_directory(test_directory)\n",
    "\n",
    "#Reshaping and normalizing the image data\n",
    "train_x = train_x_orig.reshape(train_x_orig.shape[0], -1).T / 255.\n",
    "test_x = test_x_orig.reshape(test_x_orig.shape[0], -1).T / 255.\n",
    "train_y = train_y.reshape(1, -1)\n",
    "test_y = test_y.reshape(1, -1)\n",
    "\n",
    "# # Print shapes of the datasets\n",
    "print(\"train_x's shape:\", train_x.shape)\n",
    "print(\"train_y's shape:\", train_y.shape)\n",
    "print(\"test_x's shape:\", test_x.shape)\n",
    "print(\"test_y's shape:\", test_y.shape)\n",
    "### CONSTANTS DEFINING THE MODEL ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Cost: 3.2762\n",
      "Val loss:\n",
      "[3.32886852]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: 3.2759\n",
      "Val loss:\n",
      "[3.3285723]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32886852]\n",
      "Epoch [3/100], Train Cost: 3.2756\n",
      "Val loss:\n",
      "[3.32827608]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3285723]\n",
      "Epoch [4/100], Train Cost: 3.2753\n",
      "Val loss:\n",
      "[3.32797986]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32827608]\n",
      "Epoch [5/100], Train Cost: 3.2750\n",
      "Val loss:\n",
      "[3.32768361]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32797986]\n",
      "Epoch [6/100], Train Cost: 3.2747\n",
      "Val loss:\n",
      "[3.32738734]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32768361]\n",
      "Epoch [7/100], Train Cost: 3.2744\n",
      "Val loss:\n",
      "[3.32709105]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32738734]\n",
      "Epoch [8/100], Train Cost: 3.2741\n",
      "Val loss:\n",
      "[3.32679465]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32709105]\n",
      "Epoch [9/100], Train Cost: 3.2738\n",
      "Val loss:\n",
      "[3.3264982]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32679465]\n",
      "Epoch [10/100], Train Cost: 3.2735\n",
      "Val loss:\n",
      "[3.32620172]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3264982]\n",
      "Epoch [11/100], Train Cost: 3.2732\n",
      "Val loss:\n",
      "[3.32590523]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32620172]\n",
      "Epoch [12/100], Train Cost: 3.2729\n",
      "Val loss:\n",
      "[3.32560872]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32590523]\n",
      "Epoch [13/100], Train Cost: 3.2726\n",
      "Val loss:\n",
      "[3.32531219]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32560872]\n",
      "Epoch [14/100], Train Cost: 3.2723\n",
      "Val loss:\n",
      "[3.32501566]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32531219]\n",
      "Epoch [15/100], Train Cost: 3.2720\n",
      "Val loss:\n",
      "[3.32471914]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32501566]\n",
      "Epoch [16/100], Train Cost: 3.2716\n",
      "Val loss:\n",
      "[3.32442261]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32471914]\n",
      "Epoch [17/100], Train Cost: 3.2713\n",
      "Val loss:\n",
      "[3.32412608]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32442261]\n",
      "Epoch [18/100], Train Cost: 3.2710\n",
      "Val loss:\n",
      "[3.32382956]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32412608]\n",
      "Epoch [19/100], Train Cost: 3.2707\n",
      "Val loss:\n",
      "[3.32353304]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32382956]\n",
      "Epoch [20/100], Train Cost: 3.2704\n",
      "Val loss:\n",
      "[3.3232365]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32353304]\n",
      "Epoch [21/100], Train Cost: 3.2701\n",
      "Val loss:\n",
      "[3.32293997]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3232365]\n",
      "Epoch [22/100], Train Cost: 3.2698\n",
      "Val loss:\n",
      "[3.32264343]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32293997]\n",
      "Epoch [23/100], Train Cost: 3.2695\n",
      "Val loss:\n",
      "[3.32234689]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32264343]\n",
      "Epoch [24/100], Train Cost: 3.2692\n",
      "Val loss:\n",
      "[3.32205035]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32234689]\n",
      "Epoch [25/100], Train Cost: 3.2689\n",
      "Val loss:\n",
      "[3.32175382]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32205035]\n",
      "Epoch [26/100], Train Cost: 3.2686\n",
      "Val loss:\n",
      "[3.32145728]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32175382]\n",
      "Epoch [27/100], Train Cost: 3.2683\n",
      "Val loss:\n",
      "[3.32116075]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32145728]\n",
      "Epoch [28/100], Train Cost: 3.2680\n",
      "Val loss:\n",
      "[3.32086421]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32116075]\n",
      "Epoch [29/100], Train Cost: 3.2677\n",
      "Val loss:\n",
      "[3.32056767]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32086421]\n",
      "Epoch [30/100], Train Cost: 3.2674\n",
      "Val loss:\n",
      "[3.32027114]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32056767]\n",
      "Epoch [31/100], Train Cost: 3.2671\n",
      "Val loss:\n",
      "[3.31997458]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32027114]\n",
      "Epoch [32/100], Train Cost: 3.2668\n",
      "Val loss:\n",
      "[3.31967799]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31997458]\n",
      "Epoch [33/100], Train Cost: 3.2665\n",
      "Val loss:\n",
      "[3.31938138]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31967799]\n",
      "Epoch [34/100], Train Cost: 3.2662\n",
      "Val loss:\n",
      "[3.31908478]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31938138]\n",
      "Epoch [35/100], Train Cost: 3.2659\n",
      "Val loss:\n",
      "[3.31878817]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31908478]\n",
      "Epoch [36/100], Train Cost: 3.2656\n",
      "Val loss:\n",
      "[3.31849157]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31878817]\n",
      "Epoch [37/100], Train Cost: 3.2653\n",
      "Val loss:\n",
      "[3.31819497]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31849157]\n",
      "Epoch [38/100], Train Cost: 3.2650\n",
      "Val loss:\n",
      "[3.31789837]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31819497]\n",
      "Epoch [39/100], Train Cost: 3.2647\n",
      "Val loss:\n",
      "[3.31760178]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31789837]\n",
      "Epoch [40/100], Train Cost: 3.2644\n",
      "Val loss:\n",
      "[3.31730518]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31760178]\n",
      "Epoch [41/100], Train Cost: 3.2641\n",
      "Val loss:\n",
      "[3.31700855]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31730518]\n",
      "Epoch [42/100], Train Cost: 3.2638\n",
      "Val loss:\n",
      "[3.31671192]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31700855]\n",
      "Epoch [43/100], Train Cost: 3.2635\n",
      "Val loss:\n",
      "[3.31641529]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31671192]\n",
      "Epoch [44/100], Train Cost: 3.2632\n",
      "Val loss:\n",
      "[3.31611866]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31641529]\n",
      "Epoch [45/100], Train Cost: 3.2629\n",
      "Val loss:\n",
      "[3.31582203]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31611866]\n",
      "Epoch [46/100], Train Cost: 3.2626\n",
      "Val loss:\n",
      "[3.31552539]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31582203]\n",
      "Epoch [47/100], Train Cost: 3.2623\n",
      "Val loss:\n",
      "[3.31522872]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31552539]\n",
      "Epoch [48/100], Train Cost: 3.2620\n",
      "Val loss:\n",
      "[3.31493207]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31522872]\n",
      "Epoch [49/100], Train Cost: 3.2617\n",
      "Val loss:\n",
      "[3.31463543]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31493207]\n",
      "Epoch [50/100], Train Cost: 3.2614\n",
      "Val loss:\n",
      "[3.31433878]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31463543]\n",
      "Epoch [51/100], Train Cost: 3.2611\n",
      "Val loss:\n",
      "[3.31404214]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31433878]\n",
      "Epoch [52/100], Train Cost: 3.2608\n",
      "Val loss:\n",
      "[3.31374549]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31404214]\n",
      "Epoch [53/100], Train Cost: 3.2604\n",
      "Val loss:\n",
      "[3.31344885]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31374549]\n",
      "Epoch [54/100], Train Cost: 3.2601\n",
      "Val loss:\n",
      "[3.31315221]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31344885]\n",
      "Epoch [55/100], Train Cost: 3.2598\n",
      "Val loss:\n",
      "[3.31285557]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31315221]\n",
      "Epoch [56/100], Train Cost: 3.2595\n",
      "Val loss:\n",
      "[3.31255893]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31285557]\n",
      "Epoch [57/100], Train Cost: 3.2592\n",
      "Val loss:\n",
      "[3.3122623]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31255893]\n",
      "Epoch [58/100], Train Cost: 3.2589\n",
      "Val loss:\n",
      "[3.31196566]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3122623]\n",
      "Epoch [59/100], Train Cost: 3.2586\n",
      "Val loss:\n",
      "[3.31166903]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31196566]\n",
      "Epoch [60/100], Train Cost: 3.2583\n",
      "Val loss:\n",
      "[3.3113724]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31166903]\n",
      "Epoch [61/100], Train Cost: 3.2580\n",
      "Val loss:\n",
      "[3.31107577]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3113724]\n",
      "Epoch [62/100], Train Cost: 3.2577\n",
      "Val loss:\n",
      "[3.31077914]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31107577]\n",
      "Epoch [63/100], Train Cost: 3.2574\n",
      "Val loss:\n",
      "[3.31048251]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31077914]\n",
      "Epoch [64/100], Train Cost: 3.2571\n",
      "Val loss:\n",
      "[3.31018587]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31048251]\n",
      "Epoch [65/100], Train Cost: 3.2568\n",
      "Val loss:\n",
      "[3.3098892]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31018587]\n",
      "Epoch [66/100], Train Cost: 3.2565\n",
      "Val loss:\n",
      "[3.30959253]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3098892]\n",
      "Epoch [67/100], Train Cost: 3.2562\n",
      "Val loss:\n",
      "[3.30929587]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30959253]\n",
      "Epoch [68/100], Train Cost: 3.2559\n",
      "Val loss:\n",
      "[3.30899921]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30929587]\n",
      "Epoch [69/100], Train Cost: 3.2556\n",
      "Val loss:\n",
      "[3.30870252]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30899921]\n",
      "Epoch [70/100], Train Cost: 3.2553\n",
      "Val loss:\n",
      "[3.30840583]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30870252]\n",
      "Epoch [71/100], Train Cost: 3.2550\n",
      "Val loss:\n",
      "[3.30810914]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30840583]\n",
      "Epoch [72/100], Train Cost: 3.2547\n",
      "Val loss:\n",
      "[3.30781246]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30810914]\n",
      "Epoch [73/100], Train Cost: 3.2544\n",
      "Val loss:\n",
      "[3.30751578]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30781246]\n",
      "Epoch [74/100], Train Cost: 3.2541\n",
      "Val loss:\n",
      "[3.30721909]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30751578]\n",
      "Epoch [75/100], Train Cost: 3.2538\n",
      "Val loss:\n",
      "[3.30692239]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30721909]\n",
      "Epoch [76/100], Train Cost: 3.2535\n",
      "Val loss:\n",
      "[3.30662568]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30692239]\n",
      "Epoch [77/100], Train Cost: 3.2532\n",
      "Val loss:\n",
      "[3.306329]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30662568]\n",
      "Epoch [78/100], Train Cost: 3.2529\n",
      "Val loss:\n",
      "[3.30603229]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.306329]\n",
      "Epoch [79/100], Train Cost: 3.2526\n",
      "Val loss:\n",
      "[3.30573559]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30603229]\n",
      "Epoch [80/100], Train Cost: 3.2523\n",
      "Val loss:\n",
      "[3.30543889]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30573559]\n",
      "Epoch [81/100], Train Cost: 3.2520\n",
      "Val loss:\n",
      "[3.30514219]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30543889]\n",
      "Epoch [82/100], Train Cost: 3.2517\n",
      "Val loss:\n",
      "[3.30484549]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30514219]\n",
      "Epoch [83/100], Train Cost: 3.2514\n",
      "Val loss:\n",
      "[3.30454879]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30484549]\n",
      "Epoch [84/100], Train Cost: 3.2511\n",
      "Val loss:\n",
      "[3.30425208]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30454879]\n",
      "Epoch [85/100], Train Cost: 3.2508\n",
      "Val loss:\n",
      "[3.30395536]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30425208]\n",
      "Epoch [86/100], Train Cost: 3.2505\n",
      "Val loss:\n",
      "[3.30365865]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30395536]\n",
      "Epoch [87/100], Train Cost: 3.2502\n",
      "Val loss:\n",
      "[3.30336194]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30365865]\n",
      "Epoch [88/100], Train Cost: 3.2499\n",
      "Val loss:\n",
      "[3.30306521]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30336194]\n",
      "Epoch [89/100], Train Cost: 3.2496\n",
      "Val loss:\n",
      "[3.30276846]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30306521]\n",
      "Epoch [90/100], Train Cost: 3.2492\n",
      "Val loss:\n",
      "[3.30247171]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30276846]\n",
      "Epoch [91/100], Train Cost: 3.2489\n",
      "Val loss:\n",
      "[3.30217494]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30247171]\n",
      "Epoch [92/100], Train Cost: 3.2486\n",
      "Val loss:\n",
      "[3.30187816]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30217494]\n",
      "Epoch [93/100], Train Cost: 3.2483\n",
      "Val loss:\n",
      "[3.30158138]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30187816]\n",
      "Epoch [94/100], Train Cost: 3.2480\n",
      "Val loss:\n",
      "[3.3012846]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30158138]\n",
      "Epoch [95/100], Train Cost: 3.2477\n",
      "Val loss:\n",
      "[3.30098782]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3012846]\n",
      "Epoch [96/100], Train Cost: 3.2474\n",
      "Val loss:\n",
      "[3.30069105]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30098782]\n",
      "Epoch [97/100], Train Cost: 3.2471\n",
      "Val loss:\n",
      "[3.30039428]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30069105]\n",
      "Epoch [98/100], Train Cost: 3.2468\n",
      "Val loss:\n",
      "[3.30009751]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30039428]\n",
      "Epoch [99/100], Train Cost: 3.2465\n",
      "Val loss:\n",
      "[3.29980074]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30009751]\n",
      "Epoch [100/100], Train Cost: 3.2462\n",
      "Val loss:\n",
      "[3.29950398]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.29980074]\n",
      "\n",
      "Hyperparameters: Learning Rate=8e-09, Neurons=15\n",
      "Final Train Loss: 3.2462\n",
      "Final Validation Loss: 3.2995\n",
      "Test Accuracy: 6.67%\n",
      "\n",
      "Epoch [1/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.98622308]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.9863087]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98622308]\n",
      "Epoch [3/100], Train Cost: -3.0368\n",
      "Val loss:\n",
      "[-2.98639432]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9863087]\n",
      "Epoch [4/100], Train Cost: -3.0369\n",
      "Val loss:\n",
      "[-2.98647994]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98639432]\n",
      "Epoch [5/100], Train Cost: -3.0370\n",
      "Val loss:\n",
      "[-2.98656556]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98647994]\n",
      "Epoch [6/100], Train Cost: -3.0371\n",
      "Val loss:\n",
      "[-2.98665118]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98656556]\n",
      "Epoch [7/100], Train Cost: -3.0372\n",
      "Val loss:\n",
      "[-2.9867368]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98665118]\n",
      "Epoch [8/100], Train Cost: -3.0373\n",
      "Val loss:\n",
      "[-2.98682241]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9867368]\n",
      "Epoch [9/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98690803]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98682241]\n",
      "Epoch [10/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98699365]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98690803]\n",
      "Epoch [11/100], Train Cost: -3.0375\n",
      "Val loss:\n",
      "[-2.98707928]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98699365]\n",
      "Epoch [12/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.98716491]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98707928]\n",
      "Epoch [13/100], Train Cost: -3.0377\n",
      "Val loss:\n",
      "[-2.98725055]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98716491]\n",
      "Epoch [14/100], Train Cost: -3.0378\n",
      "Val loss:\n",
      "[-2.98733618]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98725055]\n",
      "Epoch [15/100], Train Cost: -3.0379\n",
      "Val loss:\n",
      "[-2.98742181]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98733618]\n",
      "Epoch [16/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98750744]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98742181]\n",
      "Epoch [17/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98759307]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98750744]\n",
      "Epoch [18/100], Train Cost: -3.0381\n",
      "Val loss:\n",
      "[-2.98767871]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98759307]\n",
      "Epoch [19/100], Train Cost: -3.0382\n",
      "Val loss:\n",
      "[-2.98776434]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98767871]\n",
      "Epoch [20/100], Train Cost: -3.0383\n",
      "Val loss:\n",
      "[-2.98784999]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98776434]\n",
      "Epoch [21/100], Train Cost: -3.0384\n",
      "Val loss:\n",
      "[-2.98793563]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98784999]\n",
      "Epoch [22/100], Train Cost: -3.0385\n",
      "Val loss:\n",
      "[-2.98802128]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98793563]\n",
      "Epoch [23/100], Train Cost: -3.0386\n",
      "Val loss:\n",
      "[-2.98810693]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98802128]\n",
      "Epoch [24/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98819258]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98810693]\n",
      "Epoch [25/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98827824]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98819258]\n",
      "Epoch [26/100], Train Cost: -3.0388\n",
      "Val loss:\n",
      "[-2.98836389]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98827824]\n",
      "Epoch [27/100], Train Cost: -3.0389\n",
      "Val loss:\n",
      "[-2.98844955]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98836389]\n",
      "Epoch [28/100], Train Cost: -3.0390\n",
      "Val loss:\n",
      "[-2.9885352]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98844955]\n",
      "Epoch [29/100], Train Cost: -3.0391\n",
      "Val loss:\n",
      "[-2.98862085]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9885352]\n",
      "Epoch [30/100], Train Cost: -3.0392\n",
      "Val loss:\n",
      "[-2.98870651]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98862085]\n",
      "Epoch [31/100], Train Cost: -3.0393\n",
      "Val loss:\n",
      "[-2.98879216]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98870651]\n",
      "Epoch [32/100], Train Cost: -3.0393\n",
      "Val loss:\n",
      "[-2.98887782]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98879216]\n",
      "Epoch [33/100], Train Cost: -3.0394\n",
      "Val loss:\n",
      "[-2.98896348]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98887782]\n",
      "Epoch [34/100], Train Cost: -3.0395\n",
      "Val loss:\n",
      "[-2.98904914]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98896348]\n",
      "Epoch [35/100], Train Cost: -3.0396\n",
      "Val loss:\n",
      "[-2.98913481]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98904914]\n",
      "Epoch [36/100], Train Cost: -3.0397\n",
      "Val loss:\n",
      "[-2.98922047]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98913481]\n",
      "Epoch [37/100], Train Cost: -3.0398\n",
      "Val loss:\n",
      "[-2.98930613]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98922047]\n",
      "Epoch [38/100], Train Cost: -3.0399\n",
      "Val loss:\n",
      "[-2.98939179]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98930613]\n",
      "Epoch [39/100], Train Cost: -3.0400\n",
      "Val loss:\n",
      "[-2.98947744]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98939179]\n",
      "Epoch [40/100], Train Cost: -3.0400\n",
      "Val loss:\n",
      "[-2.98956309]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98947744]\n",
      "Epoch [41/100], Train Cost: -3.0401\n",
      "Val loss:\n",
      "[-2.98964874]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98956309]\n",
      "Epoch [42/100], Train Cost: -3.0402\n",
      "Val loss:\n",
      "[-2.98973439]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98964874]\n",
      "Epoch [43/100], Train Cost: -3.0403\n",
      "Val loss:\n",
      "[-2.98982003]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98973439]\n",
      "Epoch [44/100], Train Cost: -3.0404\n",
      "Val loss:\n",
      "[-2.98990568]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98982003]\n",
      "Epoch [45/100], Train Cost: -3.0405\n",
      "Val loss:\n",
      "[-2.98999133]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98990568]\n",
      "Epoch [46/100], Train Cost: -3.0406\n",
      "Val loss:\n",
      "[-2.99007698]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98999133]\n",
      "Epoch [47/100], Train Cost: -3.0407\n",
      "Val loss:\n",
      "[-2.99016262]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99007698]\n",
      "Epoch [48/100], Train Cost: -3.0407\n",
      "Val loss:\n",
      "[-2.99024827]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99016262]\n",
      "Epoch [49/100], Train Cost: -3.0408\n",
      "Val loss:\n",
      "[-2.99033392]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99024827]\n",
      "Epoch [50/100], Train Cost: -3.0409\n",
      "Val loss:\n",
      "[-2.99041957]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99033392]\n",
      "Epoch [51/100], Train Cost: -3.0410\n",
      "Val loss:\n",
      "[-2.99050522]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99041957]\n",
      "Epoch [52/100], Train Cost: -3.0411\n",
      "Val loss:\n",
      "[-2.99059086]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99050522]\n",
      "Epoch [53/100], Train Cost: -3.0412\n",
      "Val loss:\n",
      "[-2.99067651]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99059086]\n",
      "Epoch [54/100], Train Cost: -3.0413\n",
      "Val loss:\n",
      "[-2.99076216]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99067651]\n",
      "Epoch [55/100], Train Cost: -3.0413\n",
      "Val loss:\n",
      "[-2.99084781]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99076216]\n",
      "Epoch [56/100], Train Cost: -3.0414\n",
      "Val loss:\n",
      "[-2.99093346]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99084781]\n",
      "Epoch [57/100], Train Cost: -3.0415\n",
      "Val loss:\n",
      "[-2.99101911]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99093346]\n",
      "Epoch [58/100], Train Cost: -3.0416\n",
      "Val loss:\n",
      "[-2.99110475]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99101911]\n",
      "Epoch [59/100], Train Cost: -3.0417\n",
      "Val loss:\n",
      "[-2.9911904]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99110475]\n",
      "Epoch [60/100], Train Cost: -3.0418\n",
      "Val loss:\n",
      "[-2.99127605]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9911904]\n",
      "Epoch [61/100], Train Cost: -3.0419\n",
      "Val loss:\n",
      "[-2.9913617]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99127605]\n",
      "Epoch [62/100], Train Cost: -3.0420\n",
      "Val loss:\n",
      "[-2.99144735]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9913617]\n",
      "Epoch [63/100], Train Cost: -3.0420\n",
      "Val loss:\n",
      "[-2.99153299]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99144735]\n",
      "Epoch [64/100], Train Cost: -3.0421\n",
      "Val loss:\n",
      "[-2.99161864]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99153299]\n",
      "Epoch [65/100], Train Cost: -3.0422\n",
      "Val loss:\n",
      "[-2.99170429]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99161864]\n",
      "Epoch [66/100], Train Cost: -3.0423\n",
      "Val loss:\n",
      "[-2.99178994]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99170429]\n",
      "Epoch [67/100], Train Cost: -3.0424\n",
      "Val loss:\n",
      "[-2.99187558]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99178994]\n",
      "Epoch [68/100], Train Cost: -3.0425\n",
      "Val loss:\n",
      "[-2.99196123]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99187558]\n",
      "Epoch [69/100], Train Cost: -3.0426\n",
      "Val loss:\n",
      "[-2.99204688]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99196123]\n",
      "Epoch [70/100], Train Cost: -3.0426\n",
      "Val loss:\n",
      "[-2.99213253]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99204688]\n",
      "Epoch [71/100], Train Cost: -3.0427\n",
      "Val loss:\n",
      "[-2.99221817]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99213253]\n",
      "Epoch [72/100], Train Cost: -3.0428\n",
      "Val loss:\n",
      "[-2.99230382]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99221817]\n",
      "Epoch [73/100], Train Cost: -3.0429\n",
      "Val loss:\n",
      "[-2.99238947]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99230382]\n",
      "Epoch [74/100], Train Cost: -3.0430\n",
      "Val loss:\n",
      "[-2.99247512]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99238947]\n",
      "Epoch [75/100], Train Cost: -3.0431\n",
      "Val loss:\n",
      "[-2.99256077]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99247512]\n",
      "Epoch [76/100], Train Cost: -3.0432\n",
      "Val loss:\n",
      "[-2.99264641]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99256077]\n",
      "Epoch [77/100], Train Cost: -3.0433\n",
      "Val loss:\n",
      "[-2.99273206]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99264641]\n",
      "Epoch [78/100], Train Cost: -3.0433\n",
      "Val loss:\n",
      "[-2.99281771]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99273206]\n",
      "Epoch [79/100], Train Cost: -3.0434\n",
      "Val loss:\n",
      "[-2.99290336]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99281771]\n",
      "Epoch [80/100], Train Cost: -3.0435\n",
      "Val loss:\n",
      "[-2.99298901]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99290336]\n",
      "Epoch [81/100], Train Cost: -3.0436\n",
      "Val loss:\n",
      "[-2.99307467]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99298901]\n",
      "Epoch [82/100], Train Cost: -3.0437\n",
      "Val loss:\n",
      "[-2.99316032]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99307467]\n",
      "Epoch [83/100], Train Cost: -3.0438\n",
      "Val loss:\n",
      "[-2.99324597]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99316032]\n",
      "Epoch [84/100], Train Cost: -3.0439\n",
      "Val loss:\n",
      "[-2.99333163]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99324597]\n",
      "Epoch [85/100], Train Cost: -3.0439\n",
      "Val loss:\n",
      "[-2.99341728]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99333163]\n",
      "Epoch [86/100], Train Cost: -3.0440\n",
      "Val loss:\n",
      "[-2.99350294]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99341728]\n",
      "Epoch [87/100], Train Cost: -3.0441\n",
      "Val loss:\n",
      "[-2.99358859]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99350294]\n",
      "Epoch [88/100], Train Cost: -3.0442\n",
      "Val loss:\n",
      "[-2.99367424]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99358859]\n",
      "Epoch [89/100], Train Cost: -3.0443\n",
      "Val loss:\n",
      "[-2.9937599]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99367424]\n",
      "Epoch [90/100], Train Cost: -3.0444\n",
      "Val loss:\n",
      "[-2.99384555]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9937599]\n",
      "Epoch [91/100], Train Cost: -3.0445\n",
      "Val loss:\n",
      "[-2.99393122]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99384555]\n",
      "Epoch [92/100], Train Cost: -3.0446\n",
      "Val loss:\n",
      "[-2.99401691]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99393122]\n",
      "Epoch [93/100], Train Cost: -3.0446\n",
      "Val loss:\n",
      "[-2.99410259]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99401691]\n",
      "Epoch [94/100], Train Cost: -3.0447\n",
      "Val loss:\n",
      "[-2.99418828]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99410259]\n",
      "Epoch [95/100], Train Cost: -3.0448\n",
      "Val loss:\n",
      "[-2.99427396]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99418828]\n",
      "Epoch [96/100], Train Cost: -3.0449\n",
      "Val loss:\n",
      "[-2.99435964]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99427396]\n",
      "Epoch [97/100], Train Cost: -3.0450\n",
      "Val loss:\n",
      "[-2.99444533]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99435964]\n",
      "Epoch [98/100], Train Cost: -3.0451\n",
      "Val loss:\n",
      "[-2.99453101]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99444533]\n",
      "Epoch [99/100], Train Cost: -3.0452\n",
      "Val loss:\n",
      "[-2.9946167]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99453101]\n",
      "Epoch [100/100], Train Cost: -3.0452\n",
      "Val loss:\n",
      "[-2.99470238]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9946167]\n",
      "\n",
      "Hyperparameters: Learning Rate=8e-09, Neurons=10\n",
      "Final Train Loss: -3.0452\n",
      "Final Validation Loss: -2.9947\n",
      "Test Accuracy: 6.67%\n",
      "\n",
      "Epoch [1/100], Train Cost: 4.1677\n",
      "Val loss:\n",
      "[4.17195788]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: 4.1676\n",
      "Val loss:\n",
      "[4.17183171]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17195788]\n",
      "Epoch [3/100], Train Cost: 4.1674\n",
      "Val loss:\n",
      "[4.17170554]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17183171]\n",
      "Epoch [4/100], Train Cost: 4.1673\n",
      "Val loss:\n",
      "[4.17157937]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17170554]\n",
      "Epoch [5/100], Train Cost: 4.1672\n",
      "Val loss:\n",
      "[4.1714532]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17157937]\n",
      "Epoch [6/100], Train Cost: 4.1670\n",
      "Val loss:\n",
      "[4.17132704]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1714532]\n",
      "Epoch [7/100], Train Cost: 4.1669\n",
      "Val loss:\n",
      "[4.17120087]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17132704]\n",
      "Epoch [8/100], Train Cost: 4.1668\n",
      "Val loss:\n",
      "[4.1710747]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17120087]\n",
      "Epoch [9/100], Train Cost: 4.1667\n",
      "Val loss:\n",
      "[4.17094853]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1710747]\n",
      "Epoch [10/100], Train Cost: 4.1665\n",
      "Val loss:\n",
      "[4.17082237]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17094853]\n",
      "Epoch [11/100], Train Cost: 4.1664\n",
      "Val loss:\n",
      "[4.1706962]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17082237]\n",
      "Epoch [12/100], Train Cost: 4.1663\n",
      "Val loss:\n",
      "[4.17057005]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1706962]\n",
      "Epoch [13/100], Train Cost: 4.1662\n",
      "Val loss:\n",
      "[4.17044395]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17057005]\n",
      "Epoch [14/100], Train Cost: 4.1660\n",
      "Val loss:\n",
      "[4.17031786]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17044395]\n",
      "Epoch [15/100], Train Cost: 4.1659\n",
      "Val loss:\n",
      "[4.17019176]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17031786]\n",
      "Epoch [16/100], Train Cost: 4.1658\n",
      "Val loss:\n",
      "[4.17006567]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17019176]\n",
      "Epoch [17/100], Train Cost: 4.1656\n",
      "Val loss:\n",
      "[4.16993958]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17006567]\n",
      "Epoch [18/100], Train Cost: 4.1655\n",
      "Val loss:\n",
      "[4.16981349]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16993958]\n",
      "Epoch [19/100], Train Cost: 4.1654\n",
      "Val loss:\n",
      "[4.1696874]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16981349]\n",
      "Epoch [20/100], Train Cost: 4.1653\n",
      "Val loss:\n",
      "[4.16956132]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1696874]\n",
      "Epoch [21/100], Train Cost: 4.1651\n",
      "Val loss:\n",
      "[4.16943525]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16956132]\n",
      "Epoch [22/100], Train Cost: 4.1650\n",
      "Val loss:\n",
      "[4.16930918]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16943525]\n",
      "Epoch [23/100], Train Cost: 4.1649\n",
      "Val loss:\n",
      "[4.1691831]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16930918]\n",
      "Epoch [24/100], Train Cost: 4.1648\n",
      "Val loss:\n",
      "[4.16905703]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1691831]\n",
      "Epoch [25/100], Train Cost: 4.1646\n",
      "Val loss:\n",
      "[4.16893096]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16905703]\n",
      "Epoch [26/100], Train Cost: 4.1645\n",
      "Val loss:\n",
      "[4.1688049]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16893096]\n",
      "Epoch [27/100], Train Cost: 4.1644\n",
      "Val loss:\n",
      "[4.16867885]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1688049]\n",
      "Epoch [28/100], Train Cost: 4.1643\n",
      "Val loss:\n",
      "[4.16855279]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16867885]\n",
      "Epoch [29/100], Train Cost: 4.1641\n",
      "Val loss:\n",
      "[4.16842674]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16855279]\n",
      "Epoch [30/100], Train Cost: 4.1640\n",
      "Val loss:\n",
      "[4.16830068]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16842674]\n",
      "Epoch [31/100], Train Cost: 4.1639\n",
      "Val loss:\n",
      "[4.16817463]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16830068]\n",
      "Epoch [32/100], Train Cost: 4.1637\n",
      "Val loss:\n",
      "[4.16804858]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16817463]\n",
      "Epoch [33/100], Train Cost: 4.1636\n",
      "Val loss:\n",
      "[4.16792257]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16804858]\n",
      "Epoch [34/100], Train Cost: 4.1635\n",
      "Val loss:\n",
      "[4.16779656]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16792257]\n",
      "Epoch [35/100], Train Cost: 4.1634\n",
      "Val loss:\n",
      "[4.16767056]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16779656]\n",
      "Epoch [36/100], Train Cost: 4.1632\n",
      "Val loss:\n",
      "[4.16754457]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16767056]\n",
      "Epoch [37/100], Train Cost: 4.1631\n",
      "Val loss:\n",
      "[4.16741858]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16754457]\n",
      "Epoch [38/100], Train Cost: 4.1630\n",
      "Val loss:\n",
      "[4.16729258]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16741858]\n",
      "Epoch [39/100], Train Cost: 4.1629\n",
      "Val loss:\n",
      "[4.16716659]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16729258]\n",
      "Epoch [40/100], Train Cost: 4.1627\n",
      "Val loss:\n",
      "[4.1670406]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16716659]\n",
      "Epoch [41/100], Train Cost: 4.1626\n",
      "Val loss:\n",
      "[4.1669146]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1670406]\n",
      "Epoch [42/100], Train Cost: 4.1625\n",
      "Val loss:\n",
      "[4.16678861]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1669146]\n",
      "Epoch [43/100], Train Cost: 4.1624\n",
      "Val loss:\n",
      "[4.16666264]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16678861]\n",
      "Epoch [44/100], Train Cost: 4.1622\n",
      "Val loss:\n",
      "[4.1665367]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16666264]\n",
      "Epoch [45/100], Train Cost: 4.1621\n",
      "Val loss:\n",
      "[4.16641078]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1665367]\n",
      "Epoch [46/100], Train Cost: 4.1620\n",
      "Val loss:\n",
      "[4.16628486]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16641078]\n",
      "Epoch [47/100], Train Cost: 4.1618\n",
      "Val loss:\n",
      "[4.16615895]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16628486]\n",
      "Epoch [48/100], Train Cost: 4.1617\n",
      "Val loss:\n",
      "[4.16603304]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16615895]\n",
      "Epoch [49/100], Train Cost: 4.1616\n",
      "Val loss:\n",
      "[4.16590713]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16603304]\n",
      "Epoch [50/100], Train Cost: 4.1615\n",
      "Val loss:\n",
      "[4.16578122]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16590713]\n",
      "Epoch [51/100], Train Cost: 4.1613\n",
      "Val loss:\n",
      "[4.16565533]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16578122]\n",
      "Epoch [52/100], Train Cost: 4.1612\n",
      "Val loss:\n",
      "[4.16552944]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16565533]\n",
      "Epoch [53/100], Train Cost: 4.1611\n",
      "Val loss:\n",
      "[4.16540354]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16552944]\n",
      "Epoch [54/100], Train Cost: 4.1610\n",
      "Val loss:\n",
      "[4.16527765]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16540354]\n",
      "Epoch [55/100], Train Cost: 4.1608\n",
      "Val loss:\n",
      "[4.16515177]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16527765]\n",
      "Epoch [56/100], Train Cost: 4.1607\n",
      "Val loss:\n",
      "[4.1650259]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16515177]\n",
      "Epoch [57/100], Train Cost: 4.1606\n",
      "Val loss:\n",
      "[4.16490002]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1650259]\n",
      "Epoch [58/100], Train Cost: 4.1605\n",
      "Val loss:\n",
      "[4.16477414]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16490002]\n",
      "Epoch [59/100], Train Cost: 4.1603\n",
      "Val loss:\n",
      "[4.16464827]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16477414]\n",
      "Epoch [60/100], Train Cost: 4.1602\n",
      "Val loss:\n",
      "[4.16452241]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16464827]\n",
      "Epoch [61/100], Train Cost: 4.1601\n",
      "Val loss:\n",
      "[4.16439654]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16452241]\n",
      "Epoch [62/100], Train Cost: 4.1599\n",
      "Val loss:\n",
      "[4.16427068]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16439654]\n",
      "Epoch [63/100], Train Cost: 4.1598\n",
      "Val loss:\n",
      "[4.16414482]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16427068]\n",
      "Epoch [64/100], Train Cost: 4.1597\n",
      "Val loss:\n",
      "[4.16401896]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16414482]\n",
      "Epoch [65/100], Train Cost: 4.1596\n",
      "Val loss:\n",
      "[4.1638931]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16401896]\n",
      "Epoch [66/100], Train Cost: 4.1594\n",
      "Val loss:\n",
      "[4.16376725]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1638931]\n",
      "Epoch [67/100], Train Cost: 4.1593\n",
      "Val loss:\n",
      "[4.16364139]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16376725]\n",
      "Epoch [68/100], Train Cost: 4.1592\n",
      "Val loss:\n",
      "[4.16351553]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16364139]\n",
      "Epoch [69/100], Train Cost: 4.1591\n",
      "Val loss:\n",
      "[4.16338967]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16351553]\n",
      "Epoch [70/100], Train Cost: 4.1589\n",
      "Val loss:\n",
      "[4.16326382]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16338967]\n",
      "Epoch [71/100], Train Cost: 4.1588\n",
      "Val loss:\n",
      "[4.16313796]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16326382]\n",
      "Epoch [72/100], Train Cost: 4.1587\n",
      "Val loss:\n",
      "[4.16301211]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16313796]\n",
      "Epoch [73/100], Train Cost: 4.1586\n",
      "Val loss:\n",
      "[4.16288625]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16301211]\n",
      "Epoch [74/100], Train Cost: 4.1584\n",
      "Val loss:\n",
      "[4.1627604]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16288625]\n",
      "Epoch [75/100], Train Cost: 4.1583\n",
      "Val loss:\n",
      "[4.16263454]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1627604]\n",
      "Epoch [76/100], Train Cost: 4.1582\n",
      "Val loss:\n",
      "[4.16250869]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16263454]\n",
      "Epoch [77/100], Train Cost: 4.1580\n",
      "Val loss:\n",
      "[4.16238283]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16250869]\n",
      "Epoch [78/100], Train Cost: 4.1579\n",
      "Val loss:\n",
      "[4.16225698]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16238283]\n",
      "Epoch [79/100], Train Cost: 4.1578\n",
      "Val loss:\n",
      "[4.16213113]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16225698]\n",
      "Epoch [80/100], Train Cost: 4.1577\n",
      "Val loss:\n",
      "[4.16200527]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16213113]\n",
      "Epoch [81/100], Train Cost: 4.1575\n",
      "Val loss:\n",
      "[4.16187942]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16200527]\n",
      "Epoch [82/100], Train Cost: 4.1574\n",
      "Val loss:\n",
      "[4.16175357]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16187942]\n",
      "Epoch [83/100], Train Cost: 4.1573\n",
      "Val loss:\n",
      "[4.16162771]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16175357]\n",
      "Epoch [84/100], Train Cost: 4.1572\n",
      "Val loss:\n",
      "[4.16150186]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16162771]\n",
      "Epoch [85/100], Train Cost: 4.1570\n",
      "Val loss:\n",
      "[4.16137601]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16150186]\n",
      "Epoch [86/100], Train Cost: 4.1569\n",
      "Val loss:\n",
      "[4.16125015]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16137601]\n",
      "Epoch [87/100], Train Cost: 4.1568\n",
      "Val loss:\n",
      "[4.1611243]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16125015]\n",
      "Epoch [88/100], Train Cost: 4.1567\n",
      "Val loss:\n",
      "[4.16099848]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1611243]\n",
      "Epoch [89/100], Train Cost: 4.1565\n",
      "Val loss:\n",
      "[4.16087268]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16099848]\n",
      "Epoch [90/100], Train Cost: 4.1564\n",
      "Val loss:\n",
      "[4.16074687]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16087268]\n",
      "Epoch [91/100], Train Cost: 4.1563\n",
      "Val loss:\n",
      "[4.16062107]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16074687]\n",
      "Epoch [92/100], Train Cost: 4.1562\n",
      "Val loss:\n",
      "[4.16049526]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16062107]\n",
      "Epoch [93/100], Train Cost: 4.1560\n",
      "Val loss:\n",
      "[4.16036947]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16049526]\n",
      "Epoch [94/100], Train Cost: 4.1559\n",
      "Val loss:\n",
      "[4.16024369]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16036947]\n",
      "Epoch [95/100], Train Cost: 4.1558\n",
      "Val loss:\n",
      "[4.1601179]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16024369]\n",
      "Epoch [96/100], Train Cost: 4.1556\n",
      "Val loss:\n",
      "[4.15999212]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1601179]\n",
      "Epoch [97/100], Train Cost: 4.1555\n",
      "Val loss:\n",
      "[4.15986634]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.15999212]\n",
      "Epoch [98/100], Train Cost: 4.1554\n",
      "Val loss:\n",
      "[4.15974055]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.15986634]\n",
      "Epoch [99/100], Train Cost: 4.1553\n",
      "Val loss:\n",
      "[4.15961477]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.15974055]\n",
      "Epoch [100/100], Train Cost: 4.1551\n",
      "Val loss:\n",
      "[4.15948899]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.15961477]\n",
      "\n",
      "Hyperparameters: Learning Rate=8e-09, Neurons=5\n",
      "Final Train Loss: 4.1551\n",
      "Final Validation Loss: 4.1595\n",
      "Test Accuracy: 6.76%\n",
      "\n",
      "Epoch [1/100], Train Cost: 3.2762\n",
      "Val loss:\n",
      "[3.32909068]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: 3.2761\n",
      "Val loss:\n",
      "[3.32901663]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32909068]\n",
      "Epoch [3/100], Train Cost: 3.2760\n",
      "Val loss:\n",
      "[3.32894257]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32901663]\n",
      "Epoch [4/100], Train Cost: 3.2760\n",
      "Val loss:\n",
      "[3.32886852]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32894257]\n",
      "Epoch [5/100], Train Cost: 3.2759\n",
      "Val loss:\n",
      "[3.32879446]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32886852]\n",
      "Epoch [6/100], Train Cost: 3.2758\n",
      "Val loss:\n",
      "[3.32872041]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32879446]\n",
      "Epoch [7/100], Train Cost: 3.2757\n",
      "Val loss:\n",
      "[3.32864635]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32872041]\n",
      "Epoch [8/100], Train Cost: 3.2757\n",
      "Val loss:\n",
      "[3.3285723]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32864635]\n",
      "Epoch [9/100], Train Cost: 3.2756\n",
      "Val loss:\n",
      "[3.32849825]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3285723]\n",
      "Epoch [10/100], Train Cost: 3.2755\n",
      "Val loss:\n",
      "[3.32842419]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32849825]\n",
      "Epoch [11/100], Train Cost: 3.2754\n",
      "Val loss:\n",
      "[3.32835014]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32842419]\n",
      "Epoch [12/100], Train Cost: 3.2754\n",
      "Val loss:\n",
      "[3.32827608]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32835014]\n",
      "Epoch [13/100], Train Cost: 3.2753\n",
      "Val loss:\n",
      "[3.32820203]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32827608]\n",
      "Epoch [14/100], Train Cost: 3.2752\n",
      "Val loss:\n",
      "[3.32812797]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32820203]\n",
      "Epoch [15/100], Train Cost: 3.2751\n",
      "Val loss:\n",
      "[3.32805392]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32812797]\n",
      "Epoch [16/100], Train Cost: 3.2751\n",
      "Val loss:\n",
      "[3.32797987]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32805392]\n",
      "Epoch [17/100], Train Cost: 3.2750\n",
      "Val loss:\n",
      "[3.32790581]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32797987]\n",
      "Epoch [18/100], Train Cost: 3.2749\n",
      "Val loss:\n",
      "[3.32783174]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32790581]\n",
      "Epoch [19/100], Train Cost: 3.2748\n",
      "Val loss:\n",
      "[3.32775767]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32783174]\n",
      "Epoch [20/100], Train Cost: 3.2747\n",
      "Val loss:\n",
      "[3.32768361]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32775767]\n",
      "Epoch [21/100], Train Cost: 3.2747\n",
      "Val loss:\n",
      "[3.32760954]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32768361]\n",
      "Epoch [22/100], Train Cost: 3.2746\n",
      "Val loss:\n",
      "[3.32753547]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32760954]\n",
      "Epoch [23/100], Train Cost: 3.2745\n",
      "Val loss:\n",
      "[3.32746141]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32753547]\n",
      "Epoch [24/100], Train Cost: 3.2744\n",
      "Val loss:\n",
      "[3.32738734]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32746141]\n",
      "Epoch [25/100], Train Cost: 3.2744\n",
      "Val loss:\n",
      "[3.32731328]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32738734]\n",
      "Epoch [26/100], Train Cost: 3.2743\n",
      "Val loss:\n",
      "[3.32723921]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32731328]\n",
      "Epoch [27/100], Train Cost: 3.2742\n",
      "Val loss:\n",
      "[3.32716515]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32723921]\n",
      "Epoch [28/100], Train Cost: 3.2741\n",
      "Val loss:\n",
      "[3.32709105]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32716515]\n",
      "Epoch [29/100], Train Cost: 3.2741\n",
      "Val loss:\n",
      "[3.32701695]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32709105]\n",
      "Epoch [30/100], Train Cost: 3.2740\n",
      "Val loss:\n",
      "[3.32694285]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32701695]\n",
      "Epoch [31/100], Train Cost: 3.2739\n",
      "Val loss:\n",
      "[3.32686875]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32694285]\n",
      "Epoch [32/100], Train Cost: 3.2738\n",
      "Val loss:\n",
      "[3.32679465]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32686875]\n",
      "Epoch [33/100], Train Cost: 3.2738\n",
      "Val loss:\n",
      "[3.32672054]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32679465]\n",
      "Epoch [34/100], Train Cost: 3.2737\n",
      "Val loss:\n",
      "[3.32664643]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32672054]\n",
      "Epoch [35/100], Train Cost: 3.2736\n",
      "Val loss:\n",
      "[3.32657231]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32664643]\n",
      "Epoch [36/100], Train Cost: 3.2735\n",
      "Val loss:\n",
      "[3.32649819]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32657231]\n",
      "Epoch [37/100], Train Cost: 3.2735\n",
      "Val loss:\n",
      "[3.32642407]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32649819]\n",
      "Epoch [38/100], Train Cost: 3.2734\n",
      "Val loss:\n",
      "[3.32634995]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32642407]\n",
      "Epoch [39/100], Train Cost: 3.2733\n",
      "Val loss:\n",
      "[3.32627583]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32634995]\n",
      "Epoch [40/100], Train Cost: 3.2732\n",
      "Val loss:\n",
      "[3.32620171]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32627583]\n",
      "Epoch [41/100], Train Cost: 3.2732\n",
      "Val loss:\n",
      "[3.32612759]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32620171]\n",
      "Epoch [42/100], Train Cost: 3.2731\n",
      "Val loss:\n",
      "[3.32605346]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32612759]\n",
      "Epoch [43/100], Train Cost: 3.2730\n",
      "Val loss:\n",
      "[3.32597934]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32605346]\n",
      "Epoch [44/100], Train Cost: 3.2729\n",
      "Val loss:\n",
      "[3.32590521]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32597934]\n",
      "Epoch [45/100], Train Cost: 3.2729\n",
      "Val loss:\n",
      "[3.32583109]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32590521]\n",
      "Epoch [46/100], Train Cost: 3.2728\n",
      "Val loss:\n",
      "[3.32575696]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32583109]\n",
      "Epoch [47/100], Train Cost: 3.2727\n",
      "Val loss:\n",
      "[3.32568282]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32575696]\n",
      "Epoch [48/100], Train Cost: 3.2726\n",
      "Val loss:\n",
      "[3.32560869]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32568282]\n",
      "Epoch [49/100], Train Cost: 3.2726\n",
      "Val loss:\n",
      "[3.32553456]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32560869]\n",
      "Epoch [50/100], Train Cost: 3.2725\n",
      "Val loss:\n",
      "[3.32546043]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32553456]\n",
      "Epoch [51/100], Train Cost: 3.2724\n",
      "Val loss:\n",
      "[3.32538629]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32546043]\n",
      "Epoch [52/100], Train Cost: 3.2723\n",
      "Val loss:\n",
      "[3.32531216]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32538629]\n",
      "Epoch [53/100], Train Cost: 3.2723\n",
      "Val loss:\n",
      "[3.32523803]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32531216]\n",
      "Epoch [54/100], Train Cost: 3.2722\n",
      "Val loss:\n",
      "[3.3251639]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32523803]\n",
      "Epoch [55/100], Train Cost: 3.2721\n",
      "Val loss:\n",
      "[3.32508976]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3251639]\n",
      "Epoch [56/100], Train Cost: 3.2720\n",
      "Val loss:\n",
      "[3.32501563]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32508976]\n",
      "Epoch [57/100], Train Cost: 3.2720\n",
      "Val loss:\n",
      "[3.3249415]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32501563]\n",
      "Epoch [58/100], Train Cost: 3.2719\n",
      "Val loss:\n",
      "[3.32486737]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3249415]\n",
      "Epoch [59/100], Train Cost: 3.2718\n",
      "Val loss:\n",
      "[3.32479323]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32486737]\n",
      "Epoch [60/100], Train Cost: 3.2717\n",
      "Val loss:\n",
      "[3.3247191]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32479323]\n",
      "Epoch [61/100], Train Cost: 3.2716\n",
      "Val loss:\n",
      "[3.32464497]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3247191]\n",
      "Epoch [62/100], Train Cost: 3.2716\n",
      "Val loss:\n",
      "[3.32457084]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32464497]\n",
      "Epoch [63/100], Train Cost: 3.2715\n",
      "Val loss:\n",
      "[3.32449671]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32457084]\n",
      "Epoch [64/100], Train Cost: 3.2714\n",
      "Val loss:\n",
      "[3.32442258]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32449671]\n",
      "Epoch [65/100], Train Cost: 3.2713\n",
      "Val loss:\n",
      "[3.32434845]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32442258]\n",
      "Epoch [66/100], Train Cost: 3.2713\n",
      "Val loss:\n",
      "[3.32427431]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32434845]\n",
      "Epoch [67/100], Train Cost: 3.2712\n",
      "Val loss:\n",
      "[3.32420018]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32427431]\n",
      "Epoch [68/100], Train Cost: 3.2711\n",
      "Val loss:\n",
      "[3.32412605]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32420018]\n",
      "Epoch [69/100], Train Cost: 3.2710\n",
      "Val loss:\n",
      "[3.32405192]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32412605]\n",
      "Epoch [70/100], Train Cost: 3.2710\n",
      "Val loss:\n",
      "[3.32397779]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32405192]\n",
      "Epoch [71/100], Train Cost: 3.2709\n",
      "Val loss:\n",
      "[3.32390366]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32397779]\n",
      "Epoch [72/100], Train Cost: 3.2708\n",
      "Val loss:\n",
      "[3.32382953]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32390366]\n",
      "Epoch [73/100], Train Cost: 3.2707\n",
      "Val loss:\n",
      "[3.3237554]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32382953]\n",
      "Epoch [74/100], Train Cost: 3.2707\n",
      "Val loss:\n",
      "[3.32368127]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3237554]\n",
      "Epoch [75/100], Train Cost: 3.2706\n",
      "Val loss:\n",
      "[3.32360714]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32368127]\n",
      "Epoch [76/100], Train Cost: 3.2705\n",
      "Val loss:\n",
      "[3.323533]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32360714]\n",
      "Epoch [77/100], Train Cost: 3.2704\n",
      "Val loss:\n",
      "[3.32345887]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.323533]\n",
      "Epoch [78/100], Train Cost: 3.2704\n",
      "Val loss:\n",
      "[3.32338473]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32345887]\n",
      "Epoch [79/100], Train Cost: 3.2703\n",
      "Val loss:\n",
      "[3.3233106]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32338473]\n",
      "Epoch [80/100], Train Cost: 3.2702\n",
      "Val loss:\n",
      "[3.32323647]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3233106]\n",
      "Epoch [81/100], Train Cost: 3.2701\n",
      "Val loss:\n",
      "[3.32316233]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32323647]\n",
      "Epoch [82/100], Train Cost: 3.2701\n",
      "Val loss:\n",
      "[3.3230882]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32316233]\n",
      "Epoch [83/100], Train Cost: 3.2700\n",
      "Val loss:\n",
      "[3.32301406]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3230882]\n",
      "Epoch [84/100], Train Cost: 3.2699\n",
      "Val loss:\n",
      "[3.32293993]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32301406]\n",
      "Epoch [85/100], Train Cost: 3.2698\n",
      "Val loss:\n",
      "[3.32286579]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32293993]\n",
      "Epoch [86/100], Train Cost: 3.2698\n",
      "Val loss:\n",
      "[3.32279166]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32286579]\n",
      "Epoch [87/100], Train Cost: 3.2697\n",
      "Val loss:\n",
      "[3.32271752]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32279166]\n",
      "Epoch [88/100], Train Cost: 3.2696\n",
      "Val loss:\n",
      "[3.32264339]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32271752]\n",
      "Epoch [89/100], Train Cost: 3.2695\n",
      "Val loss:\n",
      "[3.32256926]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32264339]\n",
      "Epoch [90/100], Train Cost: 3.2695\n",
      "Val loss:\n",
      "[3.32249512]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32256926]\n",
      "Epoch [91/100], Train Cost: 3.2694\n",
      "Val loss:\n",
      "[3.32242099]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32249512]\n",
      "Epoch [92/100], Train Cost: 3.2693\n",
      "Val loss:\n",
      "[3.32234685]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32242099]\n",
      "Epoch [93/100], Train Cost: 3.2692\n",
      "Val loss:\n",
      "[3.32227272]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32234685]\n",
      "Epoch [94/100], Train Cost: 3.2692\n",
      "Val loss:\n",
      "[3.32219859]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32227272]\n",
      "Epoch [95/100], Train Cost: 3.2691\n",
      "Val loss:\n",
      "[3.32212445]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32219859]\n",
      "Epoch [96/100], Train Cost: 3.2690\n",
      "Val loss:\n",
      "[3.32205032]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32212445]\n",
      "Epoch [97/100], Train Cost: 3.2689\n",
      "Val loss:\n",
      "[3.32197618]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32205032]\n",
      "Epoch [98/100], Train Cost: 3.2688\n",
      "Val loss:\n",
      "[3.32190205]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32197618]\n",
      "Epoch [99/100], Train Cost: 3.2688\n",
      "Val loss:\n",
      "[3.32182792]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32190205]\n",
      "Epoch [100/100], Train Cost: 3.2687\n",
      "Val loss:\n",
      "[3.32175378]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32182792]\n",
      "\n",
      "Hyperparameters: Learning Rate=2e-09, Neurons=15\n",
      "Final Train Loss: 3.2687\n",
      "Final Validation Loss: 3.3218\n",
      "Test Accuracy: 6.67%\n",
      "\n",
      "Epoch [1/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.98615887]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.98618027]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98615887]\n",
      "Epoch [3/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.98620168]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98618027]\n",
      "Epoch [4/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.98622308]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98620168]\n",
      "Epoch [5/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.98624449]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98622308]\n",
      "Epoch [6/100], Train Cost: -3.0368\n",
      "Val loss:\n",
      "[-2.98626589]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98624449]\n",
      "Epoch [7/100], Train Cost: -3.0368\n",
      "Val loss:\n",
      "[-2.9862873]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98626589]\n",
      "Epoch [8/100], Train Cost: -3.0368\n",
      "Val loss:\n",
      "[-2.9863087]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9862873]\n",
      "Epoch [9/100], Train Cost: -3.0368\n",
      "Val loss:\n",
      "[-2.98633011]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9863087]\n",
      "Epoch [10/100], Train Cost: -3.0369\n",
      "Val loss:\n",
      "[-2.98635151]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98633011]\n",
      "Epoch [11/100], Train Cost: -3.0369\n",
      "Val loss:\n",
      "[-2.98637292]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98635151]\n",
      "Epoch [12/100], Train Cost: -3.0369\n",
      "Val loss:\n",
      "[-2.98639432]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98637292]\n",
      "Epoch [13/100], Train Cost: -3.0369\n",
      "Val loss:\n",
      "[-2.98641573]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98639432]\n",
      "Epoch [14/100], Train Cost: -3.0369\n",
      "Val loss:\n",
      "[-2.98643713]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98641573]\n",
      "Epoch [15/100], Train Cost: -3.0370\n",
      "Val loss:\n",
      "[-2.98645854]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98643713]\n",
      "Epoch [16/100], Train Cost: -3.0370\n",
      "Val loss:\n",
      "[-2.98647994]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98645854]\n",
      "Epoch [17/100], Train Cost: -3.0370\n",
      "Val loss:\n",
      "[-2.98650135]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98647994]\n",
      "Epoch [18/100], Train Cost: -3.0370\n",
      "Val loss:\n",
      "[-2.98652275]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98650135]\n",
      "Epoch [19/100], Train Cost: -3.0371\n",
      "Val loss:\n",
      "[-2.98654416]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98652275]\n",
      "Epoch [20/100], Train Cost: -3.0371\n",
      "Val loss:\n",
      "[-2.98656556]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98654416]\n",
      "Epoch [21/100], Train Cost: -3.0371\n",
      "Val loss:\n",
      "[-2.98658697]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98656556]\n",
      "Epoch [22/100], Train Cost: -3.0371\n",
      "Val loss:\n",
      "[-2.98660837]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98658697]\n",
      "Epoch [23/100], Train Cost: -3.0371\n",
      "Val loss:\n",
      "[-2.98662977]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98660837]\n",
      "Epoch [24/100], Train Cost: -3.0372\n",
      "Val loss:\n",
      "[-2.98665118]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98662977]\n",
      "Epoch [25/100], Train Cost: -3.0372\n",
      "Val loss:\n",
      "[-2.98667258]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98665118]\n",
      "Epoch [26/100], Train Cost: -3.0372\n",
      "Val loss:\n",
      "[-2.98669399]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98667258]\n",
      "Epoch [27/100], Train Cost: -3.0372\n",
      "Val loss:\n",
      "[-2.98671539]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98669399]\n",
      "Epoch [28/100], Train Cost: -3.0372\n",
      "Val loss:\n",
      "[-2.9867368]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98671539]\n",
      "Epoch [29/100], Train Cost: -3.0373\n",
      "Val loss:\n",
      "[-2.9867582]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9867368]\n",
      "Epoch [30/100], Train Cost: -3.0373\n",
      "Val loss:\n",
      "[-2.9867796]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9867582]\n",
      "Epoch [31/100], Train Cost: -3.0373\n",
      "Val loss:\n",
      "[-2.98680101]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9867796]\n",
      "Epoch [32/100], Train Cost: -3.0373\n",
      "Val loss:\n",
      "[-2.98682241]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98680101]\n",
      "Epoch [33/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98684382]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98682241]\n",
      "Epoch [34/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98686522]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98684382]\n",
      "Epoch [35/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98688663]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98686522]\n",
      "Epoch [36/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98690803]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98688663]\n",
      "Epoch [37/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98692944]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98690803]\n",
      "Epoch [38/100], Train Cost: -3.0375\n",
      "Val loss:\n",
      "[-2.98695084]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98692944]\n",
      "Epoch [39/100], Train Cost: -3.0375\n",
      "Val loss:\n",
      "[-2.98697225]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98695084]\n",
      "Epoch [40/100], Train Cost: -3.0375\n",
      "Val loss:\n",
      "[-2.98699366]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98697225]\n",
      "Epoch [41/100], Train Cost: -3.0375\n",
      "Val loss:\n",
      "[-2.98701507]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98699366]\n",
      "Epoch [42/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.98703647]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98701507]\n",
      "Epoch [43/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.98705788]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98703647]\n",
      "Epoch [44/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.98707929]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98705788]\n",
      "Epoch [45/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.9871007]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98707929]\n",
      "Epoch [46/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.98712211]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9871007]\n",
      "Epoch [47/100], Train Cost: -3.0377\n",
      "Val loss:\n",
      "[-2.98714351]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98712211]\n",
      "Epoch [48/100], Train Cost: -3.0377\n",
      "Val loss:\n",
      "[-2.98716492]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98714351]\n",
      "Epoch [49/100], Train Cost: -3.0377\n",
      "Val loss:\n",
      "[-2.98718633]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98716492]\n",
      "Epoch [50/100], Train Cost: -3.0377\n",
      "Val loss:\n",
      "[-2.98720774]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98718633]\n",
      "Epoch [51/100], Train Cost: -3.0377\n",
      "Val loss:\n",
      "[-2.98722914]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98720774]\n",
      "Epoch [52/100], Train Cost: -3.0378\n",
      "Val loss:\n",
      "[-2.98725055]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98722914]\n",
      "Epoch [53/100], Train Cost: -3.0378\n",
      "Val loss:\n",
      "[-2.98727196]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98725055]\n",
      "Epoch [54/100], Train Cost: -3.0378\n",
      "Val loss:\n",
      "[-2.98729337]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98727196]\n",
      "Epoch [55/100], Train Cost: -3.0378\n",
      "Val loss:\n",
      "[-2.98731478]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98729337]\n",
      "Epoch [56/100], Train Cost: -3.0379\n",
      "Val loss:\n",
      "[-2.98733618]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98731478]\n",
      "Epoch [57/100], Train Cost: -3.0379\n",
      "Val loss:\n",
      "[-2.98735759]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98733618]\n",
      "Epoch [58/100], Train Cost: -3.0379\n",
      "Val loss:\n",
      "[-2.987379]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98735759]\n",
      "Epoch [59/100], Train Cost: -3.0379\n",
      "Val loss:\n",
      "[-2.98740041]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.987379]\n",
      "Epoch [60/100], Train Cost: -3.0379\n",
      "Val loss:\n",
      "[-2.98742181]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98740041]\n",
      "Epoch [61/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98744322]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98742181]\n",
      "Epoch [62/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98746463]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98744322]\n",
      "Epoch [63/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98748604]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98746463]\n",
      "Epoch [64/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98750745]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98748604]\n",
      "Epoch [65/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98752886]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98750745]\n",
      "Epoch [66/100], Train Cost: -3.0381\n",
      "Val loss:\n",
      "[-2.98755026]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98752886]\n",
      "Epoch [67/100], Train Cost: -3.0381\n",
      "Val loss:\n",
      "[-2.98757167]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98755026]\n",
      "Epoch [68/100], Train Cost: -3.0381\n",
      "Val loss:\n",
      "[-2.98759308]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98757167]\n",
      "Epoch [69/100], Train Cost: -3.0381\n",
      "Val loss:\n",
      "[-2.98761449]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98759308]\n",
      "Epoch [70/100], Train Cost: -3.0382\n",
      "Val loss:\n",
      "[-2.9876359]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98761449]\n",
      "Epoch [71/100], Train Cost: -3.0382\n",
      "Val loss:\n",
      "[-2.98765731]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9876359]\n",
      "Epoch [72/100], Train Cost: -3.0382\n",
      "Val loss:\n",
      "[-2.98767872]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98765731]\n",
      "Epoch [73/100], Train Cost: -3.0382\n",
      "Val loss:\n",
      "[-2.98770013]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98767872]\n",
      "Epoch [74/100], Train Cost: -3.0382\n",
      "Val loss:\n",
      "[-2.98772153]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98770013]\n",
      "Epoch [75/100], Train Cost: -3.0383\n",
      "Val loss:\n",
      "[-2.98774294]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98772153]\n",
      "Epoch [76/100], Train Cost: -3.0383\n",
      "Val loss:\n",
      "[-2.98776435]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98774294]\n",
      "Epoch [77/100], Train Cost: -3.0383\n",
      "Val loss:\n",
      "[-2.98778576]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98776435]\n",
      "Epoch [78/100], Train Cost: -3.0383\n",
      "Val loss:\n",
      "[-2.98780717]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98778576]\n",
      "Epoch [79/100], Train Cost: -3.0384\n",
      "Val loss:\n",
      "[-2.98782858]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98780717]\n",
      "Epoch [80/100], Train Cost: -3.0384\n",
      "Val loss:\n",
      "[-2.98784999]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98782858]\n",
      "Epoch [81/100], Train Cost: -3.0384\n",
      "Val loss:\n",
      "[-2.98787141]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98784999]\n",
      "Epoch [82/100], Train Cost: -3.0384\n",
      "Val loss:\n",
      "[-2.98789282]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98787141]\n",
      "Epoch [83/100], Train Cost: -3.0384\n",
      "Val loss:\n",
      "[-2.98791423]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98789282]\n",
      "Epoch [84/100], Train Cost: -3.0385\n",
      "Val loss:\n",
      "[-2.98793564]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98791423]\n",
      "Epoch [85/100], Train Cost: -3.0385\n",
      "Val loss:\n",
      "[-2.98795705]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98793564]\n",
      "Epoch [86/100], Train Cost: -3.0385\n",
      "Val loss:\n",
      "[-2.98797846]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98795705]\n",
      "Epoch [87/100], Train Cost: -3.0385\n",
      "Val loss:\n",
      "[-2.98799988]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98797846]\n",
      "Epoch [88/100], Train Cost: -3.0385\n",
      "Val loss:\n",
      "[-2.98802129]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98799988]\n",
      "Epoch [89/100], Train Cost: -3.0386\n",
      "Val loss:\n",
      "[-2.9880427]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98802129]\n",
      "Epoch [90/100], Train Cost: -3.0386\n",
      "Val loss:\n",
      "[-2.98806411]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9880427]\n",
      "Epoch [91/100], Train Cost: -3.0386\n",
      "Val loss:\n",
      "[-2.98808553]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98806411]\n",
      "Epoch [92/100], Train Cost: -3.0386\n",
      "Val loss:\n",
      "[-2.98810694]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98808553]\n",
      "Epoch [93/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98812835]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98810694]\n",
      "Epoch [94/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98814977]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98812835]\n",
      "Epoch [95/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98817118]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98814977]\n",
      "Epoch [96/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98819259]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98817118]\n",
      "Epoch [97/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98821401]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98819259]\n",
      "Epoch [98/100], Train Cost: -3.0388\n",
      "Val loss:\n",
      "[-2.98823542]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98821401]\n",
      "Epoch [99/100], Train Cost: -3.0388\n",
      "Val loss:\n",
      "[-2.98825684]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98823542]\n",
      "Epoch [100/100], Train Cost: -3.0388\n",
      "Val loss:\n",
      "[-2.98827825]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98825684]\n",
      "\n",
      "Hyperparameters: Learning Rate=2e-09, Neurons=10\n",
      "Final Train Loss: -3.0388\n",
      "Final Validation Loss: -2.9883\n",
      "Test Accuracy: 6.67%\n",
      "\n",
      "Epoch [1/100], Train Cost: 4.1677\n",
      "Val loss:\n",
      "[4.17205251]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: 4.1676\n",
      "Val loss:\n",
      "[4.17202097]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17205251]\n",
      "Epoch [3/100], Train Cost: 4.1676\n",
      "Val loss:\n",
      "[4.17198942]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17202097]\n",
      "Epoch [4/100], Train Cost: 4.1676\n",
      "Val loss:\n",
      "[4.17195788]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17198942]\n",
      "Epoch [5/100], Train Cost: 4.1676\n",
      "Val loss:\n",
      "[4.17192634]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17195788]\n",
      "Epoch [6/100], Train Cost: 4.1675\n",
      "Val loss:\n",
      "[4.1718948]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17192634]\n",
      "Epoch [7/100], Train Cost: 4.1675\n",
      "Val loss:\n",
      "[4.17186325]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1718948]\n",
      "Epoch [8/100], Train Cost: 4.1675\n",
      "Val loss:\n",
      "[4.17183171]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17186325]\n",
      "Epoch [9/100], Train Cost: 4.1674\n",
      "Val loss:\n",
      "[4.17180017]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17183171]\n",
      "Epoch [10/100], Train Cost: 4.1674\n",
      "Val loss:\n",
      "[4.17176863]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17180017]\n",
      "Epoch [11/100], Train Cost: 4.1674\n",
      "Val loss:\n",
      "[4.17173708]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17176863]\n",
      "Epoch [12/100], Train Cost: 4.1673\n",
      "Val loss:\n",
      "[4.17170554]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17173708]\n",
      "Epoch [13/100], Train Cost: 4.1673\n",
      "Val loss:\n",
      "[4.171674]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17170554]\n",
      "Epoch [14/100], Train Cost: 4.1673\n",
      "Val loss:\n",
      "[4.17164246]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.171674]\n",
      "Epoch [15/100], Train Cost: 4.1672\n",
      "Val loss:\n",
      "[4.17161092]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17164246]\n",
      "Epoch [16/100], Train Cost: 4.1672\n",
      "Val loss:\n",
      "[4.17157937]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17161092]\n",
      "Epoch [17/100], Train Cost: 4.1672\n",
      "Val loss:\n",
      "[4.17154783]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17157937]\n",
      "Epoch [18/100], Train Cost: 4.1671\n",
      "Val loss:\n",
      "[4.17151629]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17154783]\n",
      "Epoch [19/100], Train Cost: 4.1671\n",
      "Val loss:\n",
      "[4.17148475]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17151629]\n",
      "Epoch [20/100], Train Cost: 4.1671\n",
      "Val loss:\n",
      "[4.1714532]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17148475]\n",
      "Epoch [21/100], Train Cost: 4.1670\n",
      "Val loss:\n",
      "[4.17142166]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1714532]\n",
      "Epoch [22/100], Train Cost: 4.1670\n",
      "Val loss:\n",
      "[4.17139012]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17142166]\n",
      "Epoch [23/100], Train Cost: 4.1670\n",
      "Val loss:\n",
      "[4.17135858]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17139012]\n",
      "Epoch [24/100], Train Cost: 4.1669\n",
      "Val loss:\n",
      "[4.17132704]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17135858]\n",
      "Epoch [25/100], Train Cost: 4.1669\n",
      "Val loss:\n",
      "[4.17129549]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17132704]\n",
      "Epoch [26/100], Train Cost: 4.1669\n",
      "Val loss:\n",
      "[4.17126395]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17129549]\n",
      "Epoch [27/100], Train Cost: 4.1669\n",
      "Val loss:\n",
      "[4.17123241]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17126395]\n",
      "Epoch [28/100], Train Cost: 4.1668\n",
      "Val loss:\n",
      "[4.17120087]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17123241]\n",
      "Epoch [29/100], Train Cost: 4.1668\n",
      "Val loss:\n",
      "[4.17116933]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17120087]\n",
      "Epoch [30/100], Train Cost: 4.1668\n",
      "Val loss:\n",
      "[4.17113778]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17116933]\n",
      "Epoch [31/100], Train Cost: 4.1667\n",
      "Val loss:\n",
      "[4.17110624]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17113778]\n",
      "Epoch [32/100], Train Cost: 4.1667\n",
      "Val loss:\n",
      "[4.1710747]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17110624]\n",
      "Epoch [33/100], Train Cost: 4.1667\n",
      "Val loss:\n",
      "[4.17104316]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1710747]\n",
      "Epoch [34/100], Train Cost: 4.1666\n",
      "Val loss:\n",
      "[4.17101162]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17104316]\n",
      "Epoch [35/100], Train Cost: 4.1666\n",
      "Val loss:\n",
      "[4.17098007]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17101162]\n",
      "Epoch [36/100], Train Cost: 4.1666\n",
      "Val loss:\n",
      "[4.17094853]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17098007]\n",
      "Epoch [37/100], Train Cost: 4.1665\n",
      "Val loss:\n",
      "[4.17091699]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17094853]\n",
      "Epoch [38/100], Train Cost: 4.1665\n",
      "Val loss:\n",
      "[4.17088545]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17091699]\n",
      "Epoch [39/100], Train Cost: 4.1665\n",
      "Val loss:\n",
      "[4.17085391]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17088545]\n",
      "Epoch [40/100], Train Cost: 4.1664\n",
      "Val loss:\n",
      "[4.17082237]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17085391]\n",
      "Epoch [41/100], Train Cost: 4.1664\n",
      "Val loss:\n",
      "[4.17079082]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17082237]\n",
      "Epoch [42/100], Train Cost: 4.1664\n",
      "Val loss:\n",
      "[4.17075928]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17079082]\n",
      "Epoch [43/100], Train Cost: 4.1663\n",
      "Val loss:\n",
      "[4.17072774]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17075928]\n",
      "Epoch [44/100], Train Cost: 4.1663\n",
      "Val loss:\n",
      "[4.1706962]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17072774]\n",
      "Epoch [45/100], Train Cost: 4.1663\n",
      "Val loss:\n",
      "[4.17066466]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1706962]\n",
      "Epoch [46/100], Train Cost: 4.1663\n",
      "Val loss:\n",
      "[4.17063312]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17066466]\n",
      "Epoch [47/100], Train Cost: 4.1662\n",
      "Val loss:\n",
      "[4.17060158]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17063312]\n",
      "Epoch [48/100], Train Cost: 4.1662\n",
      "Val loss:\n",
      "[4.17057005]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17060158]\n",
      "Epoch [49/100], Train Cost: 4.1662\n",
      "Val loss:\n",
      "[4.17053853]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17057005]\n",
      "Epoch [50/100], Train Cost: 4.1661\n",
      "Val loss:\n",
      "[4.170507]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17053853]\n",
      "Epoch [51/100], Train Cost: 4.1661\n",
      "Val loss:\n",
      "[4.17047548]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.170507]\n",
      "Epoch [52/100], Train Cost: 4.1661\n",
      "Val loss:\n",
      "[4.17044396]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17047548]\n",
      "Epoch [53/100], Train Cost: 4.1660\n",
      "Val loss:\n",
      "[4.17041243]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17044396]\n",
      "Epoch [54/100], Train Cost: 4.1660\n",
      "Val loss:\n",
      "[4.17038091]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17041243]\n",
      "Epoch [55/100], Train Cost: 4.1660\n",
      "Val loss:\n",
      "[4.17034939]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17038091]\n",
      "Epoch [56/100], Train Cost: 4.1659\n",
      "Val loss:\n",
      "[4.17031786]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17034939]\n",
      "Epoch [57/100], Train Cost: 4.1659\n",
      "Val loss:\n",
      "[4.17028634]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17031786]\n",
      "Epoch [58/100], Train Cost: 4.1659\n",
      "Val loss:\n",
      "[4.17025481]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17028634]\n",
      "Epoch [59/100], Train Cost: 4.1658\n",
      "Val loss:\n",
      "[4.17022329]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17025481]\n",
      "Epoch [60/100], Train Cost: 4.1658\n",
      "Val loss:\n",
      "[4.17019177]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17022329]\n",
      "Epoch [61/100], Train Cost: 4.1658\n",
      "Val loss:\n",
      "[4.17016025]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17019177]\n",
      "Epoch [62/100], Train Cost: 4.1657\n",
      "Val loss:\n",
      "[4.17012872]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17016025]\n",
      "Epoch [63/100], Train Cost: 4.1657\n",
      "Val loss:\n",
      "[4.1700972]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17012872]\n",
      "Epoch [64/100], Train Cost: 4.1657\n",
      "Val loss:\n",
      "[4.17006568]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1700972]\n",
      "Epoch [65/100], Train Cost: 4.1656\n",
      "Val loss:\n",
      "[4.17003415]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17006568]\n",
      "Epoch [66/100], Train Cost: 4.1656\n",
      "Val loss:\n",
      "[4.17000263]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17003415]\n",
      "Epoch [67/100], Train Cost: 4.1656\n",
      "Val loss:\n",
      "[4.16997111]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17000263]\n",
      "Epoch [68/100], Train Cost: 4.1656\n",
      "Val loss:\n",
      "[4.16993958]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16997111]\n",
      "Epoch [69/100], Train Cost: 4.1655\n",
      "Val loss:\n",
      "[4.16990806]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16993958]\n",
      "Epoch [70/100], Train Cost: 4.1655\n",
      "Val loss:\n",
      "[4.16987654]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16990806]\n",
      "Epoch [71/100], Train Cost: 4.1655\n",
      "Val loss:\n",
      "[4.16984502]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16987654]\n",
      "Epoch [72/100], Train Cost: 4.1654\n",
      "Val loss:\n",
      "[4.16981349]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16984502]\n",
      "Epoch [73/100], Train Cost: 4.1654\n",
      "Val loss:\n",
      "[4.16978197]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16981349]\n",
      "Epoch [74/100], Train Cost: 4.1654\n",
      "Val loss:\n",
      "[4.16975045]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16978197]\n",
      "Epoch [75/100], Train Cost: 4.1653\n",
      "Val loss:\n",
      "[4.16971893]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16975045]\n",
      "Epoch [76/100], Train Cost: 4.1653\n",
      "Val loss:\n",
      "[4.16968741]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16971893]\n",
      "Epoch [77/100], Train Cost: 4.1653\n",
      "Val loss:\n",
      "[4.16965589]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16968741]\n",
      "Epoch [78/100], Train Cost: 4.1652\n",
      "Val loss:\n",
      "[4.16962437]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16965589]\n",
      "Epoch [79/100], Train Cost: 4.1652\n",
      "Val loss:\n",
      "[4.16959285]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16962437]\n",
      "Epoch [80/100], Train Cost: 4.1652\n",
      "Val loss:\n",
      "[4.16956133]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16959285]\n",
      "Epoch [81/100], Train Cost: 4.1651\n",
      "Val loss:\n",
      "[4.16952981]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16956133]\n",
      "Epoch [82/100], Train Cost: 4.1651\n",
      "Val loss:\n",
      "[4.1694983]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16952981]\n",
      "Epoch [83/100], Train Cost: 4.1651\n",
      "Val loss:\n",
      "[4.16946678]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1694983]\n",
      "Epoch [84/100], Train Cost: 4.1650\n",
      "Val loss:\n",
      "[4.16943526]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16946678]\n",
      "Epoch [85/100], Train Cost: 4.1650\n",
      "Val loss:\n",
      "[4.16940374]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16943526]\n",
      "Epoch [86/100], Train Cost: 4.1650\n",
      "Val loss:\n",
      "[4.16937222]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16940374]\n",
      "Epoch [87/100], Train Cost: 4.1650\n",
      "Val loss:\n",
      "[4.1693407]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16937222]\n",
      "Epoch [88/100], Train Cost: 4.1649\n",
      "Val loss:\n",
      "[4.16930919]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1693407]\n",
      "Epoch [89/100], Train Cost: 4.1649\n",
      "Val loss:\n",
      "[4.16927767]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16930919]\n",
      "Epoch [90/100], Train Cost: 4.1649\n",
      "Val loss:\n",
      "[4.16924615]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16927767]\n",
      "Epoch [91/100], Train Cost: 4.1648\n",
      "Val loss:\n",
      "[4.16921463]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16924615]\n",
      "Epoch [92/100], Train Cost: 4.1648\n",
      "Val loss:\n",
      "[4.16918311]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16921463]\n",
      "Epoch [93/100], Train Cost: 4.1648\n",
      "Val loss:\n",
      "[4.16915159]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16918311]\n",
      "Epoch [94/100], Train Cost: 4.1647\n",
      "Val loss:\n",
      "[4.16912008]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16915159]\n",
      "Epoch [95/100], Train Cost: 4.1647\n",
      "Val loss:\n",
      "[4.16908856]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16912008]\n",
      "Epoch [96/100], Train Cost: 4.1647\n",
      "Val loss:\n",
      "[4.16905704]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16908856]\n",
      "Epoch [97/100], Train Cost: 4.1646\n",
      "Val loss:\n",
      "[4.16902552]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16905704]\n",
      "Epoch [98/100], Train Cost: 4.1646\n",
      "Val loss:\n",
      "[4.168994]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16902552]\n",
      "Epoch [99/100], Train Cost: 4.1646\n",
      "Val loss:\n",
      "[4.16896249]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.168994]\n",
      "Epoch [100/100], Train Cost: 4.1645\n",
      "Val loss:\n",
      "[4.16893097]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16896249]\n",
      "\n",
      "Hyperparameters: Learning Rate=2e-09, Neurons=5\n",
      "Final Train Loss: 4.1645\n",
      "Final Validation Loss: 4.1689\n",
      "Test Accuracy: 6.76%\n",
      "\n",
      "Epoch [1/100], Train Cost: 3.2762\n",
      "Val loss:\n",
      "[3.32890555]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: 3.2759\n",
      "Val loss:\n",
      "[3.32864635]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32890555]\n",
      "Epoch [3/100], Train Cost: 3.2757\n",
      "Val loss:\n",
      "[3.32838716]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32864635]\n",
      "Epoch [4/100], Train Cost: 3.2754\n",
      "Val loss:\n",
      "[3.32812797]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32838716]\n",
      "Epoch [5/100], Train Cost: 3.2751\n",
      "Val loss:\n",
      "[3.32786877]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32812797]\n",
      "Epoch [6/100], Train Cost: 3.2749\n",
      "Val loss:\n",
      "[3.32760954]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32786877]\n",
      "Epoch [7/100], Train Cost: 3.2746\n",
      "Val loss:\n",
      "[3.32735031]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32760954]\n",
      "Epoch [8/100], Train Cost: 3.2743\n",
      "Val loss:\n",
      "[3.32709105]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32735031]\n",
      "Epoch [9/100], Train Cost: 3.2741\n",
      "Val loss:\n",
      "[3.3268317]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32709105]\n",
      "Epoch [10/100], Train Cost: 3.2738\n",
      "Val loss:\n",
      "[3.32657231]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3268317]\n",
      "Epoch [11/100], Train Cost: 3.2735\n",
      "Val loss:\n",
      "[3.3263129]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32657231]\n",
      "Epoch [12/100], Train Cost: 3.2733\n",
      "Val loss:\n",
      "[3.32605348]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3263129]\n",
      "Epoch [13/100], Train Cost: 3.2730\n",
      "Val loss:\n",
      "[3.32579404]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32605348]\n",
      "Epoch [14/100], Train Cost: 3.2727\n",
      "Val loss:\n",
      "[3.32553458]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32579404]\n",
      "Epoch [15/100], Train Cost: 3.2725\n",
      "Val loss:\n",
      "[3.32527512]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32553458]\n",
      "Epoch [16/100], Train Cost: 3.2722\n",
      "Val loss:\n",
      "[3.32501565]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32527512]\n",
      "Epoch [17/100], Train Cost: 3.2720\n",
      "Val loss:\n",
      "[3.32475619]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32501565]\n",
      "Epoch [18/100], Train Cost: 3.2717\n",
      "Val loss:\n",
      "[3.32449673]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32475619]\n",
      "Epoch [19/100], Train Cost: 3.2714\n",
      "Val loss:\n",
      "[3.32423727]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32449673]\n",
      "Epoch [20/100], Train Cost: 3.2712\n",
      "Val loss:\n",
      "[3.32397781]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32423727]\n",
      "Epoch [21/100], Train Cost: 3.2709\n",
      "Val loss:\n",
      "[3.32371835]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32397781]\n",
      "Epoch [22/100], Train Cost: 3.2706\n",
      "Val loss:\n",
      "[3.3234589]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32371835]\n",
      "Epoch [23/100], Train Cost: 3.2704\n",
      "Val loss:\n",
      "[3.32319943]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3234589]\n",
      "Epoch [24/100], Train Cost: 3.2701\n",
      "Val loss:\n",
      "[3.32293996]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32319943]\n",
      "Epoch [25/100], Train Cost: 3.2698\n",
      "Val loss:\n",
      "[3.32268049]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32293996]\n",
      "Epoch [26/100], Train Cost: 3.2696\n",
      "Val loss:\n",
      "[3.32242102]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32268049]\n",
      "Epoch [27/100], Train Cost: 3.2693\n",
      "Val loss:\n",
      "[3.32216155]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32242102]\n",
      "Epoch [28/100], Train Cost: 3.2690\n",
      "Val loss:\n",
      "[3.32190208]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32216155]\n",
      "Epoch [29/100], Train Cost: 3.2688\n",
      "Val loss:\n",
      "[3.32164261]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32190208]\n",
      "Epoch [30/100], Train Cost: 3.2685\n",
      "Val loss:\n",
      "[3.32138314]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32164261]\n",
      "Epoch [31/100], Train Cost: 3.2682\n",
      "Val loss:\n",
      "[3.32112367]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32138314]\n",
      "Epoch [32/100], Train Cost: 3.2680\n",
      "Val loss:\n",
      "[3.3208642]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32112367]\n",
      "Epoch [33/100], Train Cost: 3.2677\n",
      "Val loss:\n",
      "[3.32060473]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3208642]\n",
      "Epoch [34/100], Train Cost: 3.2674\n",
      "Val loss:\n",
      "[3.32034526]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32060473]\n",
      "Epoch [35/100], Train Cost: 3.2672\n",
      "Val loss:\n",
      "[3.32008578]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32034526]\n",
      "Epoch [36/100], Train Cost: 3.2669\n",
      "Val loss:\n",
      "[3.31982628]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.32008578]\n",
      "Epoch [37/100], Train Cost: 3.2667\n",
      "Val loss:\n",
      "[3.31956675]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31982628]\n",
      "Epoch [38/100], Train Cost: 3.2664\n",
      "Val loss:\n",
      "[3.31930722]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31956675]\n",
      "Epoch [39/100], Train Cost: 3.2661\n",
      "Val loss:\n",
      "[3.31904769]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31930722]\n",
      "Epoch [40/100], Train Cost: 3.2659\n",
      "Val loss:\n",
      "[3.31878816]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31904769]\n",
      "Epoch [41/100], Train Cost: 3.2656\n",
      "Val loss:\n",
      "[3.31852864]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31878816]\n",
      "Epoch [42/100], Train Cost: 3.2653\n",
      "Val loss:\n",
      "[3.31826912]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31852864]\n",
      "Epoch [43/100], Train Cost: 3.2651\n",
      "Val loss:\n",
      "[3.31800959]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31826912]\n",
      "Epoch [44/100], Train Cost: 3.2648\n",
      "Val loss:\n",
      "[3.31775007]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31800959]\n",
      "Epoch [45/100], Train Cost: 3.2645\n",
      "Val loss:\n",
      "[3.31749054]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31775007]\n",
      "Epoch [46/100], Train Cost: 3.2643\n",
      "Val loss:\n",
      "[3.31723102]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31749054]\n",
      "Epoch [47/100], Train Cost: 3.2640\n",
      "Val loss:\n",
      "[3.31697147]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31723102]\n",
      "Epoch [48/100], Train Cost: 3.2637\n",
      "Val loss:\n",
      "[3.31671191]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31697147]\n",
      "Epoch [49/100], Train Cost: 3.2635\n",
      "Val loss:\n",
      "[3.31645236]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31671191]\n",
      "Epoch [50/100], Train Cost: 3.2632\n",
      "Val loss:\n",
      "[3.31619281]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31645236]\n",
      "Epoch [51/100], Train Cost: 3.2629\n",
      "Val loss:\n",
      "[3.31593326]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31619281]\n",
      "Epoch [52/100], Train Cost: 3.2627\n",
      "Val loss:\n",
      "[3.31567371]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31593326]\n",
      "Epoch [53/100], Train Cost: 3.2624\n",
      "Val loss:\n",
      "[3.31541414]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31567371]\n",
      "Epoch [54/100], Train Cost: 3.2622\n",
      "Val loss:\n",
      "[3.31515456]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31541414]\n",
      "Epoch [55/100], Train Cost: 3.2619\n",
      "Val loss:\n",
      "[3.314895]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31515456]\n",
      "Epoch [56/100], Train Cost: 3.2616\n",
      "Val loss:\n",
      "[3.31463543]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.314895]\n",
      "Epoch [57/100], Train Cost: 3.2614\n",
      "Val loss:\n",
      "[3.31437587]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31463543]\n",
      "Epoch [58/100], Train Cost: 3.2611\n",
      "Val loss:\n",
      "[3.3141163]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31437587]\n",
      "Epoch [59/100], Train Cost: 3.2608\n",
      "Val loss:\n",
      "[3.31385674]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3141163]\n",
      "Epoch [60/100], Train Cost: 3.2606\n",
      "Val loss:\n",
      "[3.31359717]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31385674]\n",
      "Epoch [61/100], Train Cost: 3.2603\n",
      "Val loss:\n",
      "[3.31333761]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31359717]\n",
      "Epoch [62/100], Train Cost: 3.2600\n",
      "Val loss:\n",
      "[3.31307805]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31333761]\n",
      "Epoch [63/100], Train Cost: 3.2598\n",
      "Val loss:\n",
      "[3.31281849]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31307805]\n",
      "Epoch [64/100], Train Cost: 3.2595\n",
      "Val loss:\n",
      "[3.31255893]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31281849]\n",
      "Epoch [65/100], Train Cost: 3.2592\n",
      "Val loss:\n",
      "[3.31229938]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31255893]\n",
      "Epoch [66/100], Train Cost: 3.2590\n",
      "Val loss:\n",
      "[3.31203982]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31229938]\n",
      "Epoch [67/100], Train Cost: 3.2587\n",
      "Val loss:\n",
      "[3.31178027]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31203982]\n",
      "Epoch [68/100], Train Cost: 3.2584\n",
      "Val loss:\n",
      "[3.31152072]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31178027]\n",
      "Epoch [69/100], Train Cost: 3.2582\n",
      "Val loss:\n",
      "[3.31126116]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31152072]\n",
      "Epoch [70/100], Train Cost: 3.2579\n",
      "Val loss:\n",
      "[3.31100161]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31126116]\n",
      "Epoch [71/100], Train Cost: 3.2576\n",
      "Val loss:\n",
      "[3.31074206]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31100161]\n",
      "Epoch [72/100], Train Cost: 3.2574\n",
      "Val loss:\n",
      "[3.31048251]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31074206]\n",
      "Epoch [73/100], Train Cost: 3.2571\n",
      "Val loss:\n",
      "[3.31022296]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31048251]\n",
      "Epoch [74/100], Train Cost: 3.2569\n",
      "Val loss:\n",
      "[3.30996337]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.31022296]\n",
      "Epoch [75/100], Train Cost: 3.2566\n",
      "Val loss:\n",
      "[3.30970379]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30996337]\n",
      "Epoch [76/100], Train Cost: 3.2563\n",
      "Val loss:\n",
      "[3.3094442]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30970379]\n",
      "Epoch [77/100], Train Cost: 3.2561\n",
      "Val loss:\n",
      "[3.30918463]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3094442]\n",
      "Epoch [78/100], Train Cost: 3.2558\n",
      "Val loss:\n",
      "[3.30892505]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30918463]\n",
      "Epoch [79/100], Train Cost: 3.2555\n",
      "Val loss:\n",
      "[3.30866544]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30892505]\n",
      "Epoch [80/100], Train Cost: 3.2553\n",
      "Val loss:\n",
      "[3.30840584]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30866544]\n",
      "Epoch [81/100], Train Cost: 3.2550\n",
      "Val loss:\n",
      "[3.30814623]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30840584]\n",
      "Epoch [82/100], Train Cost: 3.2547\n",
      "Val loss:\n",
      "[3.30788664]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30814623]\n",
      "Epoch [83/100], Train Cost: 3.2545\n",
      "Val loss:\n",
      "[3.30762704]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30788664]\n",
      "Epoch [84/100], Train Cost: 3.2542\n",
      "Val loss:\n",
      "[3.30736744]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30762704]\n",
      "Epoch [85/100], Train Cost: 3.2539\n",
      "Val loss:\n",
      "[3.30710783]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30736744]\n",
      "Epoch [86/100], Train Cost: 3.2537\n",
      "Val loss:\n",
      "[3.30684822]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30710783]\n",
      "Epoch [87/100], Train Cost: 3.2534\n",
      "Val loss:\n",
      "[3.3065886]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30684822]\n",
      "Epoch [88/100], Train Cost: 3.2531\n",
      "Val loss:\n",
      "[3.306329]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3065886]\n",
      "Epoch [89/100], Train Cost: 3.2529\n",
      "Val loss:\n",
      "[3.30606938]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.306329]\n",
      "Epoch [90/100], Train Cost: 3.2526\n",
      "Val loss:\n",
      "[3.30580977]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30606938]\n",
      "Epoch [91/100], Train Cost: 3.2524\n",
      "Val loss:\n",
      "[3.30555015]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30580977]\n",
      "Epoch [92/100], Train Cost: 3.2521\n",
      "Val loss:\n",
      "[3.30529054]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30555015]\n",
      "Epoch [93/100], Train Cost: 3.2518\n",
      "Val loss:\n",
      "[3.30503093]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30529054]\n",
      "Epoch [94/100], Train Cost: 3.2516\n",
      "Val loss:\n",
      "[3.30477132]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30503093]\n",
      "Epoch [95/100], Train Cost: 3.2513\n",
      "Val loss:\n",
      "[3.3045117]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30477132]\n",
      "Epoch [96/100], Train Cost: 3.2510\n",
      "Val loss:\n",
      "[3.30425208]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.3045117]\n",
      "Epoch [97/100], Train Cost: 3.2508\n",
      "Val loss:\n",
      "[3.30399246]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30425208]\n",
      "Epoch [98/100], Train Cost: 3.2505\n",
      "Val loss:\n",
      "[3.30373284]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30399246]\n",
      "Epoch [99/100], Train Cost: 3.2502\n",
      "Val loss:\n",
      "[3.30347322]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30373284]\n",
      "Epoch [100/100], Train Cost: 3.2500\n",
      "Val loss:\n",
      "[3.30321359]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[3.30347322]\n",
      "\n",
      "Hyperparameters: Learning Rate=7e-09, Neurons=15\n",
      "Final Train Loss: 3.2500\n",
      "Final Validation Loss: 3.3032\n",
      "Test Accuracy: 6.67%\n",
      "\n",
      "Epoch [1/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.98621238]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: -3.0367\n",
      "Val loss:\n",
      "[-2.9862873]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98621238]\n",
      "Epoch [3/100], Train Cost: -3.0368\n",
      "Val loss:\n",
      "[-2.98636222]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9862873]\n",
      "Epoch [4/100], Train Cost: -3.0369\n",
      "Val loss:\n",
      "[-2.98643713]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98636222]\n",
      "Epoch [5/100], Train Cost: -3.0370\n",
      "Val loss:\n",
      "[-2.98651205]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98643713]\n",
      "Epoch [6/100], Train Cost: -3.0370\n",
      "Val loss:\n",
      "[-2.98658697]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98651205]\n",
      "Epoch [7/100], Train Cost: -3.0371\n",
      "Val loss:\n",
      "[-2.98666188]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98658697]\n",
      "Epoch [8/100], Train Cost: -3.0372\n",
      "Val loss:\n",
      "[-2.9867368]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98666188]\n",
      "Epoch [9/100], Train Cost: -3.0373\n",
      "Val loss:\n",
      "[-2.98681171]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9867368]\n",
      "Epoch [10/100], Train Cost: -3.0373\n",
      "Val loss:\n",
      "[-2.98688663]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98681171]\n",
      "Epoch [11/100], Train Cost: -3.0374\n",
      "Val loss:\n",
      "[-2.98696155]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98688663]\n",
      "Epoch [12/100], Train Cost: -3.0375\n",
      "Val loss:\n",
      "[-2.98703647]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98696155]\n",
      "Epoch [13/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.9871114]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98703647]\n",
      "Epoch [14/100], Train Cost: -3.0376\n",
      "Val loss:\n",
      "[-2.98718633]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9871114]\n",
      "Epoch [15/100], Train Cost: -3.0377\n",
      "Val loss:\n",
      "[-2.98726125]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98718633]\n",
      "Epoch [16/100], Train Cost: -3.0378\n",
      "Val loss:\n",
      "[-2.98733618]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98726125]\n",
      "Epoch [17/100], Train Cost: -3.0379\n",
      "Val loss:\n",
      "[-2.98741111]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98733618]\n",
      "Epoch [18/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98748604]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98741111]\n",
      "Epoch [19/100], Train Cost: -3.0380\n",
      "Val loss:\n",
      "[-2.98756097]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98748604]\n",
      "Epoch [20/100], Train Cost: -3.0381\n",
      "Val loss:\n",
      "[-2.9876359]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98756097]\n",
      "Epoch [21/100], Train Cost: -3.0382\n",
      "Val loss:\n",
      "[-2.98771083]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9876359]\n",
      "Epoch [22/100], Train Cost: -3.0383\n",
      "Val loss:\n",
      "[-2.98778576]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98771083]\n",
      "Epoch [23/100], Train Cost: -3.0383\n",
      "Val loss:\n",
      "[-2.98786069]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98778576]\n",
      "Epoch [24/100], Train Cost: -3.0384\n",
      "Val loss:\n",
      "[-2.98793563]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98786069]\n",
      "Epoch [25/100], Train Cost: -3.0385\n",
      "Val loss:\n",
      "[-2.98801057]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98793563]\n",
      "Epoch [26/100], Train Cost: -3.0386\n",
      "Val loss:\n",
      "[-2.98808552]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98801057]\n",
      "Epoch [27/100], Train Cost: -3.0386\n",
      "Val loss:\n",
      "[-2.98816047]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98808552]\n",
      "Epoch [28/100], Train Cost: -3.0387\n",
      "Val loss:\n",
      "[-2.98823541]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98816047]\n",
      "Epoch [29/100], Train Cost: -3.0388\n",
      "Val loss:\n",
      "[-2.98831036]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98823541]\n",
      "Epoch [30/100], Train Cost: -3.0389\n",
      "Val loss:\n",
      "[-2.98838531]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98831036]\n",
      "Epoch [31/100], Train Cost: -3.0389\n",
      "Val loss:\n",
      "[-2.98846026]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98838531]\n",
      "Epoch [32/100], Train Cost: -3.0390\n",
      "Val loss:\n",
      "[-2.98853521]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98846026]\n",
      "Epoch [33/100], Train Cost: -3.0391\n",
      "Val loss:\n",
      "[-2.98861015]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98853521]\n",
      "Epoch [34/100], Train Cost: -3.0392\n",
      "Val loss:\n",
      "[-2.9886851]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98861015]\n",
      "Epoch [35/100], Train Cost: -3.0392\n",
      "Val loss:\n",
      "[-2.98876004]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9886851]\n",
      "Epoch [36/100], Train Cost: -3.0393\n",
      "Val loss:\n",
      "[-2.988835]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98876004]\n",
      "Epoch [37/100], Train Cost: -3.0394\n",
      "Val loss:\n",
      "[-2.98890995]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.988835]\n",
      "Epoch [38/100], Train Cost: -3.0395\n",
      "Val loss:\n",
      "[-2.98898491]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98890995]\n",
      "Epoch [39/100], Train Cost: -3.0395\n",
      "Val loss:\n",
      "[-2.98905986]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98898491]\n",
      "Epoch [40/100], Train Cost: -3.0396\n",
      "Val loss:\n",
      "[-2.98913481]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98905986]\n",
      "Epoch [41/100], Train Cost: -3.0397\n",
      "Val loss:\n",
      "[-2.98920977]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98913481]\n",
      "Epoch [42/100], Train Cost: -3.0398\n",
      "Val loss:\n",
      "[-2.98928472]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98920977]\n",
      "Epoch [43/100], Train Cost: -3.0398\n",
      "Val loss:\n",
      "[-2.98935968]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98928472]\n",
      "Epoch [44/100], Train Cost: -3.0399\n",
      "Val loss:\n",
      "[-2.98943463]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98935968]\n",
      "Epoch [45/100], Train Cost: -3.0400\n",
      "Val loss:\n",
      "[-2.98950957]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98943463]\n",
      "Epoch [46/100], Train Cost: -3.0401\n",
      "Val loss:\n",
      "[-2.98958451]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98950957]\n",
      "Epoch [47/100], Train Cost: -3.0402\n",
      "Val loss:\n",
      "[-2.98965945]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98958451]\n",
      "Epoch [48/100], Train Cost: -3.0402\n",
      "Val loss:\n",
      "[-2.98973439]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98965945]\n",
      "Epoch [49/100], Train Cost: -3.0403\n",
      "Val loss:\n",
      "[-2.98980934]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98973439]\n",
      "Epoch [50/100], Train Cost: -3.0404\n",
      "Val loss:\n",
      "[-2.98988428]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98980934]\n",
      "Epoch [51/100], Train Cost: -3.0405\n",
      "Val loss:\n",
      "[-2.98995922]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98988428]\n",
      "Epoch [52/100], Train Cost: -3.0405\n",
      "Val loss:\n",
      "[-2.99003416]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.98995922]\n",
      "Epoch [53/100], Train Cost: -3.0406\n",
      "Val loss:\n",
      "[-2.9901091]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99003416]\n",
      "Epoch [54/100], Train Cost: -3.0407\n",
      "Val loss:\n",
      "[-2.99018404]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9901091]\n",
      "Epoch [55/100], Train Cost: -3.0408\n",
      "Val loss:\n",
      "[-2.99025899]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99018404]\n",
      "Epoch [56/100], Train Cost: -3.0408\n",
      "Val loss:\n",
      "[-2.99033393]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99025899]\n",
      "Epoch [57/100], Train Cost: -3.0409\n",
      "Val loss:\n",
      "[-2.99040887]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99033393]\n",
      "Epoch [58/100], Train Cost: -3.0410\n",
      "Val loss:\n",
      "[-2.99048381]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99040887]\n",
      "Epoch [59/100], Train Cost: -3.0411\n",
      "Val loss:\n",
      "[-2.99055875]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99048381]\n",
      "Epoch [60/100], Train Cost: -3.0411\n",
      "Val loss:\n",
      "[-2.9906337]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99055875]\n",
      "Epoch [61/100], Train Cost: -3.0412\n",
      "Val loss:\n",
      "[-2.99070864]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9906337]\n",
      "Epoch [62/100], Train Cost: -3.0413\n",
      "Val loss:\n",
      "[-2.99078358]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99070864]\n",
      "Epoch [63/100], Train Cost: -3.0414\n",
      "Val loss:\n",
      "[-2.99085852]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99078358]\n",
      "Epoch [64/100], Train Cost: -3.0414\n",
      "Val loss:\n",
      "[-2.99093347]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99085852]\n",
      "Epoch [65/100], Train Cost: -3.0415\n",
      "Val loss:\n",
      "[-2.99100841]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99093347]\n",
      "Epoch [66/100], Train Cost: -3.0416\n",
      "Val loss:\n",
      "[-2.99108335]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99100841]\n",
      "Epoch [67/100], Train Cost: -3.0417\n",
      "Val loss:\n",
      "[-2.99115829]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99108335]\n",
      "Epoch [68/100], Train Cost: -3.0417\n",
      "Val loss:\n",
      "[-2.99123323]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99115829]\n",
      "Epoch [69/100], Train Cost: -3.0418\n",
      "Val loss:\n",
      "[-2.99130818]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99123323]\n",
      "Epoch [70/100], Train Cost: -3.0419\n",
      "Val loss:\n",
      "[-2.99138312]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99130818]\n",
      "Epoch [71/100], Train Cost: -3.0420\n",
      "Val loss:\n",
      "[-2.99145806]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99138312]\n",
      "Epoch [72/100], Train Cost: -3.0420\n",
      "Val loss:\n",
      "[-2.991533]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99145806]\n",
      "Epoch [73/100], Train Cost: -3.0421\n",
      "Val loss:\n",
      "[-2.99160794]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.991533]\n",
      "Epoch [74/100], Train Cost: -3.0422\n",
      "Val loss:\n",
      "[-2.99168288]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99160794]\n",
      "Epoch [75/100], Train Cost: -3.0423\n",
      "Val loss:\n",
      "[-2.99175783]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99168288]\n",
      "Epoch [76/100], Train Cost: -3.0424\n",
      "Val loss:\n",
      "[-2.99183277]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99175783]\n",
      "Epoch [77/100], Train Cost: -3.0424\n",
      "Val loss:\n",
      "[-2.99190771]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99183277]\n",
      "Epoch [78/100], Train Cost: -3.0425\n",
      "Val loss:\n",
      "[-2.99198265]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99190771]\n",
      "Epoch [79/100], Train Cost: -3.0426\n",
      "Val loss:\n",
      "[-2.99205759]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99198265]\n",
      "Epoch [80/100], Train Cost: -3.0427\n",
      "Val loss:\n",
      "[-2.99213253]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99205759]\n",
      "Epoch [81/100], Train Cost: -3.0427\n",
      "Val loss:\n",
      "[-2.99220748]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99213253]\n",
      "Epoch [82/100], Train Cost: -3.0428\n",
      "Val loss:\n",
      "[-2.99228242]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99220748]\n",
      "Epoch [83/100], Train Cost: -3.0429\n",
      "Val loss:\n",
      "[-2.99235736]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99228242]\n",
      "Epoch [84/100], Train Cost: -3.0430\n",
      "Val loss:\n",
      "[-2.9924323]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99235736]\n",
      "Epoch [85/100], Train Cost: -3.0430\n",
      "Val loss:\n",
      "[-2.99250724]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9924323]\n",
      "Epoch [86/100], Train Cost: -3.0431\n",
      "Val loss:\n",
      "[-2.99258219]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99250724]\n",
      "Epoch [87/100], Train Cost: -3.0432\n",
      "Val loss:\n",
      "[-2.99265713]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99258219]\n",
      "Epoch [88/100], Train Cost: -3.0433\n",
      "Val loss:\n",
      "[-2.99273207]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99265713]\n",
      "Epoch [89/100], Train Cost: -3.0433\n",
      "Val loss:\n",
      "[-2.99280701]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99273207]\n",
      "Epoch [90/100], Train Cost: -3.0434\n",
      "Val loss:\n",
      "[-2.99288195]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99280701]\n",
      "Epoch [91/100], Train Cost: -3.0435\n",
      "Val loss:\n",
      "[-2.9929569]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99288195]\n",
      "Epoch [92/100], Train Cost: -3.0436\n",
      "Val loss:\n",
      "[-2.99303184]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.9929569]\n",
      "Epoch [93/100], Train Cost: -3.0436\n",
      "Val loss:\n",
      "[-2.99310679]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99303184]\n",
      "Epoch [94/100], Train Cost: -3.0437\n",
      "Val loss:\n",
      "[-2.99318174]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99310679]\n",
      "Epoch [95/100], Train Cost: -3.0438\n",
      "Val loss:\n",
      "[-2.99325669]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99318174]\n",
      "Epoch [96/100], Train Cost: -3.0439\n",
      "Val loss:\n",
      "[-2.99333164]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99325669]\n",
      "Epoch [97/100], Train Cost: -3.0439\n",
      "Val loss:\n",
      "[-2.99340658]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99333164]\n",
      "Epoch [98/100], Train Cost: -3.0440\n",
      "Val loss:\n",
      "[-2.99348153]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99340658]\n",
      "Epoch [99/100], Train Cost: -3.0441\n",
      "Val loss:\n",
      "[-2.99355648]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99348153]\n",
      "Epoch [100/100], Train Cost: -3.0442\n",
      "Val loss:\n",
      "[-2.99363142]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[-2.99355648]\n",
      "\n",
      "Hyperparameters: Learning Rate=7e-09, Neurons=10\n",
      "Final Train Loss: -3.0442\n",
      "Final Validation Loss: -2.9936\n",
      "Test Accuracy: 6.67%\n",
      "\n",
      "Epoch [1/100], Train Cost: 4.1677\n",
      "Val loss:\n",
      "[4.17197365]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "inf\n",
      "Epoch [2/100], Train Cost: 4.1676\n",
      "Val loss:\n",
      "[4.17186325]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17197365]\n",
      "Epoch [3/100], Train Cost: 4.1675\n",
      "Val loss:\n",
      "[4.17175286]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17186325]\n",
      "Epoch [4/100], Train Cost: 4.1673\n",
      "Val loss:\n",
      "[4.17164246]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17175286]\n",
      "Epoch [5/100], Train Cost: 4.1672\n",
      "Val loss:\n",
      "[4.17153206]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17164246]\n",
      "Epoch [6/100], Train Cost: 4.1671\n",
      "Val loss:\n",
      "[4.17142166]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17153206]\n",
      "Epoch [7/100], Train Cost: 4.1670\n",
      "Val loss:\n",
      "[4.17131127]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17142166]\n",
      "Epoch [8/100], Train Cost: 4.1669\n",
      "Val loss:\n",
      "[4.17120087]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17131127]\n",
      "Epoch [9/100], Train Cost: 4.1668\n",
      "Val loss:\n",
      "[4.17109047]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17120087]\n",
      "Epoch [10/100], Train Cost: 4.1667\n",
      "Val loss:\n",
      "[4.17098007]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17109047]\n",
      "Epoch [11/100], Train Cost: 4.1666\n",
      "Val loss:\n",
      "[4.17086968]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17098007]\n",
      "Epoch [12/100], Train Cost: 4.1665\n",
      "Val loss:\n",
      "[4.17075928]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17086968]\n",
      "Epoch [13/100], Train Cost: 4.1663\n",
      "Val loss:\n",
      "[4.17064889]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17075928]\n",
      "Epoch [14/100], Train Cost: 4.1662\n",
      "Val loss:\n",
      "[4.17053853]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17064889]\n",
      "Epoch [15/100], Train Cost: 4.1661\n",
      "Val loss:\n",
      "[4.17042819]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17053853]\n",
      "Epoch [16/100], Train Cost: 4.1660\n",
      "Val loss:\n",
      "[4.17031786]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17042819]\n",
      "Epoch [17/100], Train Cost: 4.1659\n",
      "Val loss:\n",
      "[4.17020753]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17031786]\n",
      "Epoch [18/100], Train Cost: 4.1658\n",
      "Val loss:\n",
      "[4.1700972]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.17020753]\n",
      "Epoch [19/100], Train Cost: 4.1657\n",
      "Val loss:\n",
      "[4.16998687]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1700972]\n",
      "Epoch [20/100], Train Cost: 4.1656\n",
      "Val loss:\n",
      "[4.16987654]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16998687]\n",
      "Epoch [21/100], Train Cost: 4.1655\n",
      "Val loss:\n",
      "[4.16976621]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16987654]\n",
      "Epoch [22/100], Train Cost: 4.1653\n",
      "Val loss:\n",
      "[4.16965588]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16976621]\n",
      "Epoch [23/100], Train Cost: 4.1652\n",
      "Val loss:\n",
      "[4.16954556]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16965588]\n",
      "Epoch [24/100], Train Cost: 4.1651\n",
      "Val loss:\n",
      "[4.16943525]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16954556]\n",
      "Epoch [25/100], Train Cost: 4.1650\n",
      "Val loss:\n",
      "[4.16932493]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16943525]\n",
      "Epoch [26/100], Train Cost: 4.1649\n",
      "Val loss:\n",
      "[4.16921462]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16932493]\n",
      "Epoch [27/100], Train Cost: 4.1648\n",
      "Val loss:\n",
      "[4.16910431]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16921462]\n",
      "Epoch [28/100], Train Cost: 4.1647\n",
      "Val loss:\n",
      "[4.16899399]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16910431]\n",
      "Epoch [29/100], Train Cost: 4.1646\n",
      "Val loss:\n",
      "[4.16888369]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16899399]\n",
      "Epoch [30/100], Train Cost: 4.1645\n",
      "Val loss:\n",
      "[4.16877339]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16888369]\n",
      "Epoch [31/100], Train Cost: 4.1643\n",
      "Val loss:\n",
      "[4.16866309]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16877339]\n",
      "Epoch [32/100], Train Cost: 4.1642\n",
      "Val loss:\n",
      "[4.16855279]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16866309]\n",
      "Epoch [33/100], Train Cost: 4.1641\n",
      "Val loss:\n",
      "[4.16844249]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16855279]\n",
      "Epoch [34/100], Train Cost: 4.1640\n",
      "Val loss:\n",
      "[4.1683322]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16844249]\n",
      "Epoch [35/100], Train Cost: 4.1639\n",
      "Val loss:\n",
      "[4.1682219]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1683322]\n",
      "Epoch [36/100], Train Cost: 4.1638\n",
      "Val loss:\n",
      "[4.1681116]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1682219]\n",
      "Epoch [37/100], Train Cost: 4.1637\n",
      "Val loss:\n",
      "[4.16800133]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1681116]\n",
      "Epoch [38/100], Train Cost: 4.1636\n",
      "Val loss:\n",
      "[4.16789107]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16800133]\n",
      "Epoch [39/100], Train Cost: 4.1635\n",
      "Val loss:\n",
      "[4.16778082]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16789107]\n",
      "Epoch [40/100], Train Cost: 4.1633\n",
      "Val loss:\n",
      "[4.16767057]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16778082]\n",
      "Epoch [41/100], Train Cost: 4.1632\n",
      "Val loss:\n",
      "[4.16756032]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16767057]\n",
      "Epoch [42/100], Train Cost: 4.1631\n",
      "Val loss:\n",
      "[4.16745008]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16756032]\n",
      "Epoch [43/100], Train Cost: 4.1630\n",
      "Val loss:\n",
      "[4.16733983]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16745008]\n",
      "Epoch [44/100], Train Cost: 4.1629\n",
      "Val loss:\n",
      "[4.16722959]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16733983]\n",
      "Epoch [45/100], Train Cost: 4.1628\n",
      "Val loss:\n",
      "[4.16711934]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16722959]\n",
      "Epoch [46/100], Train Cost: 4.1627\n",
      "Val loss:\n",
      "[4.1670091]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16711934]\n",
      "Epoch [47/100], Train Cost: 4.1626\n",
      "Val loss:\n",
      "[4.16689886]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1670091]\n",
      "Epoch [48/100], Train Cost: 4.1625\n",
      "Val loss:\n",
      "[4.16678861]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16689886]\n",
      "Epoch [49/100], Train Cost: 4.1624\n",
      "Val loss:\n",
      "[4.16667838]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16678861]\n",
      "Epoch [50/100], Train Cost: 4.1622\n",
      "Val loss:\n",
      "[4.16656819]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16667838]\n",
      "Epoch [51/100], Train Cost: 4.1621\n",
      "Val loss:\n",
      "[4.16645801]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16656819]\n",
      "Epoch [52/100], Train Cost: 4.1620\n",
      "Val loss:\n",
      "[4.16634783]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16645801]\n",
      "Epoch [53/100], Train Cost: 4.1619\n",
      "Val loss:\n",
      "[4.16623766]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16634783]\n",
      "Epoch [54/100], Train Cost: 4.1618\n",
      "Val loss:\n",
      "[4.16612749]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16623766]\n",
      "Epoch [55/100], Train Cost: 4.1617\n",
      "Val loss:\n",
      "[4.16601731]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16612749]\n",
      "Epoch [56/100], Train Cost: 4.1616\n",
      "Val loss:\n",
      "[4.16590714]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16601731]\n",
      "Epoch [57/100], Train Cost: 4.1615\n",
      "Val loss:\n",
      "[4.16579697]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16590714]\n",
      "Epoch [58/100], Train Cost: 4.1614\n",
      "Val loss:\n",
      "[4.16568682]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16579697]\n",
      "Epoch [59/100], Train Cost: 4.1612\n",
      "Val loss:\n",
      "[4.16557666]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16568682]\n",
      "Epoch [60/100], Train Cost: 4.1611\n",
      "Val loss:\n",
      "[4.1654665]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16557666]\n",
      "Epoch [61/100], Train Cost: 4.1610\n",
      "Val loss:\n",
      "[4.16535635]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1654665]\n",
      "Epoch [62/100], Train Cost: 4.1609\n",
      "Val loss:\n",
      "[4.16524619]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16535635]\n",
      "Epoch [63/100], Train Cost: 4.1608\n",
      "Val loss:\n",
      "[4.16513605]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16524619]\n",
      "Epoch [64/100], Train Cost: 4.1607\n",
      "Val loss:\n",
      "[4.1650259]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16513605]\n",
      "Epoch [65/100], Train Cost: 4.1606\n",
      "Val loss:\n",
      "[4.16491576]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1650259]\n",
      "Epoch [66/100], Train Cost: 4.1605\n",
      "Val loss:\n",
      "[4.16480562]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16491576]\n",
      "Epoch [67/100], Train Cost: 4.1604\n",
      "Val loss:\n",
      "[4.16469548]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16480562]\n",
      "Epoch [68/100], Train Cost: 4.1602\n",
      "Val loss:\n",
      "[4.16458535]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16469548]\n",
      "Epoch [69/100], Train Cost: 4.1601\n",
      "Val loss:\n",
      "[4.16447522]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16458535]\n",
      "Epoch [70/100], Train Cost: 4.1600\n",
      "Val loss:\n",
      "[4.16436509]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16447522]\n",
      "Epoch [71/100], Train Cost: 4.1599\n",
      "Val loss:\n",
      "[4.16425496]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16436509]\n",
      "Epoch [72/100], Train Cost: 4.1598\n",
      "Val loss:\n",
      "[4.16414484]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16425496]\n",
      "Epoch [73/100], Train Cost: 4.1597\n",
      "Val loss:\n",
      "[4.16403471]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16414484]\n",
      "Epoch [74/100], Train Cost: 4.1596\n",
      "Val loss:\n",
      "[4.16392458]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16403471]\n",
      "Epoch [75/100], Train Cost: 4.1595\n",
      "Val loss:\n",
      "[4.16381446]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16392458]\n",
      "Epoch [76/100], Train Cost: 4.1594\n",
      "Val loss:\n",
      "[4.16370433]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16381446]\n",
      "Epoch [77/100], Train Cost: 4.1593\n",
      "Val loss:\n",
      "[4.16359421]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16370433]\n",
      "Epoch [78/100], Train Cost: 4.1591\n",
      "Val loss:\n",
      "[4.16348408]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16359421]\n",
      "Epoch [79/100], Train Cost: 4.1590\n",
      "Val loss:\n",
      "[4.16337396]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16348408]\n",
      "Epoch [80/100], Train Cost: 4.1589\n",
      "Val loss:\n",
      "[4.16326383]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16337396]\n",
      "Epoch [81/100], Train Cost: 4.1588\n",
      "Val loss:\n",
      "[4.16315371]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16326383]\n",
      "Epoch [82/100], Train Cost: 4.1587\n",
      "Val loss:\n",
      "[4.16304358]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16315371]\n",
      "Epoch [83/100], Train Cost: 4.1586\n",
      "Val loss:\n",
      "[4.16293346]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16304358]\n",
      "Epoch [84/100], Train Cost: 4.1585\n",
      "Val loss:\n",
      "[4.16282334]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16293346]\n",
      "Epoch [85/100], Train Cost: 4.1584\n",
      "Val loss:\n",
      "[4.16271322]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16282334]\n",
      "Epoch [86/100], Train Cost: 4.1583\n",
      "Val loss:\n",
      "[4.16260309]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16271322]\n",
      "Epoch [87/100], Train Cost: 4.1581\n",
      "Val loss:\n",
      "[4.16249297]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16260309]\n",
      "Epoch [88/100], Train Cost: 4.1580\n",
      "Val loss:\n",
      "[4.16238285]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16249297]\n",
      "Epoch [89/100], Train Cost: 4.1579\n",
      "Val loss:\n",
      "[4.16227273]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16238285]\n",
      "Epoch [90/100], Train Cost: 4.1578\n",
      "Val loss:\n",
      "[4.1621626]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16227273]\n",
      "Epoch [91/100], Train Cost: 4.1577\n",
      "Val loss:\n",
      "[4.16205248]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.1621626]\n",
      "Epoch [92/100], Train Cost: 4.1576\n",
      "Val loss:\n",
      "[4.16194236]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16205248]\n",
      "Epoch [93/100], Train Cost: 4.1575\n",
      "Val loss:\n",
      "[4.16183224]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16194236]\n",
      "Epoch [94/100], Train Cost: 4.1574\n",
      "Val loss:\n",
      "[4.16172212]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16183224]\n",
      "Epoch [95/100], Train Cost: 4.1573\n",
      "Val loss:\n",
      "[4.161612]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16172212]\n",
      "Epoch [96/100], Train Cost: 4.1571\n",
      "Val loss:\n",
      "[4.16150187]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.161612]\n",
      "Epoch [97/100], Train Cost: 4.1570\n",
      "Val loss:\n",
      "[4.16139175]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16150187]\n",
      "Epoch [98/100], Train Cost: 4.1569\n",
      "Val loss:\n",
      "[4.16128163]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16139175]\n",
      "Epoch [99/100], Train Cost: 4.1568\n",
      "Val loss:\n",
      "[4.16117151]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16128163]\n",
      "Epoch [100/100], Train Cost: 4.1567\n",
      "Val loss:\n",
      "[4.1610614]\n",
      "\n",
      "\n",
      "Best val loss:\n",
      "[4.16117151]\n",
      "\n",
      "Hyperparameters: Learning Rate=7e-09, Neurons=5\n",
      "Final Train Loss: 4.1567\n",
      "Final Validation Loss: 4.1611\n",
      "Test Accuracy: 6.76%\n",
      "\n",
      "Test Accuracy: 6.755555555555556\n"
     ]
    }
   ],
   "source": [
    "# Check for early stopping based on validation losses\n",
    "def early_stopping(val_losses, patience):\n",
    "    if len(val_losses) < patience:  # If there are fewer losses than the defined patience, continue training\n",
    "        return False\n",
    "\n",
    "    # Check if validation losses are increasing for the specified patience\n",
    "    for i in range(1, patience + 1):\n",
    "        if val_losses[-i] < val_losses[-(i + 1)]:\n",
    "            return False\n",
    "\n",
    "    return True  # If validation losses are consistently increasing, stop training\n",
    "\n",
    "# Training the model with early stopping\n",
    "def train_with_early_stopping(X_train, Y_train, X_val, Y_val, layers_dims, learning_rate, num_epochs, patience):\n",
    "    parameters = initialize_parameters_deep(layers_dims, 'he')  # Initialize network parameters\n",
    "    cost_list = []  # List to store training costs\n",
    "    val_losses = []  # List to store validation losses\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "    current_patience = 0  # Initialize current patience count\n",
    "\n",
    "    for epoch in range(num_epochs):  # Iterate through the specified number of epochs\n",
    "        # Forward propagation and backpropagation for training set\n",
    "        AL = linear_activation_forward(X_train, parameters)\n",
    "        cost = compute_cost(Y_train, Y_train.shape[1], AL)\n",
    "        grads = linear_activation_backward(X_train, Y_train, AL, ch)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        cost_list.append(cost)  # Append the training cost\n",
    "\n",
    "        # Validation loss calculation\n",
    "        AL_val = linear_activation_forward(X_val, parameters)\n",
    "        val_loss = compute_cost(Y_val, Y_val.shape[1], AL_val)\n",
    "        val_losses.append(val_loss)  # Append the validation loss\n",
    "\n",
    "        # Display current epoch's metrics\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Cost: {np.sum(cost):.4f}\")\n",
    "        print(\"Val loss:\")\n",
    "        print(val_loss)\n",
    "        print(\"\\n\")\n",
    "        print(\"Best val loss:\")\n",
    "        print(best_val_loss)\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            current_patience = 0\n",
    "        else:\n",
    "            current_patience += 1\n",
    "\n",
    "        if current_patience >= patience:  # If validation loss hasn't improved for 'patience' epochs, stop training\n",
    "            print(\"Early stopping\")\n",
    "            break  # Break the loop if early stopping condition met\n",
    "\n",
    "    return parameters, cost_list, val_losses\n",
    "\n",
    "# Model training loop with different hyperparameters\n",
    "n_y = 1\n",
    "n_x = 3072  # Number of input features\n",
    "n_h = 7  # Initial number of neurons in hidden layer\n",
    "learning_rate = 0.000007  # Initial learning rate\n",
    "num_epochs = 100  # Number of epochs for training\n",
    "patience = 2  # Patience for early stopping (adjustable)\n",
    "learning_rates = [0.000000008, 0.000000002, 0.000000007]  # Different learning rates to try\n",
    "neurons_list = [5, 10, 15, 20]  # Different neuron configurations to try\n",
    "\n",
    "test_accuracies = []  # List to store test accuracies\n",
    "\n",
    "# Loop through different hyperparameters\n",
    "for learning_rate in learning_rates:\n",
    "    for n_h in neurons_list:\n",
    "        layers_dims = (n_x, n_h, n_y)  # Set new layer dimensions based on current configuration\n",
    "\n",
    "        # Train the model with early stopping\n",
    "        parameters, train_costs, val_losses = train_with_early_stopping(train_x, train_y, val_x, val_y, layers_dims,\n",
    "                                                                        learning_rate, num_epochs, patience)\n",
    "        test_predictions, test_accuracy = predict(test_x, test_y, parameters, 0.2)\n",
    "\n",
    "        # Print performance metrics\n",
    "        print(f\"\\nHyperparameters: Learning Rate={learning_rate}, Neurons={n_h}\")\n",
    "        print(f\"Final Train Loss: {np.sum(train_costs[-1]):.4f}\")\n",
    "        print(f\"Final Validation Loss: {np.sum(val_losses[-1]):.4f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.2f}%\\n\")\n",
    "\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Test accuracy using the trained model\n",
    "test_predictions, test_accuracy = predict(test_x, test_y, parameters, 0.2)\n",
    "print(\"Test Accuracy: \" + str(test_accuracy))  # Print the final test accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, following methods are defined:\n",
    "1 - early_stopping: This function checks whether to terminate training early based on the validation losses.\n",
    "2 - train_with_early_stopping: This function trains a neural network model with early stopping. It updates the parameters using gradient descent and monitors the validation loss to determine when to stop training early.\n",
    "\n",
    "Analysis of output:\n",
    "-------------------\n",
    "The output represents a training process over multiple epochs (iterations through the dataset). It shows the best validation loss achieved at each epoch along with the training cost and validation loss at that epoch.\n",
    "\n",
    "The \"Best val loss\" signifies the lowest validation loss achieved throughout the training. The \"Epoch [X/100]\" indicates the current epoch and the corresponding training cost and validation loss at that stage.\n",
    "\n",
    "Additionally, the hyperparameters used for the training are shown: learning rate and neurons (presumably the number of neurons in a neural network layer).\n",
    "\n",
    "The training and validation losses steadily decrease over epochs, indicating the model's improvement in fitting the data. However, there's also a segment where the validation loss is constant (around the 4.17 mark for one set of hyperparameters and around 4.16 for another), indicating potential convergence or stagnation in improvement.\n",
    "\n",
    "The model's final performance on the test set indicates an accuracy of around 6.67% for one set of hyperparameters and 6.76% for another. These accuracies seem relatively low, possibly indicating an issue with model complexity, data quality, or a need for further tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 10500\n",
      "Validation set size: 2250\n",
      "Test set size: 2250\n"
     ]
    }
   ],
   "source": [
    "#FIRST PART OF PART-2 \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from early_stopping import EarlyStopping  \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.data[idx], 'label': self.labels[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "\n",
    "        return sample\n",
    "\n",
    "#Folder paths\n",
    "pre_Dir=\"./\"\n",
    "train_directory = pre_Dir+'train'\n",
    "val_directory = pre_Dir+'val'\n",
    "\n",
    "test_directory = pre_Dir+'test'\n",
    "batch_size = 32\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust size accordingly\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "#Loading and preprocessing the \"training\" data\n",
    "train_dataset = datasets.ImageFolder(root=train_directory, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Loading and preprocessing the \"training\" data\n",
    "val_dataset = datasets.ImageFolder(root=val_directory, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Loading and preprocessing the test data\n",
    "test_dataset = datasets.ImageFolder(root=test_directory, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#Print the sizes of the datasets\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "print(\"Validation set size:\", len(val_dataset))\n",
    "print(\"Test set size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up image loading, resizing, normalization, and reshaping operations to prepare image data for a machine learning model, while providing insights into the shape of the processed data for verification.\n",
    "\n",
    "Following results suggest that the images have been successfully processed and are ready for use in a machine learning model that expects input data in this flattened format. The consistency in shapes among the training, testing, and validation datasets is crucial for training and evaluating a machine learning model effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1050\n",
      "Validation set size: 225\n",
      "Test set size: 225\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms, datasets\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from early_stopping import EarlyStopping  # Assuming there's a custom early stopping module\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'image': self.data[idx], 'label': self.labels[idx]}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Directory paths for datasets\n",
    "train_directory = './train'\n",
    "val_directory = './val'\n",
    "test_directory = './test'\n",
    "batch_size = 64  # Set batch size\n",
    "\n",
    "# Assuming train_dataset is your original dataset\n",
    "original_size = len(train_dataset)\n",
    "desired_percentage = 0.1  # Change this to the desired percentage\n",
    "\n",
    "# Calculate the new size based on the desired percentage\n",
    "new_size = int(original_size * desired_percentage)\n",
    "\n",
    "# Use random_split to create a new dataset with the decreased size\n",
    "\n",
    "# Adjust the train/validation/test split or reduce the size of your original datasets\n",
    "# Load and reduce the size of the training dataset\n",
    "train_dataset = datasets.ImageFolder(root=train_directory, transform=transform)\n",
    "train_dataset, _ = torch.utils.data.random_split(train_dataset, [new_size, original_size - new_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "original_size = len(val_dataset)\n",
    "\n",
    "# Calculate the new size based on the desired percentage\n",
    "new_size = int(original_size * desired_percentage)\n",
    "\n",
    "# Load and reduce the size of the validation dataset\n",
    "val_dataset = datasets.ImageFolder(root=val_directory, transform=transform)\n",
    "val_dataset, _ = torch.utils.data.random_split(val_dataset, [new_size, original_size - new_size])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "original_size = len(test_dataset)\n",
    "\n",
    "# Calculate the new size based on the desired percentage\n",
    "new_size = int(original_size * desired_percentage)\n",
    "\n",
    "# Load and reduce the size of the test dataset\n",
    "test_dataset = datasets.ImageFolder(root=test_directory, transform=transform)\n",
    "test_dataset, _ = torch.utils.data.random_split(test_dataset, [new_size, original_size - new_size])\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Training set size:\", len(train_dataset))\n",
    "print(\"Validation set size:\", len(val_dataset))\n",
    "print(\"Test set size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block sets up image loading, resizing, normalization, and reshaping operations to prepare image data for a machine learning model, while providing insights into the shape of the processed data for verification.\n",
    "\n",
    "Following results suggest that the images have been successfully processed and are ready for use in a machine learning model that expects input data in this flattened format. The consistency in shapes among the training, testing, and validation datasets is crucial for training and evaluating a machine learning model effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Loading the pre-trained VGG-19 model\n",
    "vgg19 = models.vgg19(pretrained=True)\n",
    "num_classes = 15  # Number of output classes for your specific task\n",
    "\n",
    "#Option 1: Fine-tune all layers\n",
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = True  # Set requires_grad to True for all parameters\n",
    "\n",
    "#Option 2: Fine-tune only FC1 and FC2 layers\n",
    "#vgg19.classifier[-1] and vgg19.classifier[-3] are FC2 and FC1 layers respectively\n",
    "for param in vgg19.classifier[-1].parameters():\n",
    "    param.requires_grad = True \n",
    "for param in vgg19.classifier[-3].parameters():\n",
    "    param.requires_grad = True  \n",
    "\n",
    "#Modifying the last layer for your classification task\n",
    "vgg19.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "#Assuming 23 have num_classes for our specific case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block, following implementations are made:\n",
    "\n",
    "1 - Pre-trained VGG-19 Model Loading: The code loads a pre-trained VGG-19 model available in the torchvision.models. This model is pre-trained on the ImageNet dataset, providing a strong starting point for various computer vision tasks.\n",
    "\n",
    "2 - Fine-Tuning Options:\n",
    "Option 1: It sets requires_grad to True for all parameters in the VGG-19 model. This allows all layers to be fine-tuned during training on a new dataset/task.\n",
    "Option 2: Alternatively, it selectively sets requires_grad to True for only the last two fully connected (FC) layers (FC2 and FC1) in the classifier part of the VGG-19 model. This enables fine-tuning these specific layers while keeping the rest of the model frozen.\n",
    "\n",
    "3 - Modification of the Last Layer: The code replaces the last layer of the VGG-19 model's classifier (vgg19.classifier[-1]) with a new nn.Linear layer. The new layer is customized for a specific classification task with a different number of output classes (num_classes) by changing the out_features parameter of the nn.Linear module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg19.parameters(), lr=learning_rate)\n",
    "early_stopping = EarlyStopping(depth=5, ignore=10, method='consistency')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines set up crucial components for training a neural network:\n",
    "\n",
    "The learning rate affects the step size during optimization.\n",
    "\n",
    "The loss function (CrossEntropyLoss) measures the model's performance during training.\n",
    "\n",
    "The optimizer (Adam) updates the model parameters based on the computed gradients.\n",
    "\n",
    "Early stopping can prevent overfitting by monitoring the model's performance on validation data and stopping training when performance stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Epoch [1/5], Batch [1/10], Train Loss: 2.8615\n",
      "Epoch [1/5], Batch [2/10], Train Loss: 3.3705\n",
      "Epoch [1/5], Batch [3/10], Train Loss: 2.7339\n",
      "Epoch [1/5], Batch [4/10], Train Loss: 2.7693\n",
      "Epoch [1/5], Batch [5/10], Train Loss: 2.7210\n",
      "Epoch [1/5], Batch [6/10], Train Loss: 3.1710\n",
      "Epoch [1/5], Batch [7/10], Train Loss: 2.7186\n",
      "Epoch [1/5], Batch [8/10], Train Loss: 2.7201\n",
      "Epoch [1/5], Batch [9/10], Train Loss: 2.7320\n",
      "Epoch [1/5], Batch [10/10], Train Loss: 2.7175\n",
      "Epoch [1/5], Val Loss: 2.7122\n",
      "Validation loss improved!\n",
      "Epoch [2/5]\n",
      "Epoch [2/5], Batch [1/10], Train Loss: 2.7024\n",
      "Epoch [2/5], Batch [2/10], Train Loss: 2.7476\n",
      "Epoch [2/5], Batch [3/10], Train Loss: 2.7003\n",
      "Epoch [2/5], Batch [4/10], Train Loss: 2.7301\n",
      "Epoch [2/5], Batch [5/10], Train Loss: 2.7073\n",
      "Epoch [2/5], Batch [6/10], Train Loss: 2.7164\n",
      "Epoch [2/5], Batch [7/10], Train Loss: 2.7217\n",
      "Epoch [2/5], Batch [8/10], Train Loss: 2.7470\n",
      "Epoch [2/5], Batch [9/10], Train Loss: 2.6852\n",
      "Epoch [2/5], Batch [10/10], Train Loss: 2.7572\n",
      "Epoch [2/5], Val Loss: 2.7179\n",
      "Validation loss did not improve.\n",
      "Epoch [3/5]\n",
      "Epoch [3/5], Batch [1/10], Train Loss: 2.7035\n",
      "Epoch [3/5], Batch [2/10], Train Loss: 2.6792\n",
      "Epoch [3/5], Batch [3/10], Train Loss: 2.7311\n",
      "Epoch [3/5], Batch [4/10], Train Loss: 2.6865\n",
      "Epoch [3/5], Batch [5/10], Train Loss: 2.7158\n",
      "Epoch [3/5], Batch [6/10], Train Loss: 2.7343\n",
      "Epoch [3/5], Batch [7/10], Train Loss: 2.6992\n",
      "Epoch [3/5], Batch [8/10], Train Loss: 2.7652\n",
      "Epoch [3/5], Batch [9/10], Train Loss: 2.7137\n",
      "Epoch [3/5], Batch [10/10], Train Loss: 2.6731\n",
      "Epoch [3/5], Val Loss: 2.7183\n",
      "Validation loss did not improve.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_epochs = 5\n",
    "desired_batches_per_epoch = 10  \n",
    "#We can change this to the desired number of batches per epoch\n",
    "\n",
    "#Track validation loss across epochs\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "current_patience = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#Loop for Training\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    vgg19.train()  # Set the model to train mode\n",
    "\n",
    "    train_loader_iter = iter(train_loader)\n",
    "    \n",
    "    epoch_train_losses = []  \n",
    "    #We reset training losses for each epoch\n",
    "\n",
    "    for batch_idx in range(desired_batches_per_epoch):\n",
    "        try:\n",
    "            inputs, labels = next(train_loader_iter)\n",
    "        except StopIteration:\n",
    "            \n",
    "            train_loader_iter = iter(train_loader)\n",
    "            inputs, labels = next(train_loader_iter)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg19(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Appending training loss for each batch\n",
    "        epoch_train_losses.append(loss.item())\n",
    "\n",
    "        #We are printing training loss for each batch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{desired_batches_per_epoch}], Train Loss: {loss.item():.4f}\")\n",
    "\n",
    "    #Appending the average training loss for the epoch\n",
    "    train_losses.append(sum(epoch_train_losses) / len(epoch_train_losses))\n",
    "\n",
    "    #Validation loss calculation\n",
    "    #Set the model to evaluation mode\n",
    "    vgg19.eval() \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs_val, labels_val in val_loader:\n",
    "            outputs_val = vgg19(inputs_val)\n",
    "            val_loss = criterion(outputs_val, labels_val)\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        \n",
    "        #Printing of validation loss for each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        #We check if the validation loss has improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            current_patience = 0 \n",
    "            print(\"Validation loss improved!\")\n",
    "        else:\n",
    "            current_patience += 1\n",
    "            print(\"Validation loss did not improve.\")\n",
    "\n",
    "        #Checking early stopping condition\n",
    "        if current_patience >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "def softmax(x):\n",
    "    # Ensure numerical stability by subtracting the maximum value\n",
    "    shifted_x = x - np.max(x)\n",
    "    # Calculate exponential values\n",
    "    exp_values = np.exp(shifted_x)\n",
    "    # Compute softmax probabilities\n",
    "    probabilities = exp_values / np.sum(exp_values)\n",
    "    return probabilities  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block: \n",
    "\n",
    "Setting up the necessary variables and lists to track losses.\n",
    "\n",
    "The training loop that iterates through epochs and batches.\n",
    "\n",
    "Training the model on the training data, calculating losses, and printing them.\n",
    "\n",
    "Switching to evaluation mode for the model and computing the validation loss.\n",
    "\n",
    "Checking for improvements in validation loss and implementing early stopping if necessary.\n",
    "\n",
    "Analysis of Outputs:\n",
    "--------------------\n",
    "Our model performed a training loop over 3 epochs, printing both training and validation losses. \n",
    "\n",
    "Epoch 1:\n",
    "\n",
    "Training Losses: Varied losses ranging around 2.7 to 3.3.\n",
    "\n",
    "Validation Loss: 2.7122 (Improved from initial inf value)\n",
    "\n",
    "Epoch 2:\n",
    "\n",
    "Training Losses: Again, fluctuating losses ranging from 2.7 to 2.75.\n",
    "\n",
    "Validation Loss: 2.7179 (Slightly increased from the previous epoch)\n",
    "\n",
    "Epoch 3:\n",
    "\n",
    "Training Losses: Similar to previous epochs, ranging around 2.67 to 2.77.\n",
    "\n",
    "Validation Loss: 2.7183 (Slightly increased from the second epoch)\n",
    "\n",
    "The code executes early stopping after the third epoch due to the set patience of 2 (no improvement observed in validation loss for 2 epochs).\n",
    "\n",
    "The behavior of the validation loss seems to fluctuate slightly, showing an initial improvement and not improving further, leading to the early stopping mechanism to kick in after the third epoch.\n",
    "\n",
    "This indicates that the model is not improving significantly beyond a certain point and might need adjustments in the learning rate, model architecture, or hyperparameters to improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[15  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [16  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [22  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [18  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [10  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [17  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [14  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [12  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [18  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [23  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      1.00      0.12        15\n",
      "           1       1.00      0.00      0.00        16\n",
      "           2       1.00      0.00      0.00        22\n",
      "           3       1.00      0.00      0.00        11\n",
      "           4       1.00      0.00      0.00        18\n",
      "           5       1.00      0.00      0.00        10\n",
      "           6       1.00      0.00      0.00        17\n",
      "           7       1.00      0.00      0.00        11\n",
      "           8       1.00      0.00      0.00        14\n",
      "           9       1.00      0.00      0.00        13\n",
      "          10       1.00      0.00      0.00        11\n",
      "          11       1.00      0.00      0.00        14\n",
      "          12       1.00      0.00      0.00        12\n",
      "          13       1.00      0.00      0.00        18\n",
      "          14       1.00      0.00      0.00        23\n",
      "\n",
      "    accuracy                           0.07       225\n",
      "   macro avg       0.94      0.07      0.01       225\n",
      "weighted avg       0.94      0.07      0.01       225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vgg19.eval()\n",
    "\n",
    "#Initialization of lists to store true labels and predicted labels\n",
    "all_labels, all_preds = [], []\n",
    "\n",
    "#We use the model to predict labels on the test dataset\n",
    "with torch.no_grad():\n",
    "    for inputs_test, labels_test in test_loader:\n",
    "        outputs_test = vgg19(inputs_test)\n",
    "        _, preds = torch.max(outputs_test, 1)\n",
    "        # Extend the lists with the true and predicted labels\n",
    "        all_labels.extend(labels_test.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "#Calculation of the confusion matrix based on the true and predicted labels\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "#We generate and print the classification report based on the calculated metrics\n",
    "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds, zero_division=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block evaluates the performance of the vgg19 model on the test dataset. It iterates through the test dataset, makes predictions using the model, stores the true labels, and collects the predicted labels. Then, it calculates and prints the confusion matrix and the classification report using the classification_report function from sklearn.metrics. The classification report provides metrics like precision, recall, F1-score, and support for each class label. The confusion matrix shows the counts of true positive, false positive, true negative, and false negative predictions for each class.\n",
    "\n",
    "Analysis of outputs:\n",
    "--------------------\n",
    "The provided confusion matrix and classification report indicate that the model's predictions are diverse and are skewed towards classes for all test samples. The precision, recall, and F1-score for most classes are seen. This suggests that the model is essentially predicting classes of all inputs.\n",
    "\n",
    "To improve the model's performance, investigation of possible reasons for this behavior, such as data imbalance, model architecture, or insufficient training can be made. Also, it might be beneficial to check the data processing steps, the model's architecture, and the training parameters to ensure the model learns a diverse representation of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting error curves (train vs validation) over epochs\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to plot, despite an increase in iterations, the loss remains within the same range as expected for the specified learning rate, showing consistent fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Training and Validation Loss - Epoch {epoch + 1}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 3 epochs, the algorithm understands that \"Loss\" is not decreasing dramatically anymore and \"Early Stopping\" algorithm gets activated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]\n",
      "Epoch [1/5], Batch [1/10], Train Loss: 2.7375\n",
      "Epoch [1/5], Batch [2/10], Train Loss: 16.8256\n",
      "Epoch [1/5], Batch [3/10], Train Loss: 18.3635\n",
      "Epoch [1/5], Batch [4/10], Train Loss: 25.8804\n",
      "Epoch [1/5], Batch [5/10], Train Loss: 25.5082\n",
      "Epoch [1/5], Batch [6/10], Train Loss: 33.0311\n",
      "Epoch [1/5], Batch [7/10], Train Loss: 21.4989\n",
      "Epoch [1/5], Batch [8/10], Train Loss: 37.5049\n",
      "Epoch [1/5], Batch [9/10], Train Loss: 24.6457\n",
      "Epoch [1/5], Batch [10/10], Train Loss: 20.7447\n",
      "Epoch [1/5], Val Loss (FC Layers): 15.3032\n",
      "Validation loss (FC Layers) did not improve.\n",
      "Epoch [2/5]\n",
      "Epoch [2/5], Batch [1/10], Train Loss: 15.4223\n",
      "Epoch [2/5], Batch [2/10], Train Loss: 13.1819\n",
      "Epoch [2/5], Batch [3/10], Train Loss: 16.6128\n",
      "Epoch [2/5], Batch [4/10], Train Loss: 8.6194\n",
      "Epoch [2/5], Batch [5/10], Train Loss: 7.9365\n",
      "Epoch [2/5], Batch [6/10], Train Loss: 6.1322\n",
      "Epoch [2/5], Batch [7/10], Train Loss: 11.9801\n",
      "Epoch [2/5], Batch [8/10], Train Loss: 13.1131\n",
      "Epoch [2/5], Batch [9/10], Train Loss: 10.0828\n",
      "Epoch [2/5], Batch [10/10], Train Loss: 6.9165\n",
      "Epoch [2/5], Val Loss (FC Layers): 8.6917\n",
      "Validation loss (FC Layers) did not improve.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#SECOND PART OF PART-2 \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading pre-trained VGG-19 model\n",
    "vgg19_fc_layers = models.vgg19(pretrained=True)\n",
    "patience = 2\n",
    "current_patience = 0\n",
    "\n",
    "#We freeze all layers except the last two fully connected layers\n",
    "for param in vgg19_fc_layers.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#Modification of the last two fully connected layers for the new classification task\n",
    "num_classes = 15\n",
    "vgg19_fc_layers.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "vgg19_fc_layers.classifier[-3] = nn.Linear(in_features=4096, out_features=4096)\n",
    "\n",
    "#Allowing gradients for the last two fully connected layers\n",
    "for param in vgg19_fc_layers.classifier[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in vgg19_fc_layers.classifier[-3].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "#Definition of optimizer and criterion\n",
    "learning_rate_fc_layers = 0.001\n",
    "optimizer_fc_layers = optim.Adam(vgg19_fc_layers.parameters(), lr=learning_rate_fc_layers)\n",
    "criterion_fc_layers = nn.CrossEntropyLoss()\n",
    "\n",
    "#Training loop for the modified layers only (FC1 and FC2 layers)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "    vgg19_fc_layers.train()\n",
    "    \n",
    "    train_loader_iter_fc_layers = iter(train_loader)\n",
    "    \n",
    "    for batch_idx in range(desired_batches_per_epoch):\n",
    "        try:\n",
    "            inputs_fc_layers, labels_fc_layers = next(train_loader_iter_fc_layers)\n",
    "        except StopIteration:\n",
    "            #If the iterator reaches the end of the data, reset.\n",
    "            train_loader_iter_fc_layers = iter(train_loader)\n",
    "            inputs_fc_layers, labels_fc_layers = next(train_loader_iter_fc_layers)\n",
    "\n",
    "        optimizer_fc_layers.zero_grad()\n",
    "        outputs_fc_layers = vgg19_fc_layers(inputs_fc_layers)\n",
    "        loss_fc_layers = criterion_fc_layers(outputs_fc_layers, labels_fc_layers)\n",
    "        loss_fc_layers.backward()\n",
    "        optimizer_fc_layers.step()\n",
    "\n",
    "        #Print training loss for each batch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{desired_batches_per_epoch}], Train Loss: {loss_fc_layers.item():.4f}\")\n",
    "\n",
    "    #Validation loss for the modified layers\n",
    "    vgg19_fc_layers.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_losses_fc_layers = []\n",
    "\n",
    "        for inputs_val_fc_layers, labels_val_fc_layers in val_loader:\n",
    "            outputs_val_fc_layers = vgg19_fc_layers(inputs_val_fc_layers)\n",
    "            val_loss_fc_layers = criterion_fc_layers(outputs_val_fc_layers, labels_val_fc_layers)\n",
    "            val_losses_fc_layers.append(val_loss_fc_layers.item())\n",
    "\n",
    "        avg_val_loss_fc_layers = sum(val_losses_fc_layers) / len(val_losses_fc_layers)\n",
    "        \n",
    "        #Printing the validation loss for each epoch\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Val Loss (FC Layers): {avg_val_loss_fc_layers:.4f}\")\n",
    "\n",
    "        #Checking process if the validation loss has improved\n",
    "        if avg_val_loss_fc_layers < best_val_loss:\n",
    "            best_val_loss = avg_val_loss_fc_layers\n",
    "            current_patience = 0  \n",
    "            #Reset patience if there's an improvement\n",
    "            print(\"Validation loss (FC Layers) improved!\")\n",
    "        else:\n",
    "            current_patience += 1\n",
    "            print(\"Validation loss (FC Layers) did not improve.\")\n",
    "\n",
    "        #Checking process early stopping condition\n",
    "        if current_patience >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break  # End training loop if early stopping condition is met\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code block, following steps are made:\n",
    "\n",
    "1 - Importing necessary libraries\n",
    "\n",
    "2 - Loading Pre-trained VGG-19 Model\n",
    "\n",
    "3 - Freezing Layers and Enable Gradients\n",
    "\n",
    "4 - Define Optimizer and Loss Criterion\n",
    "\n",
    "5 - Training Loop for Modified Layers\n",
    "\n",
    "6 - Remaining Code\n",
    "\n",
    "This block aims to fine-tune only the last two fully connected layers of the VGG-19 model for a new classification task. It iterates through training and validation steps while monitoring validation loss to improve model generalization and prevent overfitting.\n",
    "\n",
    "Output Analysis:\n",
    "----------------\n",
    "According to these results, training losses are seen, indicating relatively poor performance of the model on the training data. The validation loss show a decreasing trend, leading to early stopping of the training based on the early stopping criteria. There might be improvements needed during the training process to enhance the model's accuracy. To do this ssing a smaller learning rate, trying a different optimization algorithm, or training the network for a longer duration could be potential strategies to improve the overall performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (FC Layers):\n",
      " [[ 3  4  1  0  1  0  1  0  0  5  0  0  0  0  0]\n",
      " [ 1 13  0  0  0  0  2  0  0  0  0  0  0  0  0]\n",
      " [ 0  1 16  0  0  0  3  0  1  0  0  0  0  0  1]\n",
      " [ 0  9  0  0  0  0  0  0  0  1  0  0  0  1  0]\n",
      " [ 9  3  0  0  0  0  1  0  0  5  0  0  0  0  0]\n",
      " [ 1  4  0  0  0  4  0  0  0  1  0  0  0  0  0]\n",
      " [ 1  0  3  0  0  0  9  0  1  0  0  0  2  0  1]\n",
      " [ 1  0  5  0  0  0  0  2  0  2  0  0  0  0  1]\n",
      " [ 2  1  4  0  0  0  1  0  5  1  0  0  0  0  0]\n",
      " [ 3  0  1  0  0  0  0  0  1  7  0  1  0  0  0]\n",
      " [ 0  0  3  0  0  0  3  0  3  0  1  0  1  0  0]\n",
      " [ 1  0 10  0  0  0  0  1  0  2  0  0  0  0  0]\n",
      " [ 0  0  1  0  0  0  1  0  1  0  0  0  9  0  0]\n",
      " [ 1 11  0  0  0  0  0  0  0  1  0  0  0  5  0]\n",
      " [ 0  0  4  0  0  0  0  0  2  0  0  0  0  0 17]]\n",
      "Classification Report (FC Layers):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.20      0.16        15\n",
      "           1       0.28      0.81      0.42        16\n",
      "           2       0.33      0.73      0.46        22\n",
      "           3       1.00      0.00      0.00        11\n",
      "           4       0.00      0.00      1.00        18\n",
      "           5       1.00      0.40      0.57        10\n",
      "           6       0.43      0.53      0.47        17\n",
      "           7       0.67      0.18      0.29        11\n",
      "           8       0.36      0.36      0.36        14\n",
      "           9       0.28      0.54      0.37        13\n",
      "          10       1.00      0.09      0.17        11\n",
      "          11       0.00      0.00      1.00        14\n",
      "          12       0.75      0.75      0.75        12\n",
      "          13       0.83      0.28      0.42        18\n",
      "          14       0.85      0.74      0.79        23\n",
      "\n",
      "    accuracy                           0.40       225\n",
      "   macro avg       0.53      0.37      0.48       225\n",
      "weighted avg       0.50      0.40      0.51       225\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the second model (only FC1 and FC2 layers) on the test set\n",
    "vgg19_fc_layers.eval()\n",
    "all_labels_fc_layers, all_preds_fc_layers = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs_test_fc_layers, labels_test_fc_layers in test_loader:\n",
    "        outputs_test_fc_layers = vgg19_fc_layers(inputs_test_fc_layers)\n",
    "        _, preds_fc_layers = torch.max(outputs_test_fc_layers, 1)\n",
    "        all_labels_fc_layers.extend(labels_test_fc_layers.cpu().numpy())\n",
    "        all_preds_fc_layers.extend(preds_fc_layers.cpu().numpy())\n",
    "\n",
    "# Calculate confusion matrix for the second model\n",
    "conf_matrix_fc_layers = confusion_matrix(all_labels_fc_layers, all_preds_fc_layers)\n",
    "print(\"Confusion Matrix (FC Layers):\\n\", conf_matrix_fc_layers)\n",
    "\n",
    "# Generate and print classification report for the second model\n",
    "classification_report_fc_layers = classification_report(all_labels_fc_layers, all_preds_fc_layers, zero_division=1)\n",
    "print(\"Classification Report (FC Layers):\\n\", classification_report_fc_layers)\n",
    "\n",
    "# Plotting error curves (train vs validation) over epochs for the second model\n",
    "plt.plot(train_losses, label='Train Loss (All Layers)')\n",
    "plt.plot(val_losses_fc_layers, label='Validation Loss (FC Layers)')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss (FC Layers)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Analysis:\n",
    "----------------\n",
    "\n",
    "These results are quite diverse. There seem to be different values in accuracy and other metrics for each class. Particularly, there are relatively low accuracy values indicating high error for some classes. It appears that there are some classes that need more attention. Comparing precision, recall, and f1-score values among the classes provide a more detailed insight into the model's performance. Additionally, the confusion matrices very helpful to understand the complexity of specific classes. For instance, further analysis could be done to understand why the accuracy values are so low for class 3 and class 11.\n",
    "\n",
    "For class 4, the recall and f1-score values are quite close to 1, indicating that the model almost perfectly predicts this class. The performance varies across other classes compared to the average. These results suggest that the model recognizes some classes better than others. The accuracy provides an overall correctness rate, which in this case is 40%. This metric alone may not represent the entire model performance, so it's crucial to evaluate it alongside other metrics. Specifically focusing on precision, recall, and f1-score values can be important for understanding class-based performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BONUS (CNN ALGORITHM THAT WE IMPLEMENTED)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import correlate2d\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def forward(self, inpData):\n",
    "            self.inpData = inpData\n",
    "            # Initialized the input value\n",
    "            output = np.zeros(self.output_shape)\n",
    "            for i in range(self.FltNumber):\n",
    "                output[i] = correlate2d(self.inpData, self.filters[i], mode=\"valid\")\n",
    "            #Applying Relu Activtion function\n",
    "            output = np.maximum(output, 0)\n",
    "            return output \n",
    "    def __init__(self, inpShpe, FltSize, FltNumber):\n",
    "        input_height, input_width = inpShpe\n",
    "        self.FltNumber = FltNumber\n",
    "        self.inpShpe = inpShpe\n",
    "        \n",
    "        # Size of outputs and filters\n",
    "        \n",
    "        self.filter_shape = (FltNumber, FltSize, FltSize) # (3,3)\n",
    "        self.output_shape = (FltNumber, input_height - FltSize + 1, input_width - FltSize + 1)\n",
    "        \n",
    "        self.filters = np.random.randn(*self.filter_shape)\n",
    "        self.biases = np.random.randn(*self.output_shape)\n",
    "    \n",
    "    def backward(self, dL_dout, lr):\n",
    "        # Create a random dL_dout array to accommodate output gradients\n",
    "        dL_dinput = np.zeros_like(self.inpData)\n",
    "        dL_dfilters = np.zeros_like(self.filters)\n",
    "\n",
    "        for i in range(self.FltNumber):\n",
    "                # Calculating the gradient of loss with respect to kernels\n",
    "                dL_dfilters[i] = correlate2d(self.inpData, dL_dout[i],mode=\"valid\")\n",
    "\n",
    "                dL_dinput = np.zeros_like(self.inpData, dtype=float)  # Use float instead of uint8\n",
    "\n",
    "                for i in range(self.FltNumber):\n",
    "                    dL_dinput += correlate2d(dL_dout[i], self.filters[i], mode=\"full\")\n",
    "\n",
    "\n",
    "        # Updating the parameters with learning rate\n",
    "        self.filters -= lr * dL_dfilters\n",
    "        self.biases -= lr * dL_dout\n",
    "\n",
    "        # returning the gradient of inputs\n",
    "        return dL_dinput\n",
    "class MaxPool:\n",
    "    def forward(self, inpData):\n",
    "\n",
    "            self.inpData = inpData\n",
    "            self.num_channels, self.input_height, self.input_width = inpData.shape\n",
    "            self.output_height = self.input_height // self.pool_size\n",
    "            self.output_width = self.input_width // self.pool_size\n",
    "\n",
    "            # Determining the output shape\n",
    "            self.output = np.zeros((self.num_channels, self.output_height, self.output_width))\n",
    "\n",
    "            # Iterating over different channels\n",
    "            for c in range(self.num_channels):\n",
    "                # Looping through the height\n",
    "                for i in range(self.output_height):\n",
    "                    # looping through the width\n",
    "                    for j in range(self.output_width):\n",
    "\n",
    "                        # Starting postition\n",
    "                        S_i = i * self.pool_size\n",
    "                        S_j = j * self.pool_size\n",
    "\n",
    "                        # Ending Position\n",
    "                        E_i = S_i + self.pool_size\n",
    "                        E_i = S_j + self.pool_size\n",
    "\n",
    "                        # Creating a patch from the input data\n",
    "                        patch = inpData[c, S_i:E_i, S_j:E_i]\n",
    "\n",
    "                        #Finding the maximum value from each patch/window\n",
    "                        self.output[c, i, j] = np.max(patch)\n",
    "\n",
    "            return self.output\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "    \n",
    "    def backward(self, dL_dout, lr):\n",
    "        dL_dinput = np.zeros_like(self.inpData)\n",
    "\n",
    "        for c in range(self.num_channels):\n",
    "            for i in range(self.output_height):\n",
    "                for j in range(self.output_width):\n",
    "                    S_i = i * self.pool_size\n",
    "                    S_j = j * self.pool_size\n",
    "\n",
    "                    E_i = S_i + self.pool_size\n",
    "                    E_i = S_j + self.pool_size\n",
    "                    patch = self.inpData[c, S_i:E_i, S_j:E_i]\n",
    "\n",
    "                    mask = patch == np.max(patch)\n",
    "\n",
    "                    dL_dinput[c,S_i:E_i, S_j:E_i] = dL_dout[c, i, j] * mask\n",
    "\n",
    "        return dL_dinput\n",
    "class Fully_Connected:\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size # Size of the inputs coming\n",
    "        self.output_size = output_size # Size of the output producing\n",
    "        self.weights = np.random.randn(output_size, self.input_size)\n",
    "        self.biases = np.random.rand(output_size, 1)\n",
    "    def softmax(self, z):\n",
    "        # Shift the input values to avoid numerical instability\n",
    "        shifted_z = z - np.max(z)\n",
    "        exp_values = np.exp(shifted_z)\n",
    "        sum_exp_values = np.sum(exp_values, axis=0)\n",
    "        log_sum_exp = np.log(sum_exp_values)\n",
    "\n",
    "        # Compute the softmax probabilities\n",
    "        probabilities = exp_values / sum_exp_values\n",
    "\n",
    "        return probabilities\n",
    "    def softmax_derivative(self, s):\n",
    "        return np.diagflat(s) - np.dot(s, s.T)\n",
    "    def forward(self, inpData):\n",
    "        self.inpData = inpData\n",
    "        # Flattening the inputs from the previous layer into a vector\n",
    "        flattened_input = inpData.flatten().reshape(1, -1)\n",
    "        self.z = np.dot(self.weights, flattened_input.T) + self.biases\n",
    "\n",
    "        # Applying Softmax\n",
    "        self.output = self.softmax(self.z)\n",
    "        return self.output\n",
    "    def backward(self, dL_dout, lr):\n",
    "        # Calculate the gradient of the loss with respect to the pre-activation (z)\n",
    "        dL_dy = np.dot(self.softmax_derivative(self.output), dL_dout)\n",
    "        # Calculate the gradient of the loss with respect to the weights (dw)\n",
    "        dL_dw = np.dot(dL_dy, self.inpData.flatten().reshape(1, -1))\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the biases (db)\n",
    "        dL_db = dL_dy\n",
    "\n",
    "        # Calculate the gradient of the loss with respect to the input data (dL_dinput)\n",
    "        dL_dinput = np.dot(self.weights.T, dL_dy)\n",
    "        dL_dinput = dL_dinput.reshape(self.inpData.shape)\n",
    "\n",
    "        # Update the weights and biases based on the learning rate and gradients\n",
    "        self.weights -= lr * dL_dw\n",
    "        self.biases -= lr * dL_db\n",
    "\n",
    "        # Return the gradient of the loss with respect to the input data\n",
    "        return dL_dinput\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we implemented our own CNN algorithm which gives quite good results\n",
    "\n",
    "This class represents a fully connected layer. The forward method flattens the input data, computes the dot product with weights, applies softmax activation, and returns the output. The backward method computes gradients and updates weights and biases.\n",
    "\n",
    "Each class represents a layer in a Convolutional Neural Network (CNN) and defines methods for both forward and backward propagation, essential for training the network.\n",
    "\n",
    "This code creates a basic CNN architecture using custom classes for convolution, max-pooling, and fully connected layers, enabling forward and backward propagation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for computing cross-entropy loss between predictions and targets\n",
    "def cross_entropy_loss(predictions, targets):\n",
    "    num_samples = 10\n",
    "    #We should avoid numerical instability by adding a small epsilon value\n",
    "    epsilon = 1e-7\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "    loss = -np.sum(targets * np.log(predictions)) / num_samples\n",
    "    return loss\n",
    "#Function to train the neural network\n",
    "def train_network(X, y, conv, pool, full, lr=0.01, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        C_Preds = 0\n",
    "        for i in range(len(X)):\n",
    "            #Forward pass\n",
    "            conv_out = conv.forward(X[i])\n",
    "            pool_out = pool.forward(conv_out)\n",
    "            full_out = full.forward(pool_out)\n",
    "            loss = cross_entropy_loss(full_out.flatten(), y[i])\n",
    "            total_loss += loss\n",
    "\n",
    "            #Converting to One-Hot encoding\n",
    "            OHotPred = np.zeros_like(full_out)\n",
    "            OHotPred[np.argmax(full_out)] = 1\n",
    "            OHotPred = OHotPred.flatten()\n",
    "\n",
    "            num_pred = np.argmax(OHotPred)\n",
    "            num_y = np.argmax(y[i])\n",
    "\n",
    "            if num_pred == num_y:\n",
    "                C_Preds += 1\n",
    "            #Backward pass\n",
    "            gradient = cross_entropy_loss_gradient(y[i], full_out.flatten()).reshape((-1, 1))\n",
    "            full_back = full.backward(gradient, lr)\n",
    "            pool_back = pool.backward(full_back, lr)\n",
    "            conv_back = conv.backward(pool_back, lr)\n",
    "\n",
    "        #For printing epoch statistics:\n",
    "        average_loss = total_loss / len(X)\n",
    "        accuracy = C_Preds / len(X) * 100.0\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f} - Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "#Function to compute gradient of cross-entropy loss\n",
    "def cross_entropy_loss_gradient(actual_labels, predicted_probs):\n",
    "    num_samples = actual_labels.shape[0]\n",
    "    gradient = -actual_labels / (predicted_probs + 1e-7) / num_samples\n",
    "    return gradient\n",
    "\n",
    "\n",
    "#Function to make predictions using the neural network\n",
    "def predict(input_sample, conv, pool, full):\n",
    "    # Forward pass through Convolution and pooling..\n",
    "    conv_out = conv.forward(input_sample)\n",
    "    pool_out = pool.forward(conv_out)\n",
    "    # Flattening..\n",
    "    flattened_output = pool_out.flatten()\n",
    "    # Forward pass through fully connected layer..\n",
    "    predictions = full.forward(flattened_output)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, following processes are made:\n",
    "\n",
    "1 - cross_entropy_loss: This function calculates the cross-entropy loss between predicted probabilities and actual targets. It uses the negative log-likelihood loss formula, summing over the product of the target values and logarithm of the predicted probabilities. The loss is divided by the number of samples to get the average loss.\n",
    "\n",
    "2 - cross_entropy_loss_gradient: This function computes the gradient of the cross-entropy loss with respect to the predicted probabilities. It uses the derivative of the negative log-likelihood loss function. It takes the difference between actual labels and predicted probabilities divided by the number of samples.\n",
    "\n",
    "3 - train_network: This function trains a neural network by performing forward and backward passes for a specified number of epochs. Within each epoch, it iterates through the training data, computes the forward pass through the convolutional, pooling, and fully connected layers, calculates the loss, checks for accuracy, and performs the backward pass to update the weights of the network.\n",
    "\n",
    "4 - predict: This function takes an input sample, performs a forward pass through the convolutional and pooling layers, flattens the output, and then passes it through the fully connected layer to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 28, 28)\n",
      "(10500, 15)\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    class_folders = sorted(os.listdir(directory))  #Get the list of class folders in the directory\n",
    "    for class_label, class_folder in enumerate(class_folders):\n",
    "        class_path = os.path.join(directory, class_folder)  #Path for each class folder\n",
    "        for filename in os.listdir(class_path):\n",
    "            if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_path = os.path.join(class_path, filename)  #Full path for each image\n",
    "                image = cv2.imread(image_path)  #Read image using OpenCV\n",
    "                image = cv2.resize(image, (28, 28))  #Resize the image to 28x28 pixels\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  #Convert image to grayscale\n",
    "\n",
    "                images.append(image)  #Append the processed image\n",
    "                labels.append(class_label)  #Append the label corresponding to the class folder\n",
    "\n",
    "    return np.array(images), np.array(labels)  #Return images and labels as NumPy arrays\n",
    "\n",
    "def preprocess_dataset(images, labels):\n",
    "    labels_one_hot = to_categorical(labels)  #Convert labels to one-hot encoded format\n",
    "    return labels_one_hot  #Return the one-hot encoded labels\n",
    "\n",
    "#Directories for training and testing datasets\n",
    "train_directory = './train'\n",
    "test_directory = './test'\n",
    "\n",
    "#Load and preprocess training images and labels\n",
    "train_x_orig, train_y = load_images_from_directory(train_directory)\n",
    "train_y = preprocess_dataset(train_x_orig, train_y)  # Preprocess training labels\n",
    "\n",
    "print(train_x_orig.shape)  #Print shape of the training images\n",
    "test_x_orig, test_y = load_images_from_directory(test_directory)\n",
    "test_y = preprocess_dataset(test_x_orig, test_y)  #Preprocess testing labels\n",
    "print(train_y.shape)  #Print shape of the training labels\n",
    "\n",
    "print(train_y[0])  #Print the one-hot encoded label of the first training sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we are loading images from directories, processing them (resizing, converting to grayscale), and preparing the labels as one-hot encoded vectors for both training and testing datasets. Additionally, it prints the shapes of the training images and labels and displays the one-hot encoded label of the first training sample.\n",
    "\n",
    "We can say that there are 10500 images and 15 classes in the training set, the images are sized 28x28 pixels, and the labels are represented as one-hot encoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.5081 - Accuracy: 6.43%\n",
      "Epoch 2/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 3/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 4/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 5/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 6/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 7/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 8/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 9/10 - Loss: 1.5087 - Accuracy: 6.40%\n",
      "Epoch 10/10 - Loss: 1.5087 - Accuracy: 6.40%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initialize the Convolutional Neural Network with specific configurations\n",
    "conv = Convolution(train_x_orig[0].shape, 6, 1)  # Creating a Convolution layer\n",
    "pool = MaxPool(2)  # Creating a MaxPooling layer\n",
    "full = Fully_Connected(121, 15)  # Creating a Fully Connected layer\n",
    "\n",
    "#Train the network using the training data\n",
    "train_network(train_x_orig, train_y, conv, pool, full)\n",
    "\n",
    "predictions = []  \n",
    "#Initialize an empty array to store predictions\n",
    "\n",
    "#Make predictions on the test set using the trained network\n",
    "for data in test_x_orig:\n",
    "    pred = predict(data, conv, pool, full)  #Get predictions for each data point\n",
    "    OHotPred = np.zeros_like(pred)\n",
    "    OHotPred[np.argmax(pred)] = 1  #Convert the prediction to one-hot encoding\n",
    "    predictions.append(OHotPred.flatten())  #Append the prediction to the list\n",
    "\n",
    "predictions = np.array(predictions)  #Convert the predictions to a NumPy array\n",
    "predictions  #Return the predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block initializes a convolutional neural network (CNN) with specific configurations like filter size, pooling, and fully connected layers. It then trains this network using the provided training data (train_x_orig and train_y). Afterwards, it makes predictions on the test set (X_test) using the trained network (conv, pool, full), and collects the predictions in a predictions array. \n",
    "\n",
    "Creating a CNN, trains it on the provided training data, and then makes predictions on the test set using the trained network, storing these predictions in a NumPy array named predictions.\n",
    "\n",
    "Output Analysis:\n",
    "----------------\n",
    "Training Results: Loss and accuracy values are provided for each epoch. These metrics indicate how well the model performs on the training dataset. A consistent loss throughout training may signify a limitation in the model's learning capacity.\n",
    "\n",
    "Predictions: Predictions for the test dataset are given within an array named 'predictions'. These predictions are presented in a one-hot encoding format for each data point. For instance, each row contains predictions for different data points.\n",
    "\n",
    "These outputs indicate that while showcasing the model's training, its performance is at the desired level. A relatively low accuracy rate during the training process may suggest that the model struggles to learn from the data or has difficulty representing the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
